% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrreprt}
\usepackage{xcolor}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{5}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother


\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother


% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}



\setlength{\emergencystretch}{3em} % prevent overfull lines

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}



 


\KOMAoption{captions}{tableheading}
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Stats Textbook},
  pdfauthor={Norah Jones},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{Stats Textbook}
\author{Norah Jones}
\date{2026-01-26}
\begin{document}
\maketitle

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}

\bookmarksetup{startatroot}

\chapter*{Preface}\label{preface}
\addcontentsline{toc}{chapter}{Preface}

\markboth{Preface}{Preface}

This is a Quarto book.

To learn more about Quarto books visit
\url{https://quarto.org/docs/books}.

\bookmarksetup{startatroot}

\chapter*{Introduction}\label{introduction}
\addcontentsline{toc}{chapter}{Introduction}

\markboth{Introduction}{Introduction}

This is a book created from markdown and executable code.

See Knuth (1984) for additional discussion of literate programming.

\bookmarksetup{startatroot}

\chapter{Chapter 1}\label{chapter-1}

This is Chapter 1.

\bookmarksetup{startatroot}

\chapter{Chapter 2}\label{chapter-2}

\bookmarksetup{startatroot}

\chapter{Chapter 2}\label{chapter-2-1}

This is Chapter 2.

\bookmarksetup{startatroot}

\chapter{Chapter 3}\label{chapter-3}

\bookmarksetup{startatroot}

\chapter{Chapter 3}\label{chapter-3-1}

This is Chapter 3.

\bookmarksetup{startatroot}

\chapter{Chapter 4}\label{chapter-4}

\bookmarksetup{startatroot}

\chapter{Chapter 4}\label{chapter-4-1}

This is Chapter 4.

\bookmarksetup{startatroot}

\chapter{Chapter 5}\label{chapter-5}

\bookmarksetup{startatroot}

\chapter{Chapter 5}\label{chapter-5-1}

This is Chapter 5.

\bookmarksetup{startatroot}

\chapter{Chapter 6}\label{chapter-6}

\bookmarksetup{startatroot}

\chapter{Chapter 6}\label{chapter-6-1}

This is Chapter 6.

\bookmarksetup{startatroot}

\chapter{Chapter 7}\label{chapter-7}

\bookmarksetup{startatroot}

\chapter{Chapter 7}\label{chapter-7-1}

This is Chapter 7.

\bookmarksetup{startatroot}

\chapter{Chapter 8}\label{chapter-8}

\bookmarksetup{startatroot}

\chapter*{Evaluating Group Averages Against a Benchmark: One-Sample t
Test and Goodness-of-Fit
Analysis}\label{evaluating-group-averages-against-a-benchmark-one-sample-t-test-and-goodness-of-fit-analysis}
\addcontentsline{toc}{chapter}{Evaluating Group Averages Against a
Benchmark: One-Sample t Test and Goodness-of-Fit Analysis}

\markboth{Evaluating Group Averages Against a Benchmark: One-Sample t
Test and Goodness-of-Fit Analysis}{Evaluating Group Averages Against a
Benchmark: One-Sample t Test and Goodness-of-Fit Analysis}

\section*{Learning Objectives}\label{learning-objectives}
\addcontentsline{toc}{section}{Learning Objectives}

\markright{Learning Objectives}

By the end of this chapter, you will be able to:

\begin{itemize}
\item
  Describe the differences between z tests and t tests, including when
  to use each based on knowledge of population parameters and sample
  characteristics.
\item
  Calculate and interpret the test statistic, p value, and effect size
  for one-sample t tests and goodness-of-fit analyses.
\item
  Clearly explain the distinction between statistical significance and
  practical significance, and evaluate both to provide a comprehensive
  understanding of research findings.
\item
  Use confidence intervals to quantify uncertainty in sample mean
  estimates and evaluate the plausibility of a null hypothesis.
\end{itemize}

We just learned about z tests in the previous chapter, which you can
conduct if you want to compare a sample mean to a population with a
known mean and standard deviation. Often, however, we do not know the
population standard deviation (Ïƒ), and we must estimate it with the
sample statistic, in this case sample standard deviation (s). When this
happens, we can no longer rely on the standard normal distribution, also
called the z distribution, as it assumes precise knowledge of Ïƒ.
Instead, we use a t distribution, which accounts for the added
uncertainty of estimating Ïƒ. In these cases, we use a t distribution,
which closely resembles its z-distribution counterpart but with two key
differences.

First, the tail ends of the t distribution take longer to approach the
X-axis compared to the z distribution. This is because we are estimating
the population standard deviation, and that means we have more
uncertainty. If you look at figure 8.1, imagine drawing a vertical line
out towards one of the ends. Can you see how there would be more area
under the curve for the t distribution with just 5 degrees of freedom?
Shaded areas under the curve represent 5\% of the total area. This means
the sample mean must be farther from the population mean before we can
confidently conclude that the observed difference reflects something
other than random chance.

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{chapter8_files/figure-pdf/unnamed-chunk-1-1.pdf}}

}

\caption{Fig 8.1: Comparison of Standard Normal and t-Distributions}

\end{figure}%

Second, there is more than one t distribution, and the shape of the
distribution depends on your sample size. Recall that a larger sample
size leads to a smaller standard error, which in turn increases your
statistical power to detect a significant effect. You can see how
different t distributions look in figure 8.1. The larger the sample
size, the closer the t distribution becomes to a z distribution. Once n
\textgreater{} 120, the two distributions are almost identical. Having
such a large sample makes up for the fact that we are estimating the
population standard deviation.

Another way to look at this is that the t distribution is related to the
degrees of freedom. You must know the degrees of freedom to utilize a t
table. Since sample standard deviation is used to compute the estimate
of standard error, the degrees of freedom for a t test are calculated by
using n - 1.

\section{Example: Conducting one sample
t-test}\label{example-conducting-one-sample-t-test}

Now, let's walk through an example of conducting a one-sample t test.
Let's think back to the example from the last chapter, where we compared
a sample mean to a population mean with the z test. We have a similar,
but slightly different, scenario this time. We have a sample of just 35
test scores from a single third grade class. The teacher is concerned
that their students are underperforming in English Language Arts (ELA)
and hypothesize their class will score below the state average of 440.
When the results were returned to the school, the class's average score
was 425.26. Is this average significantly lower than the expected value
of 440? Let's conduct a hypothesis test to find out.

\subsection{Step 1: Determine the null and alternative
hypotheses.}\label{step-1-determine-the-null-and-alternative-hypotheses.}

Just like last time, the null hypothesis is that there is no difference
between the teacher's class and the overall state population. So, the
null hypothesis is:

\[
H_0: \mu = 440
\]

This time, however, we are going to use a directional, or one-tailed
test, because the teacher is specifically concerned that the students
are scoring lower than the general population. So, the alternative
hypothesis is:

\[
H_1: \mu < 440
\]

There are 35 students in this class, which might be why they are not
doing so well, so n = 35. The sampling distribution in this case would
be composed of the means of all possible samples of size n = 35 taken
from the state population of third graders. It would be centered at Î¼ =
440 (the population mean, which we know), but we must calculate the
standard error using the sample standard deviation. The sample standard
deviation (s) happens to be 44.994.

\[
\frac{s}{\sqrt{n}} = \frac{44.994}{\sqrt{35}} = 7.605
\]

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{chapter8_files/figure-pdf/unnamed-chunk-2-1.pdf}}

}

\caption{Fig 8.2: Comparison of Sampling Distribution (t, df=34) and
Normal Distribution}

\end{figure}%

\subsection{Step 2: Set the criteria for a
decision.}\label{step-2-set-the-criteria-for-a-decision.}

We will set the alpha level (Î±) to 0.05. So, we will reject the null
hypothesis if the p value is less than 0.05. Since we are conducting a
one-sided directional test, the entire rejection region is in the left
tail of the distribution. To find the critical value we first need to
consider our degrees of freedom. For a t test, df = n -- 1, so we have
34 degrees of freedom. In the Critical Values of t distribution table in
the appendix, there is no line for df = 34, so round down to the nearest
degrees of freedom (30) as a conservative estimate. If you use an online
calculator, which allows you to enter precise degrees of freedom, you
will find that the critical value for 34 degrees of freedom at a .05
alpha level is -1.69. It is negative because we expect the sample mean
to be lower than the population mean.

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{chapter8_files/figure-pdf/unnamed-chunk-3-1.pdf}}

}

\caption{Sampling Distribution of the Sample Mean (n=35)}

\end{figure}%

\subsection{Step 3: Check assumptions and compute the test
statistic.}\label{step-3-check-assumptions-and-compute-the-test-statistic.}

There are three assumptions underlying this test. They are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Random sampling: The scores come from a random, representative sample.
\item
  Independence: The scores are independent of one another.
\item
  Normality: The population score distribution is approximately normal.
\end{enumerate}

It is okay that we didn't choose a random sample representing all third
graders in the state because the inferences we will draw are only about
this one class. We can't see the entire population distribution, but the
histogram for the scores looks almost perfectly normal.

(fig 4 here)

If the distribution was way off, we could still proceed but be very
cautious about our interpretation of the results. Since our sample size
is greater than 30, we would be okay even without such a normal
distribution due to the central limit theorem. So, we are all set to
calculate our test statistic.

For the one-sample t test, we use the formula:

\[
t = \frac{\bar{X} - \mu}{\frac{s}{\sqrt{n}}}
\]

Notice that the formula is very similar to what we used to calculate the
z statistic. The only difference is that we now estimate the standard
error using the sample standard deviation and sample size in the
denominator because we don't know the value of the population standard
deviation (ðœŽ).

The numerator represents the difference between the observed and test
value specified in the null hypothesis. The symbol ðœ‡ represents the
population parameter specified in the hypothesis, often referred to as
the null value, as it reflects the assumed true population mean under
the null hypothesis. The other values for the test statistic come from
our sample data.

\[
t
= \frac{425.26 - 440}{\dfrac{44.994}{\sqrt{35}}}
= \frac{-14.74}{7.605}
= -1.938
\]

\subsection{Step 4}\label{step-4}

\subsubsection{a: Find the p value.}\label{a-find-the-p-value.}

We must now determine whether our t statistic is significant, which
would mean that it is very unlikely we obtained this value due to chance
alone. In step 2, we said that the critical value for this test was
-1.69. Picture a t distribution and draw a vertical line about 1.7
standard deviations below the mean. Anything to the left of that is in
the rejection region. Is our t statistic of -1.938 in the rejection
region? Yes -- it is farther away from the center than our critical
value. This means our p value is less than 0.05.

(fig 8.5 here)

\subsubsection{b: Draw a conclusion and report your
conclusion.}\label{b-draw-a-conclusion-and-report-your-conclusion.}

Given that our p value is less than 0.05, the appropriate decision is to
reject the null hypothesis. In other words, we have found evidence in
favor of the alternative hypothesis. Students in this class scored
significantly lower than the overall state population on the test. Let's
assume we use statistical software to land on a p value of 0.03 for this
analysis.

\#\# Reporting Results in APA Format

The American Psychological Association, 7th edition (APA, 2019) has
decided that there is a correct way to report the results of a t test.
In this case, you would report the results of our test like this: Scores
for students in this third-grade class (M = 425.26) were significantly
lower than scores of third graders statewide, t(34) = -1.938, p = 0.03.

The most important part of the reporting is the section that starts with
the observed t statistic. You put the degrees of freedom in parentheses
immediately following the t; all statistics (e.g., M, t, p) are in
italics, and you report the p value after the exact t statistic.
Remember, that even if the statistical software says p = 0.000, you
should never report that. Instead, you should report p \textless{}
0.001. You can also just report that p \textless{} 0.05 (your alpha
level) if you do not have access to statistical software.

\section{Cautions with One-Sided
Tests}\label{cautions-with-one-sided-tests}

One caution with directional, one-tailed tests is that you need to
decide on one-tailed versus two-tailed tests up front. It is really not
okay to decide after you see the results that you are going to conduct a
one-tailed test. You should only be conducting a one-tailed test if you
have good reason to expect the sample mean to be lower or higher than,
rather than just different from, the population mean. And while a
one-tailed test has greater power to detect a significant result, there
is risk involved. Imagine that we expected the sample mean to be
significantly lower, but it turned out to be significantly higher than
the population mean. If we had been conducting a two-tailed test, that
would be a significant difference from the population mean. But since
our alternative hypothesis was that Î¼ \textless{} 440, we would fail to
reject the null (non-significant result). Most of the time you will be
using a two-tailed test for the possibility of a difference in either
direction. Only use a one-tailed test if you have a very good reason and
you decide ahead of time to do so.

Another caution is that type III error is introduced when using
directional tests. Type III error occurs when a correct conclusion is
drawn for the wrong reason or when the wrong question is asked, and a
statistically valid result is obtained for that incorrect question. For
example, a school district wants to determine whether lessons learned
from a new professional development program will reduce the time
teachers spend on grading. After collecting data and running a
statistical test, the null hypothesis (that the program has no effect on
grading time) is rejected, and a significant difference was found.
District leadership interpreted this result as evidence that the program
was reducing grading time, so they decided to expand the program.
However, a few weeks later, teachers were demanding that the district
take a closer look at the data because they claimed to be spending more
time grading. Sure enough, district leadership found they made a
mistake: while the statistical test correctly identified a significant
difference in grading time (rejecting the null hypothesis), they
misinterpreted the direction of the effect. The program increased
grading time instead of reducing it. This misinterpretation of the
direction of the significant effect is a Type III error: correctly
rejecting the null hypothesis but misunderstanding what the result
actually implies.

\section{Understanding Effect Size}\label{understanding-effect-size}

Hypothesis testing is important for knowing if there is a significant
difference between a sample and the population, but it will not tell you
how big the difference is. A hypothesis test will almost always return a
significant result when you have a large sample size because a test
statistic is a function of standard error, which decreases as sample
size increases.

Given the mean difference and standard deviation remain the same, as
sample size increases, the test statistic always increases. For example,
if you wanted to conduct a one-sample t test with a sample mean of 22
and a SD of 2 compared to a population mean of 23.5, you are sure to get
a statistically significant result when n = 100,000 even if the p value
is \textgreater{} 0.05 when n = 10. It is somewhat paradoxical that with
sufficiently large sample size, the statistical power becomes strong
enough to flag even small differences as significant, even if those
differences are practically trivial or meaningless. Therefore, it is
important to have another way to represent the meaningfulness, or
practical significance, of a statistically significant result. That is
why we calculate and report effect size.

Of course, one way of understanding the size of the difference is to
simply compare the sample mean and population mean on the original
measurement scale. Someone who understands a particular testing
instrument could make their own judgment as to whether a difference of
about 15 points was meaningful, or cause for concern. But it helps to
have a common language for reporting effect size, and the way we do that
is to standardize the effect.

Effect size is a numeric measure that indicates the strength or
meaningfulness of the relationship or difference between variables in a
study. Unlike p values, which only tell you whether an effect exists,
effect size tells you how large or meaningful that effect is. It
provides a standardized way to understand the practical significance of
research findings, which is particularly useful when comparing results
across studies.

Estimated Cohen's d is a common measure of effect size for t tests. We
hope you remember back in chapter 5 when we learned about z scores,
which are also known as standardized scores because they are measured in
standard deviation units. When we use estimated Cohen's d as a measure
of effect size, we are doing the same thing. Cohen's d quantifies the
magnitude of the difference between two means relative to the
variability in the data and provides a standardized measure of practical
significance, helping to interpret the importance of the result by
allowing us to express the effect size in terms of standard deviation
units. This approach provides a common scale, making it easier to
compare results across different tests or measures, rather than relying
on raw score differences.

\section{Calculating Estimated Cohen's
d}\label{calculating-estimated-cohens-d}

One quick note to get us started -- we are being careful to refer to the
effect size we use for t tests as estimated Cohen's d to distinguish it
from Cohen's d used in z tests. This distinction arises because effect
sizes for t tests are calculated using sample data, whereas z tests rely
on known population parameters. But out in the world of educational
research, you will normally just hear it called Cohen's d regardless of
the type of analysis used.

Here is the formula for estimated Cohen's d for a one-sample t test:

\[
d = \frac{\bar{X} - \mu}{SD}
\]

In our example above, the sample mean (XÌ…) = 425.26 and SD = 44.994. The
population mean (Î¼) = 440. Using the formula,

\[
d
= \frac{425.26 - 440}{44.994}
= -0.33
\]

That is, our effect size is -0.33, which according to the following
table from Privitera (2018) is a medium effect size. In our sample
research question, this means the students in the third-grade class had
scores that were about one-third of a standard deviation below the state
average. A positive effect size indicates the sample mean was larger
than the population mean, and a negative effect size indicates it was
smaller. Since effect size can be negative, you need to consider the
absolute value when making a judgement about the strength of the effect.

\textbf{Cohen's Effect Size Conventions}

\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Description of Effect & Effect Size (d) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Small & ( d \textless{} 0.2 ) \\
Medium & ( 0.2 \textless{} d \textless{} 0.8 ) \\
Large & ( d \textgreater{} 0.8 ) \\
\end{longtable}

\emph{Source:} Adapted from Privitera, G. J. (2018). \emph{Statistics
for the Behavioral Sciences} (3rd ed.). SAGE Publications.

Before we move on, we want to remind you of a few other similar formulas
because we had a tendency to get them confused in the past. First, here
is a reminder of the formula for calculating z scores:

\[
z = \frac{X - \mu}{\sigma}
\]

Do you see the differences between that and the formula used to
calculate Cohen's d? First, with a z score, we only have one X (i.e., a
particular value), rather than a sample mean, or XÌ…. But even more
importantly, we use the population standard deviation (Ïƒ) rather than
the sample SD to calculate a z score. But overall, the concept is
exactly the same: both z scores and Cohen's d involve converting a raw
difference from the mean into standard deviation units. The key
difference lies in the specific inputs used for calculations.

Do you see the differences between that and the formula used to
calculate Cohen's d? First, with a z score, we only have one X (i.e., a
particular value), rather than a sample mean, or XÌ…. But even more
importantly, we use the population standard deviation (Ïƒ) rather than
the sample SD to calculate a z score. But overall, the concept is
exactly the same: both z scores and Cohen's d involve converting a raw
difference from the mean into standard deviation units. The key
difference lies in the specific inputs used for calculations.

Another place we have seen learners, including ourselves, get tripped up
is confusing the formula for Cohen's d with the formula for the t
statistic itself. Here they are, side by side:

\[
\begin{aligned}
d &= \frac{\bar{X} - \mu}{SD}
\qquad
t &= \frac{\bar{X} - \mu}{\dfrac{s}{\sqrt{n}}}
\end{aligned}
\]

Even though the first formula, Cohen's d, uses SD to represent the
sample standard deviation, and the second, t statistic, uses s, they
represent the same value. So, the only difference is that d has nothing
to do with sample size, but sample size (n) is hugely important in
calculating standard error, which is the denominator for the t
statistic. For Cohen's d, the mean difference is divided by the standard
deviation, which represents the variability in the actual scores within
the sample. For the t statistic, the mean difference is divided by the
standard error, which represents the standard deviation of the
theoretical sampling distribution. By definition, it is going to be much
smaller than the original sample standard deviation.

In this example, the same mean difference (-14.743) served as the
numerator for both d and t. When we divided it by standard error for the
t statistic, we got -1.938. This told us how far our sample mean was
from the center of the sampling distribution. When we divided it by the
sample standard deviation for Cohen's d, we got -0.33, which told us how
far our sample mean was from the population mean, the center of the
population distribution.

If nothing else, we hope this helps you see why we need both effect size
and hypothesis tests. Effect size tells us how big a difference is while
the hypothesis test tells us whether we can be confident the difference
is a result of something other than chance. Effect size does not depend
on sample size, but you are more likely to get a significant result from
your hypothesis test as you increase the sample size.

It is common to report effect size after the results of your hypothesis
test. For example, scores for students in this third-grade class (M =
425.26) were significantly lower than scores of third graders statewide,
t(34) = -1.938, p = 0.03, d = -0.33. You might also choose to report the
significant result followed by a statement that Cohen's d (-0.33)
indicated a medium effect size. Keep in mind that if you have a small
sample size, you might be able to calculate a large effect size even
without a significant result. This can be interesting to learn and might
suggest you should try to conduct your analysis again with a larger
sample size. But we don't report effect size after a non-significant
result since it does not make sense to describe the size of an effect
you just concluded does not exist.

\section{Confidence Intervals}\label{confidence-intervals}

How do confidence intervals fit in? Confidence intervals provide an
additional layer of transparency by offering a range of values likely to
contain the true population parameter.

The way we think about it, the reason we use confidence intervals is to
be up front about our level of uncertainty. We take a sample and report
the mean, but how confident are we that the mean of our sample is equal
to the true population mean? Or we have two samples and we find the
difference between the two means (stay tuned for the next chapter), but
how confident are we that we know the true difference? If we report a
point estimate (e.g., a mean) along with an interval estimate, we are
being completely transparent about how good our estimate is likely to
be.

Another high-level concept to be aware of before we dive into the
details is the balance between precision and accuracy. Say we want to
estimate the average height of all people in the entire world who are 18
and older. We could be highly confident in our accuracy if we estimated
that the mean height of all adults was somewhere between 2 feet
(60.96cm) and 8 feet (243.84cm) . Don't you feel pretty comfortable
accepting that the true population mean is somewhere within that
interval? And yet, do you feel like you have learned any useful
information? Probably not, because our estimate has almost zero
precision. We could go in another direction and estimate that the
average height was exactly 65.8 inches (167.13cm). That estimate is much
more precise, but who knows how accurate it is?

What we often do is report both point and interval estimates. We
commonly see this with survey results. We start with a sample mean and
then use the standard error to calculate a confidence interval. The more
confident we are (higher level of accuracy), the wider the interval
(less precision). We can decide on any confidence level we want and
calculate the exact interval, but it is most common to use 90, 95, or
99\%.

\subsection{Interpreting Confidence
Intervals}\label{interpreting-confidence-intervals}

Now that you have the general idea about the purpose of confidence
intervals, we are going to get a little pedantic and refer you to the
words of
\href{https://digitalcommons.unl.edu/cgi/viewcontent.cgi?referer=&httpsredir=1&article=1009&context=imseteach}{Dr.~Paul
Savory (2008)}. Say you want to know the mean of some variable in the
population. You take a sample, find the mean, and then create a 95\%
confidence interval around it. It is incorrect to say, ``There is a 95\%
probability that Î¼ falls within this interval.'' This is incorrect
because it implies that Î¼ is random and could take on different values.
In fact, Î¼ is a constant -- we just don't know what it is -- and it
either falls within our interval or it doesn't. The correct
interpretation is, ``We are confident that if Î¼ were known, this
interval would contain it.'' This may seem like a trivial difference,
but it makes an important point that the probability or confidence level
we report is referring to the interval, not the population parameter
itself.

We highly recommend visiting the book's website to test out the
\href{no\%20link\%20right\%20now\%20:/}{confidence intervals applet} for
a helpful visualization of the concept. The way the applet works is that
you select the confidence level and sample size, and then it will
randomly pull one or 25 samples from the population distribution. If you
request 25 samples, you will see them all lined up. Each sample will
have a mean and corresponding confidence interval around it.

We used a 90\% confidence interval with a sample size of 10 in the
following example and asked the applet to take 25 samples, see figure
8.6. Almost all resulting samples had a confidence interval that
contained the true population mean, but two did not. Note that 23/25 is
92\% and not 90\%, but when we try again, sometimes we get 24/25 or
22/25. Even our 90\% confidence interval is just an estimate. The applet
provides a great visualization of the difference between the original
population distribution and the sampling distribution. You recommend
playing around in the applet to see how increasing the confidence level
and/or the sample size will typically result in a greater percentage of
possible sample intervals containing the true population mean.

(fig 8.6 here)

\subsection{Calculating Confidence
Intervals}\label{calculating-confidence-intervals}

We are going to be calculating confidence intervals around a sample
mean, but keep in mind that you can estimate a confidence interval
around other statistics as well, such as a mean difference or an odds
ratio. Here is how we estimate an unknown population mean using a
confidence interval (CI) when population standard deviation is unknown:

\[
\text{CI for Population Mean}
=
\text{Sample Mean}
\;\pm\;
(\text{Level of Confidence}) \times (\text{Standard Error})
\]

In mathematical terms:

\[
CI(\mu) = \bar{X} \pm t_{\text{critical}} \times \frac{s}{\sqrt{n}}
\]

So, there are three steps to estimate a confidence interval for a
population mean.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Compute the sample mean and standard error.
\item
  Choose the level of confidence and find the critical values of the t
  distribution for that level.
\item
  Compute the estimation formula to find the upper and lower confidence
  interval limits.
\end{enumerate}

The width of our interval depends on several factors: how confident we
want to be, the size of our sample, and the variability (or standard
deviation) of scores within our sample.

Let's work through an example. Suppose you selected a sample of 100
participants and computed sample statistics. You found a sample mean of
4 and SD of 5. Now, you want to estimate a confidence interval around
that mean.

\subsubsection{Step 1: Compute the sample mean and standard
error}\label{step-1-compute-the-sample-mean-and-standard-error}

From the information above, we already know that XÌ… = 4, SD = 5, and n =
100, so we can calculate the standard error as follows:

\[
\frac{s}{\sqrt{n}} = \frac{5}{\sqrt{100}} = 0.5
\]

\subsubsection{Step 2: Choose the level of confidence and find critical
values}\label{step-2-choose-the-level-of-confidence-and-find-critical-values}

How do we know what level of confidence to choose? This is where you
must balance precision and accuracy. In social science research we
typically use a 95\% level of confidence, which as you saw is consistent
with our typical practice of choosing an alpha level of 0.05 for
hypothesis testing.

We look to the Critical Values of t Distribution Table in the appendix
to find the critical value. Find the t value for a 95\% level of
significance for a two-tailed test with df = 99. There is no row for 99
degrees of freedom in the table, so we will use df = 100 for an
approximation, and we get the t value of 1.98.

\subsubsection{Step 3: Compute the estimation formula to find upper and
lower confidence interval
limits.}\label{step-3-compute-the-estimation-formula-to-find-upper-and-lower-confidence-interval-limits.}

Using these numbers in the formula from earlier in the chapter, we get:

\[
4 \pm 1.98 \times 0.5 = (3.01,\; 4.99)
\]

We can conclude that we are 95\% confident the interval of 3.01 to 4.99
contains the true population mean.

\subsection{Using a Confidence Interval to Evaluate a Null
Hypothesis}\label{using-a-confidence-interval-to-evaluate-a-null-hypothesis}

We are going to walk through one more scenario using our original
one-sample t test example. This example will demonstrate how estimation
through confidence intervals is connected to hypothesis testing, showing
that both methods can provide related insights into evaluating a null
hypothesis.

\subsubsection{Step 1: Compute the sample mean and standard
error}\label{step-1-compute-the-sample-mean-and-standard-error-1}

We already calculated the relevant values, but to get everything back in
one place, our sample mean was XÌ… = 425.26, our standard error was 7.605,
and we wanted to know if our sample mean could be considered equal to
the population mean of 440.

\subsubsection{Step 2: Choose the level of confidence and find critical
values}\label{step-2-choose-the-level-of-confidence-and-find-critical-values-1}

\bookmarksetup{startatroot}

\chapter{Chapter 9}\label{chapter-9}

\bookmarksetup{startatroot}

\chapter{Chapter 9}\label{chapter-9-1}

This is Chapter 9.

\bookmarksetup{startatroot}

\chapter{Chapter 10}\label{chapter-10}

\bookmarksetup{startatroot}

\chapter{Chapter 10}\label{chapter-10-1}

This is Chapter 10.

\bookmarksetup{startatroot}

\chapter{Chapter 11}\label{chapter-11}

\bookmarksetup{startatroot}

\chapter{Chapter 11}\label{chapter-11-1}

This is Chapter 11.

\bookmarksetup{startatroot}

\chapter{Chapter 12}\label{chapter-12}

\bookmarksetup{startatroot}

\chapter{Chapter 12}\label{chapter-12-1}

This is Chapter 12.

\bookmarksetup{startatroot}

\chapter{Chapter 13}\label{chapter-13}

\bookmarksetup{startatroot}

\chapter{Chapter 13}\label{chapter-13-1}

This is Chapter 13.

\bookmarksetup{startatroot}

\chapter{Chapter 14}\label{chapter-14}

\bookmarksetup{startatroot}

\chapter{Chapter 14}\label{chapter-14-1}

This is Chapter 14.

\bookmarksetup{startatroot}

\chapter*{Summary}\label{summary}
\addcontentsline{toc}{chapter}{Summary}

\markboth{Summary}{Summary}

In summary, this book has no content whatsoever.

\bookmarksetup{startatroot}

\chapter*{References}\label{references}
\addcontentsline{toc}{chapter}{References}

\markboth{References}{References}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-knuth84}
Knuth, Donald E. 1984. {``Literate Programming.''} \emph{Comput. J.} 27
(2): 97--111. \url{https://doi.org/10.1093/comjnl/27.2.97}.

\end{CSLReferences}




\end{document}
