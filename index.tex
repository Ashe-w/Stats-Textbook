% Options for packages loaded elsewhere
% Options for packages loaded elsewhere
\PassOptionsToPackage{unicode}{hyperref}
\PassOptionsToPackage{hyphens}{url}
\PassOptionsToPackage{dvipsnames,svgnames,x11names}{xcolor}
%
\documentclass[
  letterpaper,
  DIV=11,
  numbers=noendperiod]{scrreprt}
\usepackage{xcolor}
\usepackage{amsmath,amssymb}
\setcounter{secnumdepth}{5}
\usepackage{iftex}
\ifPDFTeX
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
  \usepackage{textcomp} % provide euro and other symbols
\else % if luatex or xetex
  \usepackage{unicode-math} % this also loads fontspec
  \defaultfontfeatures{Scale=MatchLowercase}
  \defaultfontfeatures[\rmfamily]{Ligatures=TeX,Scale=1}
\fi
\usepackage{lmodern}
\ifPDFTeX\else
  % xetex/luatex font selection
\fi
% Use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
\IfFileExists{microtype.sty}{% use microtype if available
  \usepackage[]{microtype}
  \UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\makeatletter
\@ifundefined{KOMAClassName}{% if non-KOMA class
  \IfFileExists{parskip.sty}{%
    \usepackage{parskip}
  }{% else
    \setlength{\parindent}{0pt}
    \setlength{\parskip}{6pt plus 2pt minus 1pt}}
}{% if KOMA class
  \KOMAoptions{parskip=half}}
\makeatother
% Make \paragraph and \subparagraph free-standing
\makeatletter
\ifx\paragraph\undefined\else
  \let\oldparagraph\paragraph
  \renewcommand{\paragraph}{
    \@ifstar
      \xxxParagraphStar
      \xxxParagraphNoStar
  }
  \newcommand{\xxxParagraphStar}[1]{\oldparagraph*{#1}\mbox{}}
  \newcommand{\xxxParagraphNoStar}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
  \let\oldsubparagraph\subparagraph
  \renewcommand{\subparagraph}{
    \@ifstar
      \xxxSubParagraphStar
      \xxxSubParagraphNoStar
  }
  \newcommand{\xxxSubParagraphStar}[1]{\oldsubparagraph*{#1}\mbox{}}
  \newcommand{\xxxSubParagraphNoStar}[1]{\oldsubparagraph{#1}\mbox{}}
\fi
\makeatother

\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{241,243,245}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.40,0.45,0.13}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\BuiltInTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\ExtensionTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.28,0.35,0.67}{#1}}
\newcommand{\ImportTok}[1]{\textcolor[rgb]{0.00,0.46,0.62}{#1}}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.68,0.00,0.00}{#1}}
\newcommand{\RegionMarkerTok}[1]{\textcolor[rgb]{0.00,0.23,0.31}{#1}}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.07,0.07,0.07}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.13,0.47,0.30}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.37,0.37,0.37}{\textit{#1}}}

\usepackage{longtable,booktabs,array}
\usepackage{calc} % for calculating minipage widths
% Correct order of tables after \paragraph or \subparagraph
\usepackage{etoolbox}
\makeatletter
\patchcmd\longtable{\par}{\if@noskipsec\mbox{}\fi\par}{}{}
\makeatother
% Allow footnotes in longtable head/foot
\IfFileExists{footnotehyper.sty}{\usepackage{footnotehyper}}{\usepackage{footnote}}
\makesavenoteenv{longtable}
\usepackage{graphicx}
\makeatletter
\newsavebox\pandoc@box
\newcommand*\pandocbounded[1]{% scales image to fit in text height/width
  \sbox\pandoc@box{#1}%
  \Gscale@div\@tempa{\textheight}{\dimexpr\ht\pandoc@box+\dp\pandoc@box\relax}%
  \Gscale@div\@tempb{\linewidth}{\wd\pandoc@box}%
  \ifdim\@tempb\p@<\@tempa\p@\let\@tempa\@tempb\fi% select the smaller of both
  \ifdim\@tempa\p@<\p@\scalebox{\@tempa}{\usebox\pandoc@box}%
  \else\usebox{\pandoc@box}%
  \fi%
}
% Set default figure placement to htbp
\def\fps@figure{htbp}
\makeatother


% definitions for citeproc citations
\NewDocumentCommand\citeproctext{}{}
\NewDocumentCommand\citeproc{mm}{%
  \begingroup\def\citeproctext{#2}\cite{#1}\endgroup}
\makeatletter
 % allow citations to break across lines
 \let\@cite@ofmt\@firstofone
 % avoid brackets around text for \cite:
 \def\@biblabel#1{}
 \def\@cite#1#2{{#1\if@tempswa , #2\fi}}
\makeatother
\newlength{\cslhangindent}
\setlength{\cslhangindent}{1.5em}
\newlength{\csllabelwidth}
\setlength{\csllabelwidth}{3em}
\newenvironment{CSLReferences}[2] % #1 hanging-indent, #2 entry-spacing
 {\begin{list}{}{%
  \setlength{\itemindent}{0pt}
  \setlength{\leftmargin}{0pt}
  \setlength{\parsep}{0pt}
  % turn on hanging indent if param 1 is 1
  \ifodd #1
   \setlength{\leftmargin}{\cslhangindent}
   \setlength{\itemindent}{-1\cslhangindent}
  \fi
  % set entry spacing
  \setlength{\itemsep}{#2\baselineskip}}}
 {\end{list}}
\usepackage{calc}
\newcommand{\CSLBlock}[1]{\hfill\break\parbox[t]{\linewidth}{\strut\ignorespaces#1\strut}}
\newcommand{\CSLLeftMargin}[1]{\parbox[t]{\csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLRightInline}[1]{\parbox[t]{\linewidth - \csllabelwidth}{\strut#1\strut}}
\newcommand{\CSLIndent}[1]{\hspace{\cslhangindent}#1}



\setlength{\emergencystretch}{3em} % prevent overfull lines

\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}



 


\KOMAoption{captions}{tableheading}
\makeatletter
\@ifpackageloaded{bookmark}{}{\usepackage{bookmark}}
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\AtBeginDocument{%
\ifdefined\contentsname
  \renewcommand*\contentsname{Table of contents}
\else
  \newcommand\contentsname{Table of contents}
\fi
\ifdefined\listfigurename
  \renewcommand*\listfigurename{List of Figures}
\else
  \newcommand\listfigurename{List of Figures}
\fi
\ifdefined\listtablename
  \renewcommand*\listtablename{List of Tables}
\else
  \newcommand\listtablename{List of Tables}
\fi
\ifdefined\figurename
  \renewcommand*\figurename{Figure}
\else
  \newcommand\figurename{Figure}
\fi
\ifdefined\tablename
  \renewcommand*\tablename{Table}
\else
  \newcommand\tablename{Table}
\fi
}
\@ifpackageloaded{float}{}{\usepackage{float}}
\floatstyle{ruled}
\@ifundefined{c@chapter}{\newfloat{codelisting}{h}{lop}}{\newfloat{codelisting}{h}{lop}[chapter]}
\floatname{codelisting}{Listing}
\newcommand*\listoflistings{\listof{codelisting}{List of Listings}}
\makeatother
\makeatletter
\makeatother
\makeatletter
\@ifpackageloaded{caption}{}{\usepackage{caption}}
\@ifpackageloaded{subcaption}{}{\usepackage{subcaption}}
\makeatother
\usepackage{bookmark}
\IfFileExists{xurl.sty}{\usepackage{xurl}}{} % add URL line breaks if available
\urlstyle{same}
\hypersetup{
  pdftitle={Stats Textbook},
  pdfauthor={Norah Jones},
  colorlinks=true,
  linkcolor={blue},
  filecolor={Maroon},
  citecolor={Blue},
  urlcolor={Blue},
  pdfcreator={LaTeX via pandoc}}


\title{Stats Textbook}
\author{Norah Jones}
\date{2026-01-26}
\begin{document}
\maketitle

\renewcommand*\contentsname{Table of contents}
{
\hypersetup{linkcolor=}
\setcounter{tocdepth}{2}
\tableofcontents
}

\bookmarksetup{startatroot}

\chapter*{Preface}\label{preface}
\addcontentsline{toc}{chapter}{Preface}

\markboth{Preface}{Preface}

This is a Quarto book.

To learn more about Quarto books visit
\url{https://quarto.org/docs/books}.

\bookmarksetup{startatroot}

\chapter*{Introduction}\label{introduction}
\addcontentsline{toc}{chapter}{Introduction}

\markboth{Introduction}{Introduction}

This is a book created from markdown and executable code.

See Knuth (1984) for additional discussion of literate programming.

\bookmarksetup{startatroot}

\chapter{Chapter 1}\label{chapter-1}

This is Chapter 1.

\bookmarksetup{startatroot}

\chapter{Chapter 2}\label{chapter-2}

\bookmarksetup{startatroot}

\chapter*{Understanding the Center: Measures of Central
Tendency}\label{understanding-the-center-measures-of-central-tendency}
\addcontentsline{toc}{chapter}{Understanding the Center: Measures of
Central Tendency}

\markboth{Understanding the Center: Measures of Central
Tendency}{Understanding the Center: Measures of Central Tendency}

\section*{Learning Objectives}\label{learning-objectives}
\addcontentsline{toc}{section}{Learning Objectives}

\markright{Learning Objectives}

By the end of this chapter, you will be able to:

\begin{itemize}
\item
  Define and distinguish among the three main measures of central
  tendency---mean, median, and mode---and describe situations in which
  each is most appropriate to use.
\item
  Accurately calculate the mean, median, and mode for a given dataset
  and interpret the results in context.
\item
  Describe the concept of skewness and its relationship to measures of
  central tendency.
\end{itemize}

In this chapter and the next two, we will discuss three ways of
summarizing data. First, we look at how to generate a \emph{typical
score} -- one value that can tell us something important about the data.
In the next chapter, we will talk about variability, which refers to how
spread out the data are. In other words, how well does that
\emph{typical score} represent the entire dataset? In chapter three, we
will learn how to visually represent data. For now, we are going to
focus on different ways to summarize data with a single value, which is
known as the \emph{central tendency} of the data.

\section{What is Central Tendency?}\label{what-is-central-tendency}

\textbf{Central tendency} refers to the statistical measure that
identifies the single value or point that best represents a dataset's
center, or typical, value. The most common measures of central tendency
are the mean (average), median (middle value), and mode (most frequent
value). These measures are used to provide a reasonably accurate
description, or summary, of a group (think of a whole class, grade
level, school, etc.). How do you know when to use the mean, median, or
mode to describe a dataset? Let's walk through an example.

A teacher gave a quiz to 8 students. Which measure of central tendency
would be the most appropriate for summarizing the following scores?

\[
13, 14, 10, 18, 11, 10, 16, 15. 
\]

Let's use the three different measures of central tendency introduced
above to summarize the scores and then decide which one provides the
best description of the data. The first step is to reorder the scores
from smallest to largest:

\[
10, 10, 11, 13, 14, 15, 16, 18 
\]

The \textbf{mode} is simply the most frequently occurring number in a
dataset. The most frequently occurring number in this example is 10 as
it appears twice, and other numbers only appear once.

The \textbf{median} is the center point in a set of numbers listed in
numerical order. It is also the 50th percentile. Sometimes the median is
referred to in terms of depth (i.e., the value halfway into the ordered
score distribution). The median provides a measure that is less
influenced by extreme values compared to the mean.

\begin{itemize}
\item
  To calculate the median by hand, you first put your numbers in
  ascending or descending order, then check to see which of the
  following two rules apply:

  \begin{itemize}
  \item
    Rule One. If you have an odd number of values, the median is the
    center number (e.g., 3 is the median for the numbers 1, 1, 3, 4, 9).
  \item
    Rule Two. If you have an even number of numbers, the median is the
    average of the two innermost numbers (e.g., 2.5 is the median for
    the numbers 1, 2, 3, 7).
  \end{itemize}
\item
  We can always define the median for N ordered raw scores with this
  formula: \(\frac{(N + 1)}{2}\) We have an even number of scores (rule
  2) for our score data, so the median is the \(\frac{9 + 1}{2} = 4.5\)
  \emph{th} ordered value.

  \begin{itemize}
  \item
    The fourth value is 13. The fifth value is 14.
  \item
    (13 + 14)/2 = 13.5
  \item
    The median of our dataset is 13.5
  \end{itemize}
\end{itemize}

The \textbf{mean} is the most commonly reported measure of central
tendency in educational statistics. It is the arithmetic average of a
dataset. More specifically, a sample mean, often denoted as \(\bar{X}\)
can be defined as \(\bar{X} = \frac{\sum_{i=1}^{n} x_i}{n}\) where
\emph{n} represents the number of cases and \(x_i\) represents the
\emph{i}th value of variable \emph{x}. In other words,
\(\sum_{i=1}^{n} x_i\) means the sum of first to \emph{n}th values of x.
The sample mean is said to estimate \(\mu\) which is the population
mean. For our score data, the mean is: \[
\bar{X} = \frac{10 + 10 + 11 + 13 + 14 + 15 + 16 + 18}{8}
= \frac{107}{8}
= 13.4
\]

The mean provides a central value that represents the overall
distribution, but it can be heavily influenced by outliers or extreme
values.

There are \emph{five key characteristics of the mean} that are typically
important when dealing with statistics in educational research:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Adding a new score or removing an existing score will change the mean,
  unless the value is equal to the mean. For example, adding a score of
  1 to our dataset would change the mean to 12.
\item
  Changing an existing score will change the mean. For example, changing
  the first 10 in our dataset to 6 would change the mean to 12.9.
\item
  Adding, subtracting, multiplying, or dividing each score by a constant
  will change the mean by that constant. For example, if we added 100 to
  each score in our sample, the new mean would be 113.4.
\item
  The sum of the differences of scores from their mean is equal to 0.
  The table below illustrates this for our example. Note that the exact
  mean of 13.375 is used to avoid rounding error.

  \textbf{Sum of the Difference of Scores from the Mean}

  \begin{longtable}[]{@{}rr@{}}
  \toprule\noalign{}
  \(x_i\) & \(x_i - \bar{X}\) \\
  \midrule\noalign{}
  \endhead
  \bottomrule\noalign{}
  \endlastfoot
  \(10\) & \(10 - 13.375 = -3.375\) \\
  \(10\) & \(10 - 13.375 = -3.375\) \\
  \(11\) & \(11 - 13.375 = -2.375\) \\
  \(13\) & \(13 - 13.375 = -0.375\) \\
  \(14\) & \(14 - 13.375 = 0.625\) \\
  \(15\) & \(15 - 13.375 = 1.625\) \\
  \(16\) & \(16 - 13.375 = 2.625\) \\
  \(18\) & \(18 - 13.375 = 4.625\) \\
  \textbf{\(\sum x_i = 107\)} & \textbf{\(\sum (x_i - \bar{X}) = 0\)} \\
  \end{longtable}
\item
  The sum of squared distances of scores from the mean is minimal,
  meaning no other value would lead to a lower sum of squared distances.
  Using squared distances will always produce a positive number. The
  larger the final product, the further scores deviate from the mean.
  The smaller the final product, the less scores deviate from the mean.

  \textbf{Sum of Squared Difference of Scores from the Mean}

  \begin{longtable}[]{@{}rr@{}}
  \toprule\noalign{}
  \(x_i\) & \((x_i - \bar{X})^2\) \\
  \midrule\noalign{}
  \endhead
  \bottomrule\noalign{}
  \endlastfoot
  \(10\) & \((10 - 13.4)^2 = 11.56\) \\
  \(10\) & \((10 - 13.4)^2 = 11.56\) \\
  \(11\) & \((11 - 13.4)^2 = 5.76\) \\
  \(13\) & \((13 - 13.4)^2 = 0.16\) \\
  \(14\) & \((14 - 13.4)^2 = 0.36\) \\
  \(15\) & \((15 - 13.4)^2 = 2.56\) \\
  \(16\) & \((16 - 13.4)^2 = 6.76\) \\
  \(18\) & \((18 - 13.4)^2 = 21.16\) \\
  \textbf{\(\sum x_i = 107\)} &
  \textbf{\(\sum (x_i - \bar{X})^2 = 59.88\)} \\
  \end{longtable}
\end{enumerate}

Often, we only collect data from a subgroup of cases (a sample) from the
entire collection of cases (population). The mean of a subsample of the
data, \(\bar{X}\), or the sample mean, is said to estimate the
population mean. The formula for a population mean is:
\(\mu_x = \frac{\sum x_i}{N}\) The difference between this and the
formula for a sample mean as seen above is ``\emph{n}'' and
``\emph{N}''. The ``\emph{N}'' represents the entire collection of cases
that belong to your target population (e.g., all undergraduate students
at a university) while ``\emph{n}'' represents the sample size you
obtained from the population (e.g., a random sample of 20 undergraduate
students at the university). The difference between \(\bar{X}\) and
\(\mu_x\) is typically not zero and considered to be sampling error.

We have three possible measures of central tendency for our data:

\[
\text{Mode} = 10,  \text{Median} = 13.5,  \text{Mean} = 13.4
\]

Which measure of central tendency do you think is the most appropriate
to summarize how the students performed on the quiz? Why?

\section{Advantages and Disadvantages of the Mode, Median and
Mean}\label{advantages-and-disadvantages-of-the-mode-median-and-mean}

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.3191}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2128}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2553}}
  >{\raggedright\arraybackslash}p{(\linewidth - 6\tabcolsep) * \real{0.2128}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Mode}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Median}
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
\textbf{Mean}
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Advantages} & Represents the most frequently occurring
numberApplicable to nominal dataNot affected by outliers & Not affected
by outliersDoes not assume the scale is interval & Can be manipulated
algebraically (i.e., inferential analyses are possible)Uses all data to
computeAllows inference about population characteristicsMore stable with
large data \\
\textbf{Disadvantages} & Does not represent the entire collection of
data & Difficult to apply inferential analysis to the median &
Influenced by outliersRequires interval properties of the data \\
\end{longtable}

The word outlier comes up a few times in the prior table, and it means a
score that is much higher than the next highest score, or much lower
than the next lowest score in your dataset. For example, imagine that
instead of our original dataset, the scores looked like this:

\[
10, 10, 11, 13, 14, 15, 16, 50 
\]

In this case, the mode and median stay the same, but the mean changes to
17.4. With the first dataset, either the median or the mean would have
been an appropriate measure of central tendency. Why? We would probably
use the mean due to the advantages listed in the table above, but
clearly the results were very similar. However, this second dataset has
an outlier -- one student who managed to score a 50. That single score
has a major influence on the mean, so much in this case that the mean is
higher than the second highest quiz score. You can imagine the outlier
\emph{pulling} the dataset up (or to the right), which makes the mean a
less useful measure to summarize our dataset. That \emph{pulling} is
referred to as skewness, or lack of symmetry in the dataset. In the
second example, we would use the median to describe the dataset.

\section{Skewness and Measures of Central
Tendency}\label{skewness-and-measures-of-central-tendency}

We will learn a lot more in the next few chapters about distribution and
the normal curve. For now, just know that if you plot all the numeric
data points from a dataset and drew a curved line to connect them, it
would look like one of the examples below. The curve in the middle is
\emph{normal}, which has many important properties. For now, we care
about the extent to which the curve is skewed, or pulled to one side or
the other. A \textbf{negatively skewed} curve will be pulled to the
left, meaning the longer tail of the curve is on the left side. A
\textbf{positively skewed} curve will be pulled to the right, meaning
the longer tail of the curve is on the right side. The dataset above
(10, 10, 11, 13, 14, 15, 16, 50) would be positively skewed because the
outlier of 50 would pull the curve to the right.

(fig 2.1) (fig 2.2) (fig 2.3)

Note that when a variable is perfectly normally distributed, the mean,
median, and mode are the same number. When the distribution is skewed to
the left (i.e., negatively skewed), the mean shifts to the left the
most, the median shifts to the left the second most, and the mode is the
least affected by the presence of skew in the data. Therefore, when the
data are negatively skewed, mean \textless{} median \textless{} mode.

When the variable is skewed to the right (i.e., positively skewed), the
mean is shifted to the right the most, the median is shifted to the
right the second most, and the mode is the least affected. Therefore,
when the data are positively skewed, mean \textgreater{} median
\textgreater{} mode. Again, this is what happened with our second
dataset example (10, 10, 11, 13, 14, 15, 16, 50).

If you go to the end of the curve, to where it is pulled out the most,
you will see that the order goes mean, median, and mode as you ``walk up
the curve'' for negatively and positively skewed curves. \,

You can use the following two rules to provide some information about
skewness, even when you cannot see a line graph on the data (i.e., all
you need is the mean and the median):

\textbf{Rule One:} \emph{If the mean is less than the median, the data
are skewed to the left.}

\textbf{Rule Two:} \emph{If the mean is greater than the median, the
data are skewed to the right.}

\section{Modal Distributions}\label{modal-distributions}

Any distribution in which one or more numbers occur most often can be
referred to as a modal distribution. We typically describe these
distributions in one of four ways. \textbf{Unimodal distributions} have
a single mode, or single score that occurs most often, and take the
shape of a curve. The normal distribution, or bell curve, is a good
example. \textbf{Bimodal distributions} have two modes, and the mean and
median are typically located between the humps. \textbf{Multimodal
distributions} have more than two modes.

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{chapter2_files/figure-pdf/unnamed-chunk-1-1.pdf}}

}

\caption{Figure 2.4: Example of a Bimodal Distribution}

\end{figure}%

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{chapter2_files/figure-pdf/unnamed-chunk-2-1.pdf}}

}

\caption{Figure 2.5: Example of a Multimodal Distribution}

\end{figure}%

\textbf{Nonmodal} or \textbf{uniform distributions} have no mode because
every value is the same. As an example, 12 people take a test with
scores ranging from 1 to 4. Three people each earn scores of 1, 2, 3,
and 4. There is no mode, but the mean score is 2.5 and the mode is 2.5 -
either would be an appropriate description of central tendency.

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{chapter2_files/figure-pdf/unnamed-chunk-3-1.pdf}}

}

\caption{Figure 2.6: Example of Nonmodal or Uniform Distribution}

\end{figure}%

\section{Conclusion}\label{conclusion}

This chapter has provided a comprehensive overview of measures of
central tendency, emphasizing the significance of the mean, median, and
mode in summarizing data. Understanding these measures is essential for
accurately interpreting datasets and making informed decisions in
educational research and statistical analysis. The chapter highlighted
the impact of skewness on central tendency and the importance of
selecting the most appropriate measure based on data distribution. By
mastering these concepts, readers can develop a strong foundation in
statistical reasoning, enabling them to analyze and communicate data
effectively. Moving forward, the next chapter will explore variability,
providing further insight into how data spreads around these central
measures.

\subsection{Key Takeaways for Educational Researchers from Chapter
2}\label{key-takeaways-for-educational-researchers-from-chapter-2}

\begin{itemize}
\item
  The mean, median, and mode are the three main measures of central
  tendency used to summarize a dataset. Mode represents the most
  frequently occurring value in a dataset; useful for nominal data and
  not affected by outliers. Median is the middle value in a dataset when
  ordered and less influenced by outliers than the mean. Mean is the
  arithmetic average computed all data in the variable, but sensitive to
  outliers.
\item
  Adding, removing or altering value in the dataset will directly impact
  the mean, but not necessarily the median or mode.
\item
  Choosing an appropriate measure of central tendency depends on the
  shape of score distribution. As a general rule, use the mean for
  symmetric data, the median for skewed data, and the mode for nominal
  or categorical data.
\item
  Understand the context of data and analyze the interested variables in
  the data set to decide which measure of central tendency best
  summarize the variables of interest.
\end{itemize}

\section{Key Definitions from Chapter
2}\label{key-definitions-from-chapter-2}

\textbf{Bimodal distributions} are a distribution of scores where two
scores occur most frequently.

\textbf{Central tendency} refers to the statistical measure that
identifies the single value or point that best represents a dataset's
center, or typical, value.

\textbf{Mean} is the arithmetic average of a dataset. The sum of a set
of scores in a distribution, divided by the total number of scores.

\textbf{Median} is the center point in a set of numbers listed in
numerical order.

\textbf{Mode} is simply the most frequently occurring number in a
dataset.

\textbf{Multimodal distributions} have more than two modes.

\textbf{Negatively skewed} data have a mean that is less than the
median. When graphed, the left tail is longer than the right tail.

\textbf{Nonmodal, or uniform, distributions} have no mode because every
value is the same.

An \textbf{outlier} is a score that is much higher than the next highest
score, or much lower than the next lowest score in your dataset.

\textbf{Positively skewed} data have a mean that is greater than the
median. When graphed, the right tail is longer than the left tail.

\textbf{Skewness} means the extent to which a curve is pulled to one
side or another.

\textbf{Unimodal distributions} have a single mode, or single score that
occurs most often.

\section{Check Your Understanding}\label{check-your-understanding}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{A researcher is investigating if any relationship exists
  between the hours a student sleeps at night and their math performance
  across a school district with more than 1,000 students. However, they
  can only survey 50 students. What notation below would be used to
  indicate the number of students surveyed?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    N = 50
  \item
    n = 50
  \end{enumerate}
\item
  \textbf{The \_\_\_\_\_\_ is the value or values in a dataset that
  appear most frequently.}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Mean
  \item
    Median
  \item
    Mode
  \end{enumerate}
\item
  \textbf{An educational researcher records the following sample of
  scores: 1, 2, 4, 4, 4, 5, 5, 7, 8, 10. Calculate the sample mean,
  median, and mode.}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Mean \_\_\_\_\_\_\_\_\_\_
  \item
    Median \_\_\_\_\_\_\_\_
  \item
    Mode \_\_\_\_\_\_\_\_
  \end{enumerate}
\item
  \textbf{An educational researcher obtains the following sample scores:
  1, 3, 3, 4, 6, 100. Using what method of central tendency to describe
  these sample scores might be problematic?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Mean
  \item
    Median
  \item
    Mode
  \end{enumerate}
\item
  \textbf{What direction of skewness is created by the following data?
  1, 1, 2, 2, 10}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Positive skewness
  \item
    Negative skewness
  \item
    No skewness
  \end{enumerate}
\end{enumerate}

\bookmarksetup{startatroot}

\chapter*{Chapter 2: Central Tendency in
R}\label{chapter-2-central-tendency-in-r}
\addcontentsline{toc}{chapter}{Chapter 2: Central Tendency in R}

\markboth{Chapter 2: Central Tendency in R}{Chapter 2: Central Tendency
in R}

This section demonstrates how to calculate and interpret the mean,
median, and mode in R using data from the Programme for International
Student Assessment (PISA). Additionally, we will explore skewness on the
distribution of math achievement scores. All examples below use R code
that can be adapted for your own datasets.

To illustrate the calculations, let's extract plausible value 1 math
achievement (PV1MATH) from the PISA U.S. dataset.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(haven) }\CommentTok{\#to read .sav files }
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{read\_sav}\NormalTok{(}\StringTok{"chapter2/US\_Data\_22.sav"}\NormalTok{) }
 
\NormalTok{math\_achi }\OtherTok{\textless{}{-}}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{PV1MATH }
\FunctionTok{head}\NormalTok{(math\_achi) }\CommentTok{\#display the first few values }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 575.451 442.014 680.277 461.899 501.214 322.342
\end{verbatim}

\section*{1 Calculating the Mean}\label{calculating-the-mean}
\addcontentsline{toc}{section}{1 Calculating the Mean}

\markright{1 Calculating the Mean}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mean\_math\_achi }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(math\_achi) }
\NormalTok{mean\_math\_achi }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 461.8733
\end{verbatim}

\section*{2 Calculating the Median}\label{calculating-the-median}
\addcontentsline{toc}{section}{2 Calculating the Median}

\markright{2 Calculating the Median}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{median\_math\_achi }\OtherTok{\textless{}{-}} \FunctionTok{median}\NormalTok{(math\_achi) }
\NormalTok{median\_math\_achi }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 457.907
\end{verbatim}

\section*{3 Calculating the Mode}\label{calculating-the-mode}
\addcontentsline{toc}{section}{3 Calculating the Mode}

\markright{3 Calculating the Mode}

Since R does not have a built-in function for mode, we will define a
custom function.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Custom function to calculate mode }
\NormalTok{calculate\_mode }\OtherTok{\textless{}{-}} \ControlFlowTok{function}\NormalTok{(x) \{ }
\NormalTok{  uniq\_x }\OtherTok{\textless{}{-}} \FunctionTok{unique}\NormalTok{(x) }\SpecialCharTok{|\textgreater{}} \FunctionTok{sort}\NormalTok{() }
\NormalTok{  uniq\_x[}\FunctionTok{table}\NormalTok{(x) }\SpecialCharTok{|\textgreater{}} \FunctionTok{which.max}\NormalTok{()] }
\NormalTok{\} }
 
\CommentTok{\# Calculate the mode }
\CommentTok{\#the data are rounded to the nearest whole number to calculate the mode }
\NormalTok{mode\_math\_achi }\OtherTok{\textless{}{-}} \FunctionTok{calculate\_mode}\NormalTok{(math\_achi }\SpecialCharTok{|\textgreater{}} \FunctionTok{round}\NormalTok{()) }
\NormalTok{mode\_math\_achi }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 415
\end{verbatim}

\section*{4 Examining Skewness}\label{examining-skewness}
\addcontentsline{toc}{section}{4 Examining Skewness}

\markright{4 Examining Skewness}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(psych) }\CommentTok{\#for skew function }
\NormalTok{skewness\_math\_achi }\OtherTok{\textless{}{-}} \FunctionTok{skew}\NormalTok{(math\_achi, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{) }
\NormalTok{skewness\_math\_achi }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.1730788
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Visualize the distribution }
\FunctionTok{hist}\NormalTok{(math\_achi, }\AttributeTok{main =} \StringTok{"Distribution of Math Scores"}\NormalTok{, }\AttributeTok{xlab =} \StringTok{"Math Scores"}\NormalTok{,  }
     \AttributeTok{col =} \StringTok{"lightblue"}\NormalTok{, }\AttributeTok{border =} \StringTok{"white"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{chapter2_files/figure-pdf/unnamed-chunk-9-1.pdf}}

Recall that:

\begin{itemize}
\item
  a positive skewness value indicates a right-skewed distribution
  (longer tail on the right), which is consistent with the histogram
  above.
\item
  when the data are right-skewed, the mean is greater than the median,
  and the mode is less than the median. These are consistent with what
  we observed in the calculations above.
\end{itemize}

\bookmarksetup{startatroot}

\chapter{Chapter 3}\label{chapter-3}

\bookmarksetup{startatroot}

\chapter*{Exploring Data Spread: Measures of
Variability}\label{exploring-data-spread-measures-of-variability}
\addcontentsline{toc}{chapter}{Exploring Data Spread: Measures of
Variability}

\markboth{Exploring Data Spread: Measures of Variability}{Exploring Data
Spread: Measures of Variability}

\section*{Learning Objectives}\label{learning-objectives-1}
\addcontentsline{toc}{section}{Learning Objectives}

\markright{Learning Objectives}

By the end of this chapter, you will be able to:

\begin{itemize}
\item
  Define and explain the concept of variability and its importance in
  statistical analysis, distinguishing between measures such as range,
  variance, and standard deviation.
\item
  Calculate measures of variability (range, interquartile range,
  variance, and standard deviation) for a given dataset and interpret
  what these values indicate about the spread of the data.
\end{itemize}

In this chapter, we are continuing the discussion about summarizing
data. In the last chapter, we discussed how we can use one statistic to
represent the central tendency of a dataset. Most people are likely
already familiar with the concept of using an average, or the mean, to
summarize data. In this chapter, we will explore how we can use other
statistics to represent the spread, or variability, in a dataset.
Variability refers to the extent to which data points in a dataset
differ from each other and/or from the central value (e.g., the mean).
It is a measure of how spread out or dispersed the data are. High
variability indicates that the data points are more spread out, while
low variability means they are closer together. A measure of variability
should be reported along with any measure of central tendency because
they provide different, but complementary information.

Variability is often a little tricky to understand, so let's look at two
simple datasets to help with our learning.

\begin{itemize}
\item
  \textbf{Set A}: 47, 48, 49, 50, 51, 52, 53
\item
  \textbf{Set B}: 20, 30, 40, 50, 60, 70, 80
\end{itemize}

The mean for each dataset is 50, but these sets of numbers are quite
different! The numbers in Set B are more spread out from the center,
which is another way of saying they have greater variability. If we only
report the mean for each dataset, then we have really not summarized the
data well.

Before we talk about the different measures of variability, the table
below includes some examples of variables with high, low, and no
variability.

High variability

\begin{itemize}
\tightlist
\item
  Semester tuition rates across all public and private colleges and
  universities in the United States.
\item
  Salaries of college graduates entering the workforce.
\end{itemize}

Low variability

\begin{itemize}
\tightlist
\item
  Starting salaries of state employees in similar positions.
\item
  High school GPAs of students who are accepted to elite private
  colleges.
\item
  Standardized math scores for 8th grade students in a gifted education
  program.
\end{itemize}

No variability

\begin{itemize}
\tightlist
\item
  Grade level (for a program that only includes 5th graders).
\item
  Fixed-rate mortgage payments over the course of a loan.
\item
  Enrollment fee for a program in dollars.
\end{itemize}

\section{Measures of Variability}\label{measures-of-variability}

\textbf{\emph{note: Perhpas splitting into sections would be nice
here?}}

Just like with central tendency, there are several different ways to
measure and report the variability of a dataset. We introduce four
different measures of variability: range, interquartile range, standard
deviation and variance.

A relatively crude or simple indicator of variability is the
\textbf{range}. The range is a measure of variability that represents
the difference between the largest and smallest values in a dataset. It
provides a simple indication of how spread out the data are. For
example, the range in Set A above is 6 (the range = the largest value --
the smallest value, i.e., \(53 - 47 = 6\)), and the range in Set B is
\(60\) (\(80 - 20\)). You can see that even if we only knew the range,
we would get a sense that these two datasets were different.

However, a major limitation of range is that it is affected by outliers.
Let's insert a third dataset that looks like this:
\(0, 56, 57, 58, 59, 60, 60\). We still have a mean of \(50\) and a
range of \(60\), but the range seems misleading. That \(0\) in our third
dataset is a complete outlier because it is far below the rest of the
values. It could be an odd case, or even a typo, but it changes the
range of our third dataset from \(4\) to \(60\), completely changing our
understanding of what the data look like.

\textbf{Interquartile range}, or \textbf{IQR}, is the difference between
the first and third quartiles, or the 25th and 75th percentiles. Much
like how the median was better than the mean when the data included
major outliers, the IQR is better than the range in the same
circumstances, because it is less likely to be affected by extreme
values.

To find the IQR, start by organizing your dataset in ascending order.
Next, divide the data into two halves to calculate the first quartile
(\(Q_1\)) and third quartile (\(Q_3\)). The first quartile (\(Q_1\)) is
the median of the lower half of the data, representing the 25th
percentile, while the third quartile (\(Q_3\)) is the median of the
upper half of the data, representing the 75th percentile. The IQR is
then calculated as \(Q_3\)−\(Q_1\), which measures the range of the
middle 50\% of the data, providing a robust indicator of variability
that is less sensitive to outliers. Let's compute the IQR with a sample
set of data.

Suppose you have the dataset: \(3, 7, 8, 12, 13, 15, 16, 20, 21\).

\begin{itemize}
\item
  We first find the median.

  \begin{itemize}
  \tightlist
  \item
    Since we have 9 values, the median is the fifth value:
    \((9+1)/2 = 5\). The fifth value is \(13\). The median of this
    dataset is \(13\).
  \end{itemize}
\item
  We next find the median of the lower half of the dataset. Since we
  have four values, we will need to add the two middle values
  (\(7 + 8\)) and then divide by \(2\) to get \(7.5\). The first
  quartile is equal to \(7.5\).
\item
  We then find the median of the upper half of the dataset. Since we
  again have four values, we will need to add the two middle values
  (\(16 + 20\)) and divide by \(2\) to get \(18\). The third quartile is
  equal to \(18\).
\item
  Last, we find the difference between the third quartile and first
  quartile (\(18 – 7.5\)), which is equal to \(10.5\). Thus, the IQR for
  this example is \(10.5\).
\end{itemize}

\textbf{Standard deviation (SD)} tells us how far away, on average, each
individual data point is from the mean. In other words, is this a
dataset where most of the individual points are clustered close
together, so each point is very close to the mean (small standard
deviation)? Or are the individual points spread much farther apart, so
any given value could be quite different from the mean (large standard
deviation)?

Think about how you might figure out the standard deviation if you had
to invent statistics yourself. It was stated earlier that it is how far
away each data point is, on average. We know that we find an average by
adding up a bunch of values and dividing by the number of values, right?
So, in theory, we could find the mean, then take every single individual
data point, and figure out how far away they are from the mean. Add them
all up, divide by the number of data points, and then we have the
average distance away, right?

Not quite, due to one problem -- negative numbers. By definition, some
data points are going to be above the mean and some will be below. Say
you have a mean of 50, and one data point is 45 while another is 55. If
we take the differences and add them up, we get -5 + 5 = 0. This is not
helpful at all. So, what do we do? Think back to the last chapter when
we were learning about the mean. The mathematical way to solve this
problem is to square the differences, which results in only positive
numbers.

This brings us to \textbf{variance}, which is a measure of variability
that indicates how far the data points in a dataset are spread out from
the mean. It is calculated as the average of the squared differences
between each data point and the mean, and the equation is listed in the
next table. By squaring the differences, variance gives greater weight
to larger deviations, making it sensitive to extreme values in the
dataset. While variance effectively quantifies dispersion, it is
expressed in squared units of the original data, which can make it less
intuitive for interpretation compared to the standard deviation.
Variance is a fundamental concept in statistics, as it forms the basis
for other measures of variability, such as the standard deviation, and
is crucial in fields like probability, regression, and hypothesis
testing.

Let's calculate the variance for a very simple dataset containing just
four numbers: \(5, 7, 9, 11\).

\begin{itemize}
\item
  The mean for our dataset is \(8\):

  \begin{itemize}
  \tightlist
  \item
    \((5 + 7 + 9 + 11)/4 = 8\)
  \end{itemize}
\item
  Subtract the mean from each of our values and square the result:

  \begin{itemize}
  \item
    \(5 – 8 = -3, -3^2 = 9\)
  \item
    \(7 – 8 = -1, -1^2 = 1\)
  \item
    \(9 – 8 = 1, 1^2 = 1\)
  \item
    \(11 – 8 = 3, 3^2 = 9\)
  \end{itemize}
\item
  Sum the squared differences and divide by the number of data points:

  \begin{itemize}
  \tightlist
  \item
    \(9 + 1 + 1 + 9 = 20, 20/4 = 5\)
  \end{itemize}
\item
  The variance for our dataset is \(5\).
\end{itemize}

This concept of the ``sum of squares'' is going to become very important
when we learn topics like Analysis of Variance (ANOVA) in later
chapters. If you find yourself confused later about what ``sum of
squares'' means, just remember that it has to do with variance.

Back to the current topic, we now have a value of 5, the variance, that
tells us the average squared distance each point is from the mean.
However, it is hard to interpret squared values, so what is our last
step? We take the square root of the variance, and we call that the
standard deviation, which is on the same scale as the original variable.
Essentially, we square the differences to deal with negative numbers,
and then we un-square the result so the statistic is easier to work with
for interpretation. This means the standard deviation for our example is
approximately 2.24 (i.e., 5--√ 5

). Any time you see variance and standard deviation reported, the second
value will be the square root of the first.

A couple more notes before we get to the formulas. First, we can find
the variance and standard deviation for a population or a sample, and
the notation is different for each (see the following table). Most of
the time we're working with samples, but it's good to know both. Second,
if we only have a sample, then we have to divide by (n -- 1) rather than
N, where N represents the population size and n represents sample size,
respectively. The quantity of (n -- 1) for the sample variance and
standard deviation estimators is known as the ``degrees of freedom'' (or
df). We use the df to obtain an unbiased, or more accurate, estimate of
the population parameter when we only have a sample to analyze.

The table below shows how to calculate the variance and standard
deviation by hand, with the correct notations for populations and
samples: \(σ_x^2\) and \(σ_x\) represent population variance and
standard deviation of variable \(x\), respectively. \(s_x^2\) and
\(s_x\) represent sample variance and standard deviation of variable
\(x\), respectively. \(\mu\) is a population mean and \(\bar{x}\) is a
sample mean of variable \(x\).

\begin{longtable}[]{@{}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}
  >{\raggedright\arraybackslash}p{(\linewidth - 4\tabcolsep) * \real{0.3333}}@{}}
\toprule\noalign{}
\begin{minipage}[b]{\linewidth}\raggedright
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Population (\(\sigma^2,\,\sigma\))
\end{minipage} & \begin{minipage}[b]{\linewidth}\raggedright
Sample (\(s^2,\,s\))
\end{minipage} \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
\textbf{Variance} & \(\sigma_x^2=\dfrac{\sum (x_i-\mu)^2}{N}\) &
\(s_x^2=\dfrac{\sum (x_i-\bar{x})^2}{n-1}\) \\
\textbf{StandardDeviation} &
\(\sigma_x=\sqrt{\dfrac{\sum (x_i-\mu)^2}{N}}\) &
\(s_x=\sqrt{\dfrac{\sum (x_i-\bar{x})^2}{n-1}}\) \\
\end{longtable}

\section{Characteristics of
Variability}\label{characteristics-of-variability}

Here is a summary of the characteristics of data variability measures.

\begin{itemize}
\item
  Higher values indicate a larger amount of variability than lower
  numbers.
\item
  When you have no variability, the numbers are constant (e.g., for the
  dataset 3, 3, 3, 3, 3, 3, the variance and standard deviation equal
  0).
\item
  The variance tells you the average distance from the mean, in squared
  units.
\item
  The standard deviation is just the square root of the variance (i.e.,
  it brings the squared units back to regular units).
\item
  The standard deviation tells you approximately how far the numbers
  tend to vary from the mean. Making a judgment about how large or small
  the standard deviation is depends entirely on the unit of measurement
  for your variable.

  \begin{itemize}
  \item
    If the entire range of scores is 1 to 100, then you can say a
    dataset with a standard deviation of 20 has more variability than a
    dataset with a standard deviation of 10.
  \item
    However, a standard deviation of 10 on a dataset that ranges from 1
    to 100 could indicate much greater variability than a standard
    deviation of 20 on a dataset that ranges from 1 to 1,000.
  \end{itemize}
\end{itemize}

\section{Standard Deviation and the Normal
Curve}\label{standard-deviation-and-the-normal-curve}

We are going to learn a lot more about the normal curve, or bell curve,
later but many current and future educators already have at least some
knowledge of the concept. Thus, let's focus on how the normal curve
relates to standard deviation.

If data are normally distributed, then an easy rule to apply to the data
is the \textbf{empirical rule}, or \emph{68/95/99.7 percent rule}. That
is:

\begin{itemize}
\item
  Approximately 68\%, or just over two-thirds, of all individual cases
  will fall within one standard deviation of the mean, either above or
  below.
\item
  Approximately 95\% of cases will fall within two standard deviations
  of the mean.
\item
  Approximately 99.7\% of cases will fall within three standard
  deviations of the mean.
\end{itemize}

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{chapter3_files/figure-pdf/fig-empirical-rule-1.pdf}}

}

\caption{\label{fig-empirical-rule}Figure 3.1 Normal Distribution
Showing the Empirical Rule}

\end{figure}%

When you hear the term standardized tests, what that usually means is
that the test has been standardized to fit the normal distribution. If
you hear that a national standardized test has a mean of 500 and a
standard deviation of 100, then you can apply the empirical rule and
know that 68\% of test takers score between 400 and 600 points.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{chapter3_files/figure-pdf/fig-standardized-test-scores-1.pdf}}

}

\caption{\label{fig-standardized-test-scores}Figure 3.2 Normal
Distribution of Standardized Test Scores}

\end{figure}%

\subsection{Common Misconceptions About
Variability}\label{common-misconceptions-about-variability}

Here are some common past misconceptions that students have had about
variability, with brief explanations of the correct understanding for
each concept.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{Misconception}: Variability is undesirable and should be
  minimized or eliminated.

  \textbf{Reality}: Variability is an inherent and necessary component
  of any dataset. In many cases, it offers crucial insights and is
  essential for understanding differences within a dataset and patterns.
\item
  \textbf{Misconception}: Variability in data occurs due to measurement
  errors.

  \textbf{Reality}: Although variability may include measurement error,
  it also reflects genuine differences between observations, which are
  essential for understanding patterns and drawing meaningful
  conclusions.
\item
  \textbf{Misconception}: Low variability is often interpreted as a sign
  of grader accuracy and consistency in the data and considered a
  desired characteristic of data.

  \textbf{Reality}: While low variability may indicate precision, it
  does not necessarily ensure accuracy or representativeness. For
  example, a thermometer that consistently provides incorrect readings
  has low variability but isn't accurate.
\item
  \textbf{Misconception}: Increasing the sample size (e.g., from 10
  cases to 100 cases) eliminates variability in the data.

  \textbf{Reality}: Larger sample sizes reduce sampling variability (the
  differences between sample statistics and the population parameters,
  which is sampling error), but the natural variability in the
  population remains unchanged. This means that, in theory, the sample
  size doesn't affect the inherent variability in data.
\item
  \textbf{Misconception}: It is commonly believed that all measures of
  variability, such as range, variance, standard deviation, or
  interquartile range, tell the same story.

  \textbf{Reality}: Various measures of variability emphasize different
  characteristics of a dataset. For example, the range focuses solely on
  the extremes, whereas the standard deviation accounts for the
  distances of all data points from the mean.
\item
  \textbf{Misconception}: Variability measures, like standard deviation,
  are unrelated to measures of central tendency, like the mean.

  \textbf{Reality}: Variability is closely linked to the central value
  used in its calculation. For instance, variance is calculated relative
  to the mean, highlighting the importance of the chosen measures of
  central tendency in interpreting variability.
\item
  \textbf{Misconception}: Mean and standard deviation values alone give
  enough insights into a dataset.

  \textbf{Reality}: Mean and standard deviation offer valuable
  statistical summaries, but they may miss crucial patterns like extreme
  values (outliers), asymmetric distributions (skewed distributions), or
  multiple peaks in the data. Additional analysis beyond these basic
  measures is often needed for a complete understanding.
\item
  \textbf{Misconception}: High variability indicates that the data may
  be unreliable.

  \textbf{Reality}: Actually, high variability in data doesn't
  necessarily mean the data are untrustworthy. High variability simply
  means there's a wide spread or dispersion in the values. The data
  could be perfectly accurate and reliable while still showing large
  variations.
\item
  \textbf{Misconception}: Knowing standard deviation of a dataset is
  sufficient to understand how data are spread.

  \textbf{Reality}: While standard deviation measures typical distance
  from the mean, it fails to capture the full picture of how data are
  distributed, including the shape, symmetry, and concentration of
  values.
\end{enumerate}

\section{Conclusion}\label{conclusion-1}

In conclusion, this chapter has provided a thorough exploration of
variability, a crucial aspect of statistical analysis that complements
measures of central tendency. By understanding range, interquartile
range, variance, and standard deviation, readers can better assess how
data are spread and how individual data points deviate from the mean.
This chapter emphasized the importance of selecting the appropriate
measure of variability based on data characteristics, ensuring a more
accurate interpretation of statistical findings. Additionally, it
highlighted common misconceptions about variability, reinforcing the
idea that variability is not an error but an essential component of data
analysis. As we move forward, the next chapter will introduce visual
representations of data, providing further insights into how data
distributions can be effectively communicated and understood.

\subsection{Key Takeaways for Educational Researchers from Chapter
3\,}\label{key-takeaways-for-educational-researchers-from-chapter-3}

\begin{verbatim}
- Variability highlights the extent of differences of each data point within a dataset beyond the central value. Two datasets may have the same mean values but differ drastically in their spread, impacting the interpretation of results.  

- High variability in datasets (e.g., 8th graders’ standardized reading test scores across states) may suggest diverse population characteristics. On the other hand, low variability (e.g., standardized test scores within a specialized or sub-group) indicates uniformity, useful for specific interventions.   

- Range can be largely impacted by outliers, while IQR or standard deviation will be less impacted by outliers. 

- Central tendency, such as mean, median and mode, and variability are complementary metrics for understanding how data look. Reporting both metrics (e.g., mean and standard deviation) ensures a more complete summary.  

- Educational researchers should choose measures of central tendency and variability that align with their data characteristics and research questions.
\end{verbatim}

\section{Key Definitions from Chapter
3}\label{key-definitions-from-chapter-3}

The \textbf{empirical rule}, also known as the 68-95-99.7 rule, is a
guideline stating that 68\% of the data falls within 1 standard
deviation of the mean, 95\% of the data falls within 2 standard
deviations of the mean, and 99.7\% of the data falls within 3 standard
deviations of the mean.

\textbf{First quartile} is another name for the median of the lower half
of the values in a dataset (25th percentile).

\textbf{Interquartile range (IQR)} is a measure of variability that
indicates the spread of the middle 50\% of a dataset. It is the
difference between the third quartile and the first quartile.

\textbf{Range} is a measure of variability that represents the
difference between the largest and smallest values in a dataset. It
provides a simple indication of how spread out the data is.

\textbf{Standard deviation (SD)} is the square root of the variance and
is expressed in the same units as the data. It quantifies how much the
individual data points deviate, on average, from the mean of the
dataset.

\textbf{Third quartile} is another name for the median of the upper half
of the values in a dataset (75th percentile).

\textbf{Variability} refers to the extent to which data points in a
dataset differ from each other and from the central value (e.g., the
mean or median). It is a measure of how spread out or dispersed the data
is. High variability indicates that the data points are more spread out,
while low variability means they are closer together.

\textbf{Variance} is a measure of variability that indicates how far the
data points in a dataset are spread out from the mean.

\section{Check Your Understanding}\label{check-your-understanding-1}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{What is the range of the following dataset? (1, 2, 3, 4, 5, 6,
  7, 8, 9, 10)}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
    10
  \item
    9
  \item
    1
  \item
    0
  \end{enumerate}
\item
  \textbf{The \_\_\_\_\_\_ is the square root of the variance?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
    Interquartile Range
  \item
    Range
  \item
    Standard Deviation
  \item
    Mean
  \end{enumerate}
\item
  \textbf{What is the population variance of the following dataset? (1,
  2, 3, 4, 5)}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
    1
  \item
    2
  \item
    2.5
  \item
    3
  \end{enumerate}
\item
  \textbf{What is the population standard deviation of the following
  dataset? (1, 2, 3, 4, 5)}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
    1.41
  \item
    1.73
  \item
    4.00
  \item
    9.00
  \end{enumerate}
\item
  \textbf{What percentage of values fall within 1 SD of the mean if data
  are normally distributed?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
    50\%
  \item
    68\%
  \item
    95\%
  \item
    99.7\%
  \end{enumerate}
\end{enumerate}

\bookmarksetup{startatroot}

\chapter*{Chapter 3: Exploring Data Spread - Measures of Variability in
R}\label{chapter-3-exploring-data-spread---measures-of-variability-in-r}
\addcontentsline{toc}{chapter}{Chapter 3: Exploring Data Spread -
Measures of Variability in R}

\markboth{Chapter 3: Exploring Data Spread - Measures of Variability in
R}{Chapter 3: Exploring Data Spread - Measures of Variability in R}

In this section, we demonstrate how to compute and interpret measures of
variability---\textbf{range, interquartile range (IQR), variance}, and
\textbf{standard deviation}---using math achievement data from the U.S.
sample in the Programme for International Student Assessment (PISA). We
will use the PV1MATH variable (Math Plausible Value 1) as an
illustrative example.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load required package }
\FunctionTok{library}\NormalTok{(haven) }
 
\CommentTok{\# Load the dataset }
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{read\_sav}\NormalTok{(}\StringTok{"chapter3/Clean{-}data\_mar6.sav"}\NormalTok{) }
 
\CommentTok{\# Extract the math plausible value }
\NormalTok{math\_achi }\OtherTok{\textless{}{-}}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{PV1MATH }
\end{Highlighting}
\end{Shaded}

\section*{1 Calculating the Range}\label{calculating-the-range}
\addcontentsline{toc}{section}{1 Calculating the Range}

\markright{1 Calculating the Range}

The range shows the spread between the smallest and largest scores.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{range\_math }\OtherTok{\textless{}{-}} \FunctionTok{range}\NormalTok{(math\_achi) }\CommentTok{\#return the minimum and maximum values }
\NormalTok{range\_math }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 188.352 819.508
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{range\_diff }\OtherTok{\textless{}{-}} \FunctionTok{diff}\NormalTok{(range\_math) }\CommentTok{\#calculate the difference between the two values }
\NormalTok{range\_diff }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 631.156
\end{verbatim}

\section*{2 Calculating the Interquartile Range
(IQR)}\label{calculating-the-interquartile-range-iqr}
\addcontentsline{toc}{section}{2 Calculating the Interquartile Range
(IQR)}

\markright{2 Calculating the Interquartile Range (IQR)}

IQR represents the middle 50\% of the scores and is more robust to
outliers.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{iqr\_math }\OtherTok{\textless{}{-}} \FunctionTok{IQR}\NormalTok{(math\_achi) }\CommentTok{\# Calculate the IQR }
\NormalTok{iqr\_math }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 137.578
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\#return the 1st quartile (25th percentile), median (50th percentile), and 3rd quartile (75th percentile) }
\FunctionTok{quantile}\NormalTok{(math\_achi, }\AttributeTok{probs =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.25}\NormalTok{, }\FloatTok{0.5}\NormalTok{, }\FloatTok{0.75}\NormalTok{)) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
     25%      50%      75% 
391.3428 457.9070 528.9207 
\end{verbatim}

\bookmarksetup{startatroot}

\chapter*{3 Calculating the Variance and Standard
Deviation}\label{calculating-the-variance-and-standard-deviation}
\addcontentsline{toc}{chapter}{3 Calculating the Variance and Standard
Deviation}

\markboth{3 Calculating the Variance and Standard Deviation}{3
Calculating the Variance and Standard Deviation}

Variance shows the average squared distance from the mean, and the
standard deviation is its square root. By default, R calculates the
\emph{sample} variance and \emph{sample} standard deviation, which
divide by \((n - 1)\):

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{var\_math }\OtherTok{\textless{}{-}} \FunctionTok{var}\NormalTok{(math\_achi) }\CommentTok{\# Calculate the variance }
\NormalTok{var\_math }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 9049.605
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{sd\_math }\OtherTok{\textless{}{-}} \FunctionTok{sd}\NormalTok{(math\_achi) }\CommentTok{\# Calculate the standard deviation }
\NormalTok{sd\_math }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 95.12941
\end{verbatim}

If needed, we can approximate population variance from the sample
variance using the following adjustment:

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{n }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(math\_achi))  }
\NormalTok{pop\_var\_math }\OtherTok{\textless{}{-}}\NormalTok{ var\_math }\SpecialCharTok{*}\NormalTok{ (n }\SpecialCharTok{{-}} \DecValTok{1}\NormalTok{) }\SpecialCharTok{/}\NormalTok{ n  }\CommentTok{\# Convert sample variance to population variance }
\NormalTok{pop\_sd\_math }\OtherTok{\textless{}{-}} \FunctionTok{sqrt}\NormalTok{(pop\_var\_math) }
\end{Highlighting}
\end{Shaded}

\section*{4 Visualizing the Distribution with Mean and
SD}\label{visualizing-the-distribution-with-mean-and-sd}
\addcontentsline{toc}{section}{4 Visualizing the Distribution with Mean
and SD}

\markright{4 Visualizing the Distribution with Mean and SD}

We can use a histogram to examine how the scores are spread around the
mean.

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(ggplot2) }
\NormalTok{mean\_math }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(math\_achi) }
 
\CommentTok{\# Plot with mean and ±1 SD }
\FunctionTok{ggplot}\NormalTok{(}\FunctionTok{data.frame}\NormalTok{(math\_achi), }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ math\_achi)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{binwidth =} \DecValTok{10}\NormalTok{, }\AttributeTok{fill =} \StringTok{"lightblue"}\NormalTok{, }\AttributeTok{color =} \StringTok{"white"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{geom\_vline}\NormalTok{(}\AttributeTok{xintercept =}\NormalTok{ mean\_math, }\AttributeTok{color =} \StringTok{"red"}\NormalTok{, }\AttributeTok{linetype =} \StringTok{"solid"}\NormalTok{, }\AttributeTok{size =} \DecValTok{1}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{geom\_vline}\NormalTok{(}\AttributeTok{xintercept =}\NormalTok{ mean\_math }\SpecialCharTok{+}\NormalTok{ sd\_math, }\AttributeTok{color =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{geom\_vline}\NormalTok{(}\AttributeTok{xintercept =}\NormalTok{ mean\_math }\SpecialCharTok{{-}}\NormalTok{ sd\_math, }\AttributeTok{color =} \StringTok{"blue"}\NormalTok{, }\AttributeTok{linetype =} \StringTok{"dashed"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Distribution of Math Scores with Mean and ±1 SD"}\NormalTok{, }
       \AttributeTok{x =} \StringTok{"Math Scores (PV1MATH)"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Frequency"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{chapter3_files/figure-pdf/unnamed-chunk-9-1.pdf}}

Let's see how many students scored within 1, 2, and 3 standard
deviations of the mean.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{within\_1sd }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(math\_achi }\SpecialCharTok{\textgreater{}}\NormalTok{ (mean\_math }\SpecialCharTok{{-}}\NormalTok{ sd\_math) }\SpecialCharTok{\&}\NormalTok{ math\_achi }\SpecialCharTok{\textless{}}\NormalTok{ (mean\_math }\SpecialCharTok{+}\NormalTok{ sd\_math)) }
\NormalTok{within\_2sd }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(math\_achi }\SpecialCharTok{\textgreater{}}\NormalTok{ (mean\_math }\SpecialCharTok{{-}} \DecValTok{2}\SpecialCharTok{*}\NormalTok{sd\_math) }\SpecialCharTok{\&}\NormalTok{ math\_achi }\SpecialCharTok{\textless{}}\NormalTok{ (mean\_math }\SpecialCharTok{+} \DecValTok{2}\SpecialCharTok{*}\NormalTok{sd\_math)) }
\NormalTok{within\_3sd }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(math\_achi }\SpecialCharTok{\textgreater{}}\NormalTok{ (mean\_math }\SpecialCharTok{{-}} \DecValTok{3}\SpecialCharTok{*}\NormalTok{sd\_math) }\SpecialCharTok{\&}\NormalTok{ math\_achi }\SpecialCharTok{\textless{}}\NormalTok{ (mean\_math }\SpecialCharTok{+} \DecValTok{3}\SpecialCharTok{*}\NormalTok{sd\_math)) }
\FunctionTok{c}\NormalTok{(within\_1sd, within\_2sd, within\_3sd) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.6612478 0.9611160 0.9989016
\end{verbatim}

\section*{5 Interpreting Variability in Math Achievement
Scores}\label{interpreting-variability-in-math-achievement-scores}
\addcontentsline{toc}{section}{5 Interpreting Variability in Math
Achievement Scores}

\markright{5 Interpreting Variability in Math Achievement Scores}

\subsection*{5.1 5.1 Measures of
Variability}\label{measures-of-variability-1}
\addcontentsline{toc}{subsection}{5.1 5.1 Measures of Variability}

The analysis of math achievement scores (PV1MATH) from the PISA U.S.
dataset provides key insights into the distribution using measures of
variability.

\begin{itemize}
\item
  \textbf{Range} : The math scores in the U.S. sample range from 188.35
  to 819.51, giving a total spread of 631.16 points. This wide range
  suggests a broad performance gap among students, though extreme scores
  may influence this value.
\item
  \textbf{Interquartile Range} : The IQR is 137.58, with the 25th
  percentile at 391.34 and the 75th percentile at 528.92. This indicates
  that the middle 50\% of students scored within this 138-point window,
  providing a more stable summary of typical score spread.
\item
  \textbf{Variance} and \textbf{Standard Deviation}: The sample variance
  is 9049.61, and the standard deviation is 95.13. This tells us that,
  on average, individual scores deviate by about 95 points from the mean
  score.
\end{itemize}

\subsection*{5.2 Visualizing Spread and
Normality}\label{visualizing-spread-and-normality}
\addcontentsline{toc}{subsection}{5.2 Visualizing Spread and Normality}

The histogram confirms a roughly symmetric distribution centered around
the mean, with the bulk of scores falling within one standard deviation.

\begin{itemize}
\item
  About 66.1\% of students scored within 1 SD of the mean (approximately
  between 366.74 and 557.00).
\item
  About 96.1\% fell within 2 SDs, and 99.9\% were within 3 SDs.
\end{itemize}

These proportions align well with the 68-95-99.7\% empirical rule,
indicating that the distribution of math scores is approximately normal.

In summary, these metrics offer a comprehensive picture of performance
variation, helping educational researchers understand both how typical
students perform and how much their scores vary. Such insights are
essential for designing instruction, identifying gaps, and guiding
data-informed decision-making.

\bookmarksetup{startatroot}

\chapter{Chapter 4}\label{chapter-4}

\bookmarksetup{startatroot}

\chapter*{Displaying Data
Graphically}\label{displaying-data-graphically}
\addcontentsline{toc}{chapter}{Displaying Data Graphically}

\markboth{Displaying Data Graphically}{Displaying Data Graphically}

\section*{Learning Objectives}\label{learning-objectives-2}
\addcontentsline{toc}{section}{Learning Objectives}

\markright{Learning Objectives}

By the end of this chapter, you will be able to:

\begin{itemize}
\item
  Identify appropriate graphical methods (e.g., bar graphs, histograms,
  box plots, scatter plots) to summarize and communicate patterns,
  trends, and variability in categorical and continuous data.
\item
  Construct clear, accurate, and meaningful data visualizations that
  effectively communicate key insights while avoiding common pitfalls
  such as distortion, poor labeling, or overcomplicated designs.
\end{itemize}

\textbf{Exploratory data analysis} is an approach that involves
examining datasets to summarize their main characteristics, identify
patterns, and uncover insights, often using visual methods. This
technique is widely used in education-related fields to gain insights
from educational data, improve decision-making, and inform policy and
practice. You could take an entire course about data visualization, so
for now we will only summarize some of the key concepts.

Data visualization refers to the graphical representation or structured
representation of information and data to facilitate understanding and
interpretation. By using charts, graphs, figures and tables, we present
information in a way that highlights trends, outliers, and patterns in
raw data. While graphs and charts are commonly associated with data
visualization in educational research, tables also serve as a
fundamental tool by systematically organizing numerical and categorical
data for easy comparison. Unlike graphical displays, which emphasize
patterns and relationships, tables provide precise values and detailed
breakdowns, making them particularly useful when exact numerical
reporting is required. Understanding how to display data is important
for multiple steps in the research process. You may notice an
interesting pattern that you want to investigate more closely, so
creating well-designed tables and graphs might be the start of deeper
investigation into your research.

Frequently, the first step in data analysis is visualizing the data to
confirm it meets the assumptions of your planned analysis method. For
example:

\begin{itemize}
\item
  If you want to know whether your dependent variable is approximately
  normally distributed, you create a histogram.
\item
  If you want to see if the amount of variation is similar between two
  groups, you create a box plot.
\item
  If you want to see if there is a linear relationship between two
  variables, you create a scatter plot.
\end{itemize}

Displaying your data graphically can also be a critical part of sharing
the results of any analysis, and you want to do so in a way that
communicates your message effectively.

A good graph should

\begin{itemize}
\tightlist
\item
  Show the data with the appropriate visualization (e.g., bar chart,
  scatter plot, histogram) based on the type of data and relationships
  being displayed
\item
  Induce the reader to think about the data being presented
\item
  Get as much information across as possible within limited space (use a
  clean and simple design that enhances clarity without excessive grid
  lines or redundant elements)
\item
  Make large data sets coherent
\item
  Encourage the reader to ask additional questions about the data
\item
  Communicate the findings or your key message about the data
\item
  Maintain uniform styles, colors, and labeling conventions across
  multiple graphs
\end{itemize}

A good graph should NOT

\begin{itemize}
\tightlist
\item
  Distort the data (intentionally or unintentionally)
\item
  Use 3-D effects for a graph plotting two variables
\item
  Go overboard with patterns and effects that distract from the key
  message (overloading a graph with too many data series can make it
  unreadable)
\item
  Have badly labeled axes (number of what???)
\end{itemize}

While graphical representations are powerful for identifying patterns
and trends, there are instances where a structured, numerical
representation is more effective. In cases where precision, exact
comparisons, or detailed breakdowns are needed, tables serve as an
essential tool for organizing and presenting data.

A \textbf{table} is an organized arrangement of data in rows and
columns, designed to systematically display values, frequencies, or
relationships for easy comparison and interpretation. Tables are
essential tools for summarizing data in educational contexts because
they provide a clear, structured way to organize and present
information. They allow educators, administrators, and policymakers to
quickly interpret numerical and categorical data, making it easier to
identify trends, compare variables, and draw meaningful conclusions. For
instance, tables can display student performance across subjects,
attendance rates, or resource allocation across schools, enabling
stakeholders to make data-driven decisions. Unlike graphical displays,
tables provide precise values, making them particularly useful when
exact numbers are necessary for analysis or reporting. Moreover, tables
are versatile, as they can accommodate large amounts of data and be
customized to highlight specific details, such as subgroup comparisons
or longitudinal trends, fostering better understanding and informed
actions in educational settings.

Let's walk through a scenario to illustrate how tables and graphical
displays can be used. A school administrator wants to compare the
average test scores of students across grade levels in one middle
school. The table below shows average scores. It is very easy to see
that the average scores for grades 6, 7, and 8 are 78, 85, and 92,
respectively.

\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Grade Level & Average Test Score \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
6 & 78 \\
7 & 85 \\
8 & 92 \\
\end{longtable}

Another way we could visual this data is through the use of a
\textbf{bar graph}. Bar graphs are particularly effective for displaying
data across discrete categories, such as grade levels, school districts,
or subject areas, making complex information easier to understand. For
example, bar charts can be used to compare average test scores across
grade levels, participation rates in extracurricular activities, or
funding allocations across departments. Their straightforward design
allows educators and administrators to quickly identify patterns,
trends, and disparities, such as gaps in performance or resource
distribution. Bar graphs also help communicate findings effectively to
diverse audiences, including students, parents, and policymakers, as the
visual format is easy to interpret. By simplifying the presentation of
categorical data, bar graphs support data-driven decision-making and
facilitate meaningful discussions about improving educational outcomes.

Bar graphs use vertical bars to represent the data. The height of the
bars often represents the frequencies of some dependent variable for the
categories that are displayed along the X-axis. Note that by tradition,
the X-axis (independent variable) is the horizontal axis and the Y-axis
(dependent variable) is the vertical axis. The following bar graph shows
the same information as the previous table.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{chapter4_files/figure-pdf/fig-avg-test-scores-1.pdf}}

}

\caption{\label{fig-avg-test-scores}Average Test Scores by Grade Level}

\end{figure}%

\textbf{Histograms} summarize the frequency of continuous data that are
grouped. Values of the independent variable are along the X-axis,
grouped by equal intervals. Typically, the height of the rectangle,
measured by the Y-axis, equals the frequency recorded for each interval.
The base of each rectangle begins and ends at the lower and upper
boundaries, seen from left to right, of each interval, and each
rectangle touches adjacent rectangles at the boundaries of each
interval.

It may seem like a bar graph and a histogram are the same, but in the
world of statistics, it matters whether the bars touch each other. A bar
graph, with separated bars, is used for categorical or discrete
variables, or sometimes for variables based on ordinal scales as well.
The separation shows that these are discrete categories rather than
groupings of a continuous variable where the group size could be
adjusted to whatever you want. Unlike bar charts, histograms group data
into intervals, bins, to show the distribution of a dataset, making them
useful for identifying skewness, outliers, or overall patterns in
continuous variables.

Our school administrator now wants to look at the distribution of test
scores for just 7th graders, but they decided to group scores in the
following categories: 50-60, \textless60-70, \textless70-80,
\textless80-90, and \textless90-100. The histogram showing a summary of
the dataset is included below in Figure 4.2.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{chapter4_files/figure-pdf/fig-7th-histogram-1.pdf}}

}

\caption{\label{fig-7th-histogram}Distribution of 7th Grade Test Scores}

\end{figure}%

A \textbf{stem-and-leaf plot} is a graphic display where each individual
score from the original data set is listed in a relatively compact
manner. Common digits shared by all scores are listed to the left, and
the remaining digits for each score are listed to the right. So, the
stems are the first digit(s) and the leaves are the last digit(s).
Rather than giving a frequency count, this is a way of summarizing the
data visually because you can see which stems have the most leaves while
also displaying the actual scores. Stem-and-leaf plots are best suited
for small datasets where all numbers are greater than 0 (Moore, McCabe,
\& Craig, 2021). Here is a stem-and-leaf plot for our example:

\textbf{Stem-and-Leaf Plot of Test Scores}

\begin{verbatim}
5 | 4 5 5 5 6
6 | 3 4 4 5 5 5 5 6 6 6 6 7 7
7 | 3 4 4 4 5 5 5 5 5 6 6 6 6 6 7 7
8 | 3 3 4 4 4 5 5 5 5 5 5 5 6 6 6 6 6 6
9 | 4 4 5 5 5 6 6

Key: 7 | 5 represents a score of 75
\end{verbatim}

A \textbf{box plot}, or box-and-whisker plot, is a graphical
representation of data that shows its central tendency, variability, and
distribution through a box indicating the interquartile range (IQR), a
line for the median, and \emph{whiskers} extending to the smallest and
largest non-outlier values. They are typically oriented vertically, but
the example below is horizontal for ease of displaying the various
parts. At the center of the box plot is a line representing the median
(50th percentile) for the data, surrounded by a box representing the
inter-quartile range (IQR, or the 25th to 75th percentile). Then there
are lines extending out from the box (these are the \emph{whiskers}).
Most often these will extend to the smallest and largest observed points
in the dataset that are within 1.5 times the IQR. If there are outliers
in the dataset that fall beyond the whiskers, they are typically
represented by individual circles.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{chapter4_files/figure-pdf/fig-boxplot-iqr-demo-1.pdf}}

}

\caption{\label{fig-boxplot-iqr-demo}Boxplot anatomy with IQR, 1.5×IQR
and 3×IQR boundaries (using the stem-and-leaf data)}

\end{figure}%

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{chapter4_files/figure-pdf/fig-boxplot-five-number-1.pdf}}

}

\caption{\label{fig-boxplot-five-number}Boxplot of test scores with
five-number summary (using the stem-and-leaf data)}

\end{figure}%

A \textbf{scatterplot} is a graphical representation that displays
individual data points based on two variables, with one variable plotted
along the X-axis and the other along the Y-axis, allowing researchers to
visualize relationships between them. In educational research,
scatterplots are essential for identifying correlations and trends, such
as the relationship between socioeconomic status and academic
performance or hours spent studying and test scores. By highlighting
patterns, such as positive, negative, or no correlation, scatterplots
provide insights into potential causal or influential factors that can
guide further investigation. Additionally, scatterplots help researchers
detect outliers or anomalies, such as students performing significantly
above or below the expected trend, which can inform targeted
interventions or support. This ability to visually explore complex data
relationships makes scatterplots a valuable tool in understanding and
improving educational outcomes. Examples of scatterplots showing each
individual test score are included below in Figure 4.5. (figure 4.5
here)

A \textbf{Q-Q (Quantile-Quantile) plot} is a graphical tool used to
assess whether a dataset follows a specific theoretical distribution,
most commonly the normal distribution. In a Q-Q plot, the quantiles of
the observed data are plotted against the quantiles of the chosen
distribution (e.g., normal distribution). If the points in the plot fall
approximately along a 45-degree reference line, it suggests that the
data conforms to that distribution. Deviations from this line indicate
departures from the expected distribution, such as skewness, kurtosis,
or outliers. Q-Q plots are particularly valuable in educational research
when determining if data, such as student test scores or survey
responses, meet the assumptions required for parametric statistical
tests that we will cover later in the book. For instance, a researcher
analyzing standardized test scores might use a Q-Q plot to check for
normality before conducting a t test or an analysis of variance (ANOVA)
-- we will learn both methods later. While histograms provide a general
sense of distribution, Q-Q plots offer a more precise visual check by
directly comparing observed data with a theoretical model.

Imagine an educational researcher wants to assess whether a sample of
students' math scores follows a normal distribution. The Q-Q plot below
demonstrates how well the data aligns with a normal distribution. Most
points fall along the 45-degree line, indicating normality, though a few
points deviate slightly at the ends, suggesting potential outliers.

(fig 4.6 here)

Including Q-Q plots in your data visualization toolkit enhances your
ability to diagnose data distribution issues, ensuring that your chosen
analytical methods are appropriate and reliable. This tool, combined
with histograms, box plots, and other visualizations, provides a
comprehensive approach to exploratory data analysis.

A \textbf{pie chart} is a circular graph divided into slices, where each
slice represents a proportion of a whole, making it ideal for
visualizing percentages or relative frequencies. In educational
research, pie charts are often used to illustrate the composition of
groups or categories, such as the distribution of students by
socioeconomic status, the percentage of students enrolled in various
extracurricular activities, or the allocation of a school's budget
across different programs. For example, a pie chart could show the
proportion of students achieving different performance levels on a
standardized test, allowing researchers to identify the largest and
smallest groups at a quick glance. Pie charts are particularly effective
for presenting summary data to audiences who may not have extensive
statistical training, as they provide an intuitive and visually engaging
way to communicate the breakdown of categories. However, they are most
useful when there are relatively few categories, as too many slices can
make interpretation challenging. An example showing students enrollment
across grades 6, 7, and 8 from the administrator's middle school is
included in figure 4.7. (FIG 4.7)

\section{Conclusion}\label{conclusion-2}

Chapter 4 emphasizes the importance of data visualization in educational
research, providing an overview of various graphical methods and their
applications. From bar charts and histograms to box plots and
scatterplots, each visualization tool serves a unique purpose in
summarizing and communicating data. These tools help researchers and
educators uncover patterns, trends, and outliers, offering valuable
insights that inform decision-making and policy development. The chapter
also highlights the distinction between categorical and continuous data
representations and underscores the importance of designing graphs that
are clear, accurate, and effective in delivering key messages.

Through practical examples, such as comparing test scores across grade
levels or visualizing the relationship between two continuous variables
like study hours and academic performance, this chapter demonstrates how
graphical displays can reveal meaningful patterns and insights from raw
data. By integrating tables, stem-and-leaf plots, and pie charts
alongside more complex visualizations, educators and researchers are
equipped with the tools to present findings in ways that are both clear
and informative. This foundational understanding of data visualization
sets the stage for exploring deeper analytical techniques and leveraging
data to improve educational outcomes. As we move forward, the next
chapter will explore the normal distribution, an essential statistical
concept that will further refine our ability to interpret data and make
meaningful inferences.

\subsection{Key Takeaways for Educational Researchers from Chapter
4}\label{key-takeaways-for-educational-researchers-from-chapter-4}

\begin{itemize}
\item
  Incorporate data visualization to summarize patterns and trends. In
  educational research, data visualization helps uncover meaningful
  insights from student performance metrics, survey responses, and
  institutional data. Bar graphs, histograms, and scatterplots allow
  researchers to identify trends in student achievement, compare
  subgroup differences, and detect outliers that may indicate unique
  challenges or exceptional performance. By transforming raw numbers
  into graphical representations, educators and policymakers can more
  easily interpret and act on findings.
\item
  Choose the right visualization for your data and analysis needs.
  Different types of data require different graphical techniques to
  accurately convey insights. Choose the correct visualization (e.g.,
  box plots for variability, Q-Q plots for checking normality) based on
  the type of data and analysis requirements to ensure clarity and
  enhance interpretability.
\item
  Ensure clarity and accuracy in graph design. Well-designed graphs
  improve comprehension and reduce the risk of misinterpretation. In
  educational research, this means clearly labeling axes, ensuring
  consistent scales when comparing multiple groups, and avoiding
  misleading visual distortions (e.g., truncating axes to exaggerate
  differences). Additionally, color choices and legends should enhance
  readability, especially when presenting findings to diverse audiences,
  including educators, administrators, and policymakers. A clear,
  accurate graph allows stakeholders to draw valid conclusions and make
  informed decisions.
\item
  Use data visualization for exploratory analysis. Before applying
  statistical models, researchers can use graphical methods to check
  assumptions. For instance, histograms and box plots help assess the
  distribution of student test scores before conducting an ANOVA, while
  scatterplots with trendlines can reveal nonlinear relationships
  between study time and GPA. Detecting patterns early through
  visualization can prevent errors in model selection and improve the
  robustness of conclusions in educational research.
\item
  Incorporate visualizations in research reporting for effective
  communication. Effective visualizations bridge the gap between complex
  analyses and actionable insights. When reporting findings on student
  achievement or the effectiveness of instructional interventions,
  incorporating well-crafted charts, tables, and dashboards can make
  data accessible to decision-makers. Visuals enhance storytelling in
  research, making data-driven recommendations more compelling and
  easier to act upon.
\end{itemize}

\section{Key Definitions from Chapter
4}\label{key-definitions-from-chapter-4}

A \textbf{bar graph} is a graphical representation of data using
rectangular bars, where the length or height of each bar corresponds to
the value or frequency of a specific category or group. There is a gap
between each bar.

A \textbf{box plot} (or box-and-whisker plot) is a graphical
representation of data that shows its central tendency, variability, and
distribution through a box indicating the interquartile range (IQR), a
line for the median, and ``whiskers'' extending to the smallest and
largest non-outlier values.

\textbf{Exploratory data analysis} is an approach that involves
examining datasets to summarize their main characteristics, identify
patterns, and uncover insights, often using visual methods.

\textbf{Histograms} summarize the frequency of continuous data that are
grouped. The bars on a histogram touch one another.

A \textbf{pie chart} is a circular graph divided into slices, where each
slice represents a proportion of a whole, making it ideal for
visualizing percentages or relative frequencies.

A \textbf{scatterplot} is a graphical representation that displays
individual data points based on two variables, with one variable plotted
along the x-axis and the other along the y-axis, allowing researchers to
visualize relationships between them.

A \textbf{stem-and-leaf plot} is a graphic display where each individual
score from the original data set is listed in a relatively compact
manner. Common digits shared by all scores are listed to the left, and
the remaining digits for each score are listed to the right.

A \textbf{Q-Q (Quantile-Quantile) plot} is a graphical tool used to
assess whether a dataset follows a specific theoretical distribution,
most commonly the normal distribution.

A \textbf{table} is an organized arrangement of data in rows and
columns, designed to systematically display values, frequencies, or
relationships for easy comparison and interpretation.

\section{Check Your Understanding}\label{check-your-understanding-2}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{What is the primary purpose of using a histogram in
  educational research?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
    To compare categories of data.
  \item
    To display the relationship between two variables.
  \item
    To summarize the frequency of continuous data grouped into
    intervals.
  \item
    To show proportions of a whole.
  \end{enumerate}
\item
  \textbf{Which visualization method is best suited for identifying
  relationships or correlations between two variables?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
    Pie chart
  \item
    Box plot
  \item
    Scatterplot
  \item
    Stem-and-leaf plot
  \end{enumerate}
\item
  \textbf{In a box-and-whisker plot, the interquartile range (IQR) is
  represented by:}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
    The lines extending from the box to the smallest and largest values.
  \item
    The box itself, showing the range between the 25th and 75th
    percentiles.
  \item
    The line in the center of the box representing the median.
  \item
    The individual dots outside the whiskers.
  \end{enumerate}
\item
  \textbf{What is a key advantage of tables in presenting data?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
    They simplify relationships between variables visually.
  \item
    They display trends and patterns intuitively.
  \item
    They highlight outliers in large datasets.
  \item
    They summarize data with precise values for comparison.
  \end{enumerate}
\item
  \textbf{What is the key difference between a bar chart and a
  histogram?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
    Bar charts use touching bars to represent continuous data, while
    histograms use separated bars for discrete data.
  \item
    Bar charts use separated bars for discrete data, while histograms
    use touching bars for continuous data.
  \item
    Bar charts are only used for percentages, while histograms are used
    for counts.
  \item
    There is no difference; the terms are interchangeable.
  \end{enumerate}
\end{enumerate}

\bookmarksetup{startatroot}

\chapter*{Chapter 4: Displaying Data Graphically in
R}\label{chapter-4-displaying-data-graphically-in-r}
\addcontentsline{toc}{chapter}{Chapter 4: Displaying Data Graphically in
R}

\markboth{Chapter 4: Displaying Data Graphically in R}{Chapter 4:
Displaying Data Graphically in R}

This section demonstrates various data visualization techniques using
the PISA 2022 U.S. dataset. We will illustrate how to effectively
present categorical and continuous variables using \textbf{pie charts,
bar charts, histograms, stem-and-leaf plots, box plots, scatterplot
matrices}, and \textbf{Q-Q plots}.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load required package }
\FunctionTok{library}\NormalTok{(haven) }
\FunctionTok{library}\NormalTok{(ggplot2) }
\FunctionTok{library}\NormalTok{(dplyr) }
\FunctionTok{library}\NormalTok{(car)        }\CommentTok{\# for QQ plot helper if desired }
\FunctionTok{library}\NormalTok{(GGally)     }\CommentTok{\# for scatterplot matrix }
\FunctionTok{library}\NormalTok{(ggrepel)    }\CommentTok{\# for better text labels in plots }
\FunctionTok{library}\NormalTok{(ggpubr)     }\CommentTok{\# for Q{-}Q plot }
 
\CommentTok{\# Load the dataset }
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{read\_sav}\NormalTok{(}\StringTok{"chapter3/Clean{-}data\_mar6.sav"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\section*{1 Pie Chart: Categorical
Distribution}\label{pie-chart-categorical-distribution}
\addcontentsline{toc}{section}{1 Pie Chart: Categorical Distribution}

\markright{1 Pie Chart: Categorical Distribution}

We begin by visualizing the distribution of school governance types
(SC014Q01TA: ``What kind of organization runs your school?'').

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Convert the SPSS‑labelled variable to an R factor }
\NormalTok{data }\OtherTok{\textless{}{-}}\NormalTok{ data }\SpecialCharTok{|\textgreater{}} \FunctionTok{mutate}\NormalTok{(}\AttributeTok{OrgType =} \FunctionTok{as\_factor}\NormalTok{(SC014Q01TA)) }
\CommentTok{\# Create a frequency table and calculate percentages }
\NormalTok{school\_org\_df }\OtherTok{\textless{}{-}}\NormalTok{ data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{count}\NormalTok{(OrgType) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(OrgType)) }\SpecialCharTok{|\textgreater{}} \CommentTok{\# Filter out NA values }
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Percent =} \FunctionTok{round}\NormalTok{(}\DecValTok{100} \SpecialCharTok{*}\NormalTok{ n }\SpecialCharTok{/} \FunctionTok{sum}\NormalTok{(n), }\DecValTok{1}\NormalTok{)) }\CommentTok{\# Calculate percentage }
\NormalTok{school\_org\_df }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 3 x 3
  OrgType                                      n Percent
  <fct>                                    <int>   <dbl>
1 A church or other religious organisation    97     2.3
2 Another not-for-profit organisation        181     4.2
3 The government                            4013    93.5
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Create the pie chart }
\CommentTok{\# Create a label that combines OrgType, Count, and Percent }
\NormalTok{school\_org\_df }\OtherTok{\textless{}{-}}\NormalTok{ school\_org\_df }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{(}\AttributeTok{Label =} \FunctionTok{ifelse}\NormalTok{(Percent}\SpecialCharTok{\textless{}}\DecValTok{5}\NormalTok{, }\StringTok{""}\NormalTok{, }\FunctionTok{paste0}\NormalTok{(}\StringTok{"n="}\NormalTok{, n, }\StringTok{", "}\NormalTok{, Percent, }\StringTok{"\%"}\NormalTok{))) }
  \CommentTok{\#data label appear when percentage is greater than 5\% }
 
\FunctionTok{ggplot}\NormalTok{(school\_org\_df, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \StringTok{""}\NormalTok{, }\AttributeTok{y =}\NormalTok{ n, }\AttributeTok{fill =}\NormalTok{ OrgType)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_bar}\NormalTok{(}\AttributeTok{width =} \DecValTok{1}\NormalTok{, }\AttributeTok{stat =} \StringTok{"identity"}\NormalTok{, }\AttributeTok{color =} \StringTok{"white"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{coord\_polar}\NormalTok{(}\StringTok{"y"}\NormalTok{) }\SpecialCharTok{+} \CommentTok{\#wrapping the bars around a circle, using the y{-}axis (the count) as the angle for each slice }
  \FunctionTok{theme\_void}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{geom\_text}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{label =}\NormalTok{ Label), }
            \AttributeTok{position =} \FunctionTok{position\_stack}\NormalTok{(}\AttributeTok{vjust =} \FloatTok{0.5}\NormalTok{), }
            \AttributeTok{size =} \DecValTok{4}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Distribution of School Governance Types"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{chapter4_files/figure-pdf/unnamed-chunk-3-1.pdf}}

\textbf{Interpretation}: The frequency table and pie chart above
indicates that, within this PISA sample, most students attend
government-run schools (n=4013, 93.5\%), while relatively few are
enrolled in schools managed by religious (n=97, 2.3\%) or other
not-for-profit organizations (n=181, 4.2\%).

\section*{2 Bar Chart: Comparing Mean Math Scores by School
Type}\label{bar-chart-comparing-mean-math-scores-by-school-type}
\addcontentsline{toc}{section}{2 Bar Chart: Comparing Mean Math Scores
by School Type}

\markright{2 Bar Chart: Comparing Mean Math Scores by School Type}

We will now create a bar chart to compare the mean math scores (MATH)
across different school governance types.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Calculate means for each school type }
\NormalTok{mean\_scores }\OtherTok{\textless{}{-}}\NormalTok{ data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{group\_by}\NormalTok{(OrgType) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{summarise}\NormalTok{(}\AttributeTok{MeanMath =} \FunctionTok{round}\NormalTok{(}\FunctionTok{mean}\NormalTok{(PV1MATH, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{))) }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{filter}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(OrgType)) }
 
\CommentTok{\# recode the values for concise display }
\NormalTok{mean\_scores }\OtherTok{\textless{}{-}}\NormalTok{ mean\_scores }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{mutate}\NormalTok{( }
    \AttributeTok{SchoolType =} \FunctionTok{case\_when}\NormalTok{( }
\NormalTok{      OrgType }\SpecialCharTok{==} \StringTok{"A church or other religious organisation"} \SpecialCharTok{\textasciitilde{}} \StringTok{"Religious"}\NormalTok{, }
\NormalTok{      OrgType }\SpecialCharTok{==} \StringTok{"Another not{-}for{-}profit organisation"} \SpecialCharTok{\textasciitilde{}} \StringTok{"Non{-}profit"}\NormalTok{, }
\NormalTok{      OrgType }\SpecialCharTok{==} \StringTok{"The government"} \SpecialCharTok{\textasciitilde{}} \StringTok{"Government"} 
\NormalTok{    ) }
\NormalTok{  ) }
\NormalTok{mean\_scores }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 3 x 3
  OrgType                                  MeanMath SchoolType
  <fct>                                       <dbl> <chr>     
1 A church or other religious organisation      462 Religious 
2 Another not-for-profit organisation           398 Non-profit
3 The government                                464 Government
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Plot bar chart }
\FunctionTok{ggplot}\NormalTok{(mean\_scores, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ SchoolType, }\AttributeTok{y =}\NormalTok{ MeanMath, }\AttributeTok{fill =}\NormalTok{ OrgType)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_col}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{geom\_text}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{label =}\NormalTok{ MeanMath),    }
          \AttributeTok{vjust =} \SpecialCharTok{{-}}\FloatTok{0.1}\NormalTok{,                     }\CommentTok{\# Position labels just above the bar }
          \AttributeTok{size =} \DecValTok{4}\NormalTok{) }\SpecialCharTok{+}                        
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Average Math Score by School Type"}\NormalTok{, }
       \AttributeTok{x =} \StringTok{"School Organization"}\NormalTok{, }
       \AttributeTok{y =} \StringTok{"Mean Math Score"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{theme\_minimal}\NormalTok{() }\SpecialCharTok{+} 
  \FunctionTok{theme}\NormalTok{(}\AttributeTok{legend.position =} \StringTok{"none"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{chapter4_files/figure-pdf/unnamed-chunk-5-1.pdf}}

\textbf{Interpretation}: The bar chart shows that students in
government-run schools have the highest average math score (464),
closely followed by those in schools run by religious organizations
(462). Students in schools operated by other not-for-profit
organizations have a lower average math score (398). \#\# 3 Histogram:
Overall Distribution of Math Scores \{.unnumberred\} We now inspect the
distribution of math scores (PV1MATH) across all students.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Calculate summary statistics }
\NormalTok{mean\_math }\OtherTok{\textless{}{-}} \FunctionTok{mean}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{PV1MATH, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{) }
\NormalTok{sd\_math }\OtherTok{\textless{}{-}} \FunctionTok{sd}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{PV1MATH, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{) }
\NormalTok{n\_math }\OtherTok{\textless{}{-}} \FunctionTok{sum}\NormalTok{(}\SpecialCharTok{!}\FunctionTok{is.na}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{PV1MATH)) }
 
\CommentTok{\# Format summary text }
\NormalTok{summary\_text }\OtherTok{\textless{}{-}} \FunctionTok{paste0}\NormalTok{( }
  \StringTok{"Mean = "}\NormalTok{, }\FunctionTok{round}\NormalTok{(mean\_math, }\DecValTok{1}\NormalTok{),  }
  \StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{SD = "}\NormalTok{, }\FunctionTok{round}\NormalTok{(sd\_math, }\DecValTok{1}\NormalTok{),  }
  \StringTok{"}\SpecialCharTok{\textbackslash{}n}\StringTok{N = "}\NormalTok{, n\_math }
\NormalTok{) }
 
\FunctionTok{ggplot}\NormalTok{(data, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =}\NormalTok{ PV1MATH)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_histogram}\NormalTok{(}\AttributeTok{binwidth =} \DecValTok{25}\NormalTok{, }\AttributeTok{fill =} \StringTok{"lightblue"}\NormalTok{, }\AttributeTok{color =} \StringTok{"white"}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{geom\_vline}\NormalTok{(}\FunctionTok{aes}\NormalTok{(}\AttributeTok{xintercept =}\NormalTok{ mean\_math), }\AttributeTok{color =} \StringTok{"red"}\NormalTok{, }\AttributeTok{size =} \FloatTok{1.2}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Histogram of Math Scores (PV1MATH)"}\NormalTok{, }
       \AttributeTok{x =} \StringTok{"Math Score"}\NormalTok{, }\AttributeTok{y =} \StringTok{"Frequency"}\NormalTok{) }\SpecialCharTok{+} 
    \FunctionTok{annotate}\NormalTok{( }
    \StringTok{"text"}\NormalTok{, }
    \AttributeTok{x =} \FunctionTok{max}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{PV1MATH, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{) }\SpecialCharTok{{-}} \DecValTok{100}\NormalTok{, }\CommentTok{\# Adjust as needed }
    \AttributeTok{y =} \ConstantTok{Inf}\NormalTok{, }\AttributeTok{label =}\NormalTok{ summary\_text, }\AttributeTok{hjust =} \DecValTok{0}\NormalTok{, }\AttributeTok{vjust =} \DecValTok{1}\NormalTok{, }
    \AttributeTok{size =} \DecValTok{4} 
\NormalTok{  ) }\SpecialCharTok{+} 
  \FunctionTok{theme\_minimal}\NormalTok{() }
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{chapter4_files/figure-pdf/unnamed-chunk-6-1.pdf}}

\textbf{Interpretation}: The histogram shows the distribution of U.S.
students' math scores (PV1MATH) on the PISA assessment. The distribution
is approximately symmetric and bell-shaped, centered around a mean of
461.9 with a standard deviation of 95.1.This pattern suggests that math
achievement is fairly normally distributed in the sample. \#\# 4 Q-Q
Plot: Assessing Normality of Math Scores \{.unnumbered\}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{ggqqplot}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{PV1MATH, }
         \AttributeTok{title =} \StringTok{"Q{-}Q Plot of Math Scores (PV1MATH)"}\NormalTok{, }
         \AttributeTok{xlab =} \StringTok{"Theoretical Quantiles"}\NormalTok{, }\AttributeTok{ylab =} \StringTok{"Sample Quantiles"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{chapter4_files/figure-pdf/unnamed-chunk-7-1.pdf}}

\textbf{Interpretation}: The Q-Q plot compares the distribution of math
scores (PV1MATH) to a theoretical normal distribution. Most of the data
points fall close to the reference line, indicating that the
distribution of math scores is approximately normal. However, In the
lower tail, the observed quantiles sit above the reference line
(i.e.\,less extreme than a normal lower tail would predict); in the
upper tail, they lie below the line (again, less extreme than normal).
That pattern at both ends means the math‑score distribution has slightly
lighter tails than a true normal distribution.

Overall, the distribution of math scores appears reasonably normal,
supporting the use of parametric statistical methods for further
analysis, which will be introduced in the following chapters.

\section*{5 Stem-and-Leaf Plot and Box
Plot}\label{stem-and-leaf-plot-and-box-plot}
\addcontentsline{toc}{section}{5 Stem-and-Leaf Plot and Box Plot}

\markright{5 Stem-and-Leaf Plot and Box Plot}

We will create a stem-and-leaf plot and a box plot to further visualize
the distribution of math scores (PV1MATH).

Note that the stem-and-leaf plots are best for small to moderate
datasets (e.g., N \textless{} 100--200). For illustration, we will
randomly select 100 math scores to create the plot.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Stem{-}and{-}leaf plot }
\FunctionTok{set.seed}\NormalTok{(}\DecValTok{12345}\NormalTok{) }
\NormalTok{subset\_scores }\OtherTok{\textless{}{-}} \FunctionTok{sample}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{PV1MATH, }\DecValTok{100}\NormalTok{) }\CommentTok{\# randomly select 100 scores }
\FunctionTok{stem}\NormalTok{(subset\_scores) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

  The decimal point is 2 digit(s) to the right of the |

  2 | 66
  3 | 000133344
  3 | 55667777788888889999
  4 | 00000011122334444
  4 | 55566777999999
  5 | 00011223334444
  5 | 677777888999
  6 | 00111224
  6 | 5556
\end{verbatim}

\textbf{Interpretation}: The stem-and-leaf plot provides a compact
visualization of the distribution of the selected math scores,
displaying the actual data values in a way that preserves information
about the shape and spread.

\begin{itemize}
\item
  \textbf{Range of Scores}: The math scores in this subset range from
  approximately 260 (2 \textbar{} 66) up to around 660 (6 \textbar{}
  5556).
\item
  \textbf{Central Tendency}: The 400s and 500s stems are the most
  densely populated, indicating that most students scored between 400
  and 599.
\item
  \textbf{Skewness and Symmetry}: The plot appears slightly
  right-skewed, evidenced by a longer tail on the higher end (the 600s).
\end{itemize}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Box plot }
\FunctionTok{ggplot}\NormalTok{(data, }\FunctionTok{aes}\NormalTok{(}\AttributeTok{x =} \StringTok{""}\NormalTok{, }\AttributeTok{y =}\NormalTok{ PV1MATH)) }\SpecialCharTok{+} 
  \FunctionTok{geom\_boxplot}\NormalTok{(}\AttributeTok{fill =} \StringTok{"lightblue"}\NormalTok{, }\AttributeTok{width=}\FloatTok{0.3}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{labs}\NormalTok{(}\AttributeTok{title =} \StringTok{"Box Plot of Math Scores (PV1MATH)"}\NormalTok{, }
       \AttributeTok{y =} \StringTok{"Math Score"}\NormalTok{, }\AttributeTok{x =} \ConstantTok{NULL}\NormalTok{) }\SpecialCharTok{+} 
  \FunctionTok{theme\_minimal}\NormalTok{() }
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{chapter4_files/figure-pdf/unnamed-chunk-9-1.pdf}}

\textbf{Interpretation}: The box plot shows that most students' math
scores are clustered in the mid-range, with a few students scoring
exceptionally high. The distribution is slightly right-skewed, and the
presence of upper outliers indicates that some students performed much
better than their peers. \#\# 6 Scatterplot Matrix: Exploring
Relationships Between Math Subscores \{.unnumberred\} We use a
scatterplot matrix to explore relationships between different cognitive
dimensions of math: reasoning (PV1MPRE), change \& relationships
(PV1MCCR), quantity (PV1MCQN).

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Subset relevant variables }
\NormalTok{math\_subscores }\OtherTok{\textless{}{-}}\NormalTok{ data }\SpecialCharTok{|\textgreater{}} 
  \FunctionTok{select}\NormalTok{(PV1MPRE, PV1MCCR, PV1MCQN) }
 
\CommentTok{\# Plot scatterplot matrix }
\FunctionTok{ggpairs}\NormalTok{(math\_subscores, }
        \AttributeTok{title =} \StringTok{"Scatterplot Matrix of Math Cognitive Subscores"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{chapter4_files/figure-pdf/unnamed-chunk-10-1.pdf}}

\textbf{Interpretation}: This scatterplot matrix provides a
comprehensive overview of the relationships between three cognitive
subscores of math achievement:

\begin{itemize}
\tightlist
\item
  Lower panels show scatterplots for each pair of subscores. These plots
  visually reveal strong, positive associations; as one subscore
  increases, the other tends to increase as well.
\end{itemize}

Additionally:

\begin{itemize}
\item
  Diagonal panels display the distribution (density curves) of each
  subscore, giving a quick sense of how each score is spread.
\item
  Upper panels present the numerical correlations for each pair. For
  example, the correlation between PV1MPRE (reasoning) and PV1MCCR
  (change \& relationships) is 0.85, indicating a strong positive
  relationship. We will formally introduce the concept of correlation
  and how to interpret it in a later chapter.
\end{itemize}

\bookmarksetup{startatroot}

\chapter{Chapter 5}\label{chapter-5}

\bookmarksetup{startatroot}

\chapter*{The Normal Distribution}\label{the-normal-distribution}
\addcontentsline{toc}{chapter}{The Normal Distribution}

\markboth{The Normal Distribution}{The Normal Distribution}

\section*{Learning Objectives}\label{learning-objectives-3}
\addcontentsline{toc}{section}{Learning Objectives}

\markright{Learning Objectives}

By the end of this chapter, you will be able to:

\begin{itemize}
\item
  Identify the defining characteristics of a normal distribution.
\item
  Compute z scores from raw data and interpret their meaning in terms of
  standard deviations from the mean.
\item
  Use the properties of a normal distribution to calculate probabilities
  and interpret results for real-world scenarios using z tables and
  statistical software or applets.
\end{itemize}

The objectives for this chapter are to learn about the normal
distribution, sometimes called the bell curve due to its shape, and to
understand the concepts of probability associated with the normal
distribution. An understanding of the normal distribution is very
important because we use what we know about the normal distribution to
make inferences about populations based on samples.

\section{What is a Normal
Distribution?}\label{what-is-a-normal-distribution}

A \textbf{normal distribution}, or bell-shaped distribution, has the
following characteristics:

\begin{itemize}
\item
  The shape is symmetric, unimodal, and bell-shaped.
\item
  The mean, median, and mode are the same---all are right in the center
  of the distribution.
\item
  Following the empirical rule, approximately 68\% of the values in a
  normal distribution fall within 1 standard deviation of the mean,
  approximately 95\% of the values fall within 2 standard deviations of
  the mean, and approximately 99.7\% of the values fall within 3
  standard deviations of the mean.
\end{itemize}

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{chapter5_files/figure-pdf/fig-empirical-rule-normal-1.pdf}}

}

\caption{\label{fig-empirical-rule-normal}Normal Distribution Showing
the Empirical Rule}

\end{figure}%

For example, suppose a class took a science test and their scores were
normally distributed. The distribution of student scores had a mean of
50 points (the center of the normal distribution) and a standard
deviation of 15 points. Based on the empirical rule, you know that about
68\% of the students scored between 35 (1 SD below the mean) and 65 (1
SD above the mean) points.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{chapter5_files/figure-pdf/fig-empirical-rule-green-1.pdf}}

}

\caption{\label{fig-empirical-rule-green}Normal Distribution with
Empirical Rule}

\end{figure}%

It is important to understand the characteristics of the normal
distribution not only because certain variables (e.g., weight, height,
test scores) are likely to have normal distributions, but because many
of the statistical techniques used to make inferences about populations
rely on the properties of the normal distribution.

This rule is something you can use to interpret results from normally
distributed data, but it is also part of what defines a normal
distribution. Not all tests that students take are normally distributed.
In those cases, the empirical rule would not apply.

Software programs like SPSS® and JASP can provide tests of normality
that tell you how close your distribution is to the normal distribution.
You can assess how closely your data approximate normality by examining
two key statistics: skewness and kurtosis.

\begin{itemize}
\item
  As we learned in chapter 2, \textbf{skewness} means the extent to
  which a curve is pulled to one side or the other. Skewness is about
  symmetry, or really, the lack of symmetry. In a true normal
  distribution, half of the data points are above the mean, and half are
  below. A skewed distribution has more outliers on one side of the
  mean.
\item
  Kurtosis measures the \emph{tailedness} of a distribution, or how
  heavy or light the tails are compared to a normal distribution. The
  strict definition can be confusing. For now, think about it in terms
  of how likely it is that there are outliers, or extreme values, in
  your data. A heavy-tailed distribution will have a higher kurtosis
  value, and a greater percentage of scores in the distribution will be
  out in the tails rather than the \emph{shoulders}.
\end{itemize}

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{chapter5_files/figure-pdf/fig-kurtosis-compare-true-1.pdf}}

}

\caption{\label{fig-kurtosis-compare-true}Distributions with
K\textgreater0, K=0, and K\textless0 (true excess kurtosis),
standardized to mean 0 and SD 1}

\end{figure}%

\textbf{Positive Kurtosis} (K \textgreater{} 0): The blue plot
represents a distribution with a narrow peak and thick tails.

\textbf{Zero Kurtosis} (K = 0): The green plot represents a standard
normal distribution with neither exaggerated tails nor an unusually
sharp peak.

\textbf{Negative Kurtosis} (K \textless{} 0): The red plot represents a
distribution with a flatter peak and thinner tails

A true normal distribution will have skewness and kurtosis values of 0.
The closer the reported values are to 0, the more normal the
distribution. A common rule of thumb is that a distribution with
skewness and kurtosis values between -1 and 1 can be considered
approximately normal.

(fig 5.4 here)

A normal distribution can have any mean and any standard deviation -- it
is about the shape of the distribution, not the specific numbers. For
example, if you are looking at the heights of people, the mean and
standard deviation will be quite different depending on whether your
sample includes all people in the United States or all kindergarten
students in one elementary school. Both height distributions will likely
be normal and have the same basic shape, but the average height and
variation in heights will probably both be lower if we are just looking
at kindergartners.

(fig 5.5)

\section{\texorpdfstring{\(z\) Scores and the Standard Normal
Distribution}{z Scores and the Standard Normal Distribution}}\label{z-scores-and-the-standard-normal-distribution}

We often convert raw data values in a normal distribution to z scores,
or standardized scores. A z score is a statistical measure that
describes how far a particular data point is from the mean, center, of
its distribution. Instead of describing the difference between one score
and the mean in terms of points, or inches, or whatever unit was used
for the original distribution, we are using standard deviation units.
This allows for the comparison of scores from different distributions or
scales. We do this because simply knowing the difference between one
score and the overall mean doesn't tell us much. For example, you learn
you scored 5 points above the mean. Is your performance a lot better
than average? Is your performance barely above average? The answer
depends on the standard deviation. If the standard deviation of this
distribution were 20, your score of 5 points above the mean would be
considered close to average. However, if the standard deviation were
only 2, your score would be considered outstanding.

Using the example above, if you scored 5 points above the mean and the
standard deviation was 20, your z score would be 0.25. Your score would
be a quarter of a standard deviation above the mean. However, if the
standard deviation were 2, your z score would be 2.5. In this case, you
would have scored 2.5 standard deviations above the mean. Remember this
graph from chapter 3? Boxes have been added to illustrate our example to
help visualize how far different z scores are from the mean.

(fig 5.6)

If we convert an entire distribution from raw scores to z scores, that
means we are standardizing the dataset. The mean of the distribution
then becomes 0, the standard deviation becomes 1, and we refer to the
distribution as the \textbf{standard normal distribution}.

What we have discussed so far are likely the most important concepts for
you to understand. The rest of this chapter will mostly involve finding
areas under the normal curve by using z scores and the z table. You will
also learn how you can get to the same answer using online applets.

\subsection{Finding Areas Under the Normal
Curve}\label{finding-areas-under-the-normal-curve}

Let's say we have a continuous random variable: body temperature.
Imagine that we have access to the body temperatures of a particular
population (e.g., all residents of the United States), and when we graph
this data, we see a normal distribution with a mean of 98.2 and a
standard deviation of 0.73. Remember that we collected data from
individuals who belong to a particular population (i.e., all Americans).
So, these numbers (\(\mu\) =  98.2 and \(\sigma\) =  0.73 ) are not
sample statistics, they are population parameters. Our normal
distribution is illustrated in figure 5.7.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{chapter5_files/figure-pdf/fig-normal-mu-sd-1.pdf}}

}

\caption{\label{fig-normal-mu-sd}Normal Distribution with Mean and
Standard Deviation}

\end{figure}%

We now have three questions we want to answer based on this
distribution.

\textbf{Question \#1: \emph{What is the probability that a randomly
sampled individual from this population has a body temperature of 98.6
degrees or higher?}}

If we let X denote body temperature, we are basically asking what is
\emph{the probability that X is greater than or equal} to 98.6 degrees?
We can equivalently express this as follows: \(P(X \ge 98.6) = \, ?\) In
shorthand, \(P\) represents probability.

To answer the question, we first need to turn our random variable X into
a z score. We then need to use this z score to find the area under the
normal curve.

We convert X to a z score using the \emph{z transformation formula}. The
formula differs for a population of scores versus a sample of scores.

\(\text{Population of scores: } z = \frac{x - \mu}{\sigma}\)

\(\text{Sample of scores: } z = \frac{x - M}{SD}\)

Since we obtained information for the entire population of the United
States: \(z = \frac{98.6 - 98.2}{.73} = .55\)

Our z-score is 0.55. So, we can re-express the above shorthand notation
like this: \(P(X \ge 98.6) = P(z \ge 0.55) = ?\)

Therefore, the probability that we would obtain a temperature that is
98.6 degrees or higher is the same as the probability of obtaining a z
score of 0.55 or higher. The probability is the area under the curve, to
the right of a z score of 0.55, or to the right of the body temperature
of 98.6. Figure 5.8 shows you this area---it is the area to the right of
the red line.

(fig 5.8 here\ldots{} need to figure out how to do the little thought
cloud \ldots{} :/ )

Now, let's determine the probability of obtaining a body temperature of
98.6 or higher, which corresponds to a z score of 0.55 or above. Here,
we want to introduce a
\href{https://homepage.divms.uiowa.edu/~mbognar/applets/normal.html}{free
applet} to help find the probability (Bognar, 2021). Enter the
population mean, µ, of 98.2 and the standard deviation, \(\sigma\), of
.73 to define the normal distribution as shown in Figure 5.9. You will
see that the X-axis has been scaled using the specified mean and
standard deviation. This is a good example of how the normal
distribution can take on different values for μ and σ without changing
shape.

\begin{figure}[H]

{\centering \includegraphics[width=0.6\linewidth,height=\textheight,keepaspectratio]{chapter5/fig5-9.png}

}

\caption{Fig 5.9: Aplet Demo 1}

\end{figure}%

Next, enter the value of 98.6 in the box for x. Figure 5.10 shows the
output.

\begin{figure}[H]

{\centering \includegraphics[width=0.6\linewidth,height=\textheight,keepaspectratio]{chapter5/fig5-10.png}

}

\caption{Fig 5.10: Aplet Demo 2}

\end{figure}%

According to the applet, the probability of obtaining a temperature of
98.6, or a z score of 0.55, is roughly 0.29. Recall that probabilities
range from 0 to 1, with the total area under the normal curve equaling 1
(or 100\%). Thus, a probability of 0.29 indicates there is approximately
a 29\% chance of randomly selecting an individual from this population
who has a body temperature of at least 98.6 degrees.

Although it is easy to use an applet to find our answer, let's learn how
to use the Standard Normal Cumulative Probability Table in the back of
the book to find the answer. Remember that we are now expressing our
question as follows: \(P(z \ge 0.55) = ?\)

We must go to Standard Normal Cumulative Probability Table in the back
of the book and look up our z score of 0.55. The column on the far left
indicates the first two values of the z score, so find 0.5. The top row
indicates the next two values, so find 0.05. Follow the 0.05 column down
to where it intersects the 0.5 column to find a value of .7088. This
indicates the cumulative probability up to 0.55 standard deviations
above the mean. Since we are interested in the area on the opposite side
of 0.55 (a body temperature of at least 98.6), we subtract .7088 from 1
to get .2912 - roughly the same number given by the applet. This means
that the probability of selecting an individual at random who has a body
temperature of 98.6 degrees or greater is .2912, or 29.12\%.

\textbf{Question \#2: \emph{What is the probability that a randomly
sampled individual from this population has a body temperature of 97.5
degrees or lower?}}

Hopefully, this question will now be easy to answer. In shorthand
notation, we are asking: \(P(X < 97.5) = ?\)

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{chapter5_files/figure-pdf/fig-px-less-97-5-1.pdf}}

}

\caption{\label{fig-px-less-97-5}Figure 5.11 Area representing
\(P(X < 97.5)\) for a normal distribution with \(\mu=98.2\)}

\end{figure}%

Now, we are looking for the probability of having a body temperature of
97.5 or below, which is represented by the left side of the red line. To
answer this question, we first turn 97.5 into a z score:

\[
z=\frac{97.5-98.2}{0.73}=-0.96
\]

\[
z=\frac{X-\mu}{\sigma}
\]

Let's use
\href{https://homepage.divms.uiowa.edu/~mbognar/applets/normal.html}{the
applet} to find \(P(x\le 97.5) = P(z\le -0.96)\) or the probability of
\(z\) is less than or equal to -0.96.

\begin{figure}[H]

{\centering \includegraphics[width=0.6\linewidth,height=\textheight,keepaspectratio]{chapter5/fig5-12.png}

}

\caption{Fig 5.12: Aplet Demo 2}

\end{figure}%

The probability that a randomly selected individual from the population
has a body temperature less than 97.5 is 0.1688, or 16.9\%.

Now, let's find this probability using the Standard Normal Cumulative
Probability Table. Since our z score is --0.96, find the intersection of
the --0.9 row and 0.06 column. The stated value is .1685. Since we are
interested in the probability of having a body temperature less than or
equal to 97.5 and the table represents cumulative probability up until
that point, we can simply report that value. The proportion of
temperatures that fall at, or below, 97.5 is about 0.1685, or there is a
16.85\% chance we would randomly sample an individual who has a body
temperature of 97.5 degrees or less. Again, the value is slightly off
from the one obtained from the applet due to rounding differences.

\textbf{Question \#3: \emph{What is the probability that a randomly
sampled individual from this population has a body temperature between
97.5 and 98.6 degrees?}}

With this final question, we are asking: \(P(97.5 \le X \le 98.6) = ?\)
Although this might seem like a more difficult question, it is actually
the same basic concept. The first step is to convert both values to
\(z\) scores, which we have already done. We know that 98.6 expressed as
a \(z\) score is about 0.55, and 97.5 expressed as a \(z\) score is
about -0.96. We have also already figured out the probability of having
a temperature above 98.6, \(z\) = 0.55, and the probability of having a
temperature below 97.5, \(z\) = -0.96. We know that the entire
probability under the normal distribution is 1.0, so we can get to the
probability of scoring between these two \(z\) scores by subtracting the
probabilities we already know.

Here's how the math works out:

\[
P(97.5 \le X \le 98.6)
= 1 - P(X \le 97.5) - P(X \ge 98.6)
= 1 - (0.1685 + 0.2912)
= 1 - 0.4597
= 0.5403
\]

This means there is a probability of about 0.54, or 54\%, that we would
randomly sample an individual from this population who has a body
temperature between 97.5 and 98.6 degrees.

\begin{figure}

\centering{

\pandocbounded{\includegraphics[keepaspectratio]{chapter5_files/figure-pdf/fig-between-97-5-98-6-1.pdf}}

}

\caption{\label{fig-between-97-5-98-6}Area representing}

\end{figure}%

\section{Conclusion}\label{conclusion-3}

Chapter 5 delved into the fundamental concepts of the normal
distribution, a cornerstone of statistical analysis. It outlined the key
features of this distribution, including its symmetric, bell-shaped
curve, unimodal nature, and the alignment of its mean, median, and mode
at the center. The chapter revisited the empirical rule (68-95-99.7\%),
explaining how it provides a framework for interpreting the proportion
of data within one, two, or three standard deviations from the mean.
Additionally, it emphasized the importance of understanding normal
distribution properties for making population inferences and highlighted
how skewness and kurtosis helped assess a distribution's symmetry and
the likelihood of outliers.

The chapter also introduced z scores as a way to standardize data,
enabling meaningful comparisons across different datasets or scales. It
explained how to calculate z scores, interpret their significance as
standard deviations from the mean, and apply them in the context of the
standard normal distribution, where the mean was 0 and the standard
deviation was 1. Key applications, such as using z scores to compute
probabilities and find areas under the normal curve, were explored
through practical examples involving real-world scenarios like body
temperature analysis. Tools like z tables and statistical software were
presented as essential for calculating probabilities and interpreting
results, building a strong foundation for further statistical methods
like sampling distributions. Together, the chapter equipped readers with
essential skills for understanding and applying the principles of the
normal distribution in various contexts.

\subsection{Key Takeaways for Educational Researchers from Chapter
5\,}\label{key-takeaways-for-educational-researchers-from-chapter-5}

\begin{itemize}
\item
  Understanding the characteristics of a normal distribution (symmetry,
  unimodal shape, and alignment of mean, median, and mode) is important
  for interpreting data and essential for assessing its appropriateness
  for applying statistical analyses you will learn later in this book.
\item
  Skewness and kurtosis are key metrics for assessing the symmetry and
  \emph{tailedness} of a distribution, helping researchers evaluate how
  closely their data approximates a normal distribution. A distribution
  is generally considered approximately normal when these two statistics
  fall within the range of -1 to 1.
\item
  The empirical rule (68-95-99.7\%) helps researchers interpret the
  portion of data within standard deviations from the means, making it a
  critical tool for understanding variability of data in education
  research contexts.
\item
  Z scores provide a standardized measure of how far a data point
  deviates from the mean, expressed in terms of standard deviation
  units. By converting raw scores into z scores---a process known as
  standardization---we enable meaningful comparisons of relative
  positions across different datasets, even when the original scores are
  measured on varying scales. Understanding standardization is essential
  for cross-study comparisons and evaluating individual performance
  within a population or reference group.
\item
  Although probabilities are rarely calculated manually using z scores
  in practice, understanding the process provides researchers with a
  foundational knowledge of probability, enabling them to make
  population inferences and conduct hypothesis testing---essential
  components of quantitative analysis in educational research. \,
\end{itemize}

\section{Key Definitions from Chapter
5}\label{key-definitions-from-chapter-5}

\textbf{Kurtosis} measures the tailedness of a distribution, or how
heavy or light the tails are compared to a normal distribution.

\textbf{Normal distribution (bell curve)} is a statistical distribution
that is symmetric about its mean. All three measures of central tendency
(mean, median, and mode) are equal and located at the center of the
distribution, and the left and right halves of the curve are mirror
images of each other.

\textbf{Skewness} means the extent to which a curve is pulled to one
side or another.

The \textbf{standard normal distribution} is a specific type of normal
distribution that has been standardized so that the mean is 0, the
standard deviation is 1, and the distribution is symmetric.

A \textbf{z score} (standardized score) is a statistical measure that
describes how far a particular data point is from the mean of its
distribution, measured in terms of standard deviations.

\section{Check Your Understanding}\label{check-your-understanding-3}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{A distribution with high kurtosis has lighter tails than a
  normal distribution.}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
    True
  \item
    False
  \end{enumerate}
\item
  \textbf{Which of the following values for skewness and kurtosis
  indicates that a distribution is approximately normal?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
    Skewness = 0.5, Kurtosis = 2.5
  \item
    Skewness = 0.25, Kurtosis = 0.5
  \item
    Skewness = 1.0, Kurtosis = 3.5
  \item
    Skewness = -2.0, Kurtosis = 4.0
  \end{enumerate}
\item
  \textbf{What is the probability of a data point falling within 1
  standard deviation of the mean in a normal distribution?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
    68\%
  \item
    95\%
  \item
    99.7\%
  \item
    50\%
  \end{enumerate}
\item
  \textbf{A standardized test has a mean score of 500 and a standard
  deviation of 100. What is the probability that a randomly selected
  student scores higher than 650? Use the z score formula and the
  standard normal distribution table to find the answer.}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
    0.1056 (10.56\%)
  \item
    0.1587 (15.87\%)
  \item
    0.0668 (6.68\%)
  \item
    0.0228 (2.28\%)
  \end{enumerate}
\item
  \textbf{What is the purpose of converting raw data to z scores?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \item
    To reduce the range of the data.
  \item
    To standardize data for comparison across distributions.
  \item
    To eliminate outliers.
  \item
    To calculate the mean and median more accurately.
  \end{enumerate}
\end{enumerate}

\bookmarksetup{startatroot}

\chapter*{Chapter 5: Exploring the Normal Distribution in
R}\label{chapter-5-exploring-the-normal-distribution-in-r}
\addcontentsline{toc}{chapter}{Chapter 5: Exploring the Normal
Distribution in R}

\markboth{Chapter 5: Exploring the Normal Distribution in R}{Chapter 5:
Exploring the Normal Distribution in R}

This section demonstrates how to work with the normal distribution in R,
including checking for normality, calculating skewness and kurtosis,
standardizing scores (z-scores), and computing probabilities using the
properties of the normal curve. We use the PV1MATH variable as an
example.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load required package }
\CommentTok{\# Load required package }
\FunctionTok{library}\NormalTok{(haven) }
 
\CommentTok{\# Load the dataset }
\NormalTok{data }\OtherTok{\textless{}{-}} \FunctionTok{read\_sav}\NormalTok{(}\StringTok{"chapter2/US\_Data\_22.sav"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\section*{1 Visualizing and Summarizing the
Distribution}\label{visualizing-and-summarizing-the-distribution}
\addcontentsline{toc}{section}{1 Visualizing and Summarizing the
Distribution}

\markright{1 Visualizing and Summarizing the Distribution}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Histogram and summary }
\FunctionTok{hist}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{PV1MATH, }\AttributeTok{breaks=}\DecValTok{30}\NormalTok{, }\AttributeTok{main=}\StringTok{"Distribution of Math Scores"}\NormalTok{, }\AttributeTok{xlab=}\StringTok{"PV1MATH"}\NormalTok{, }\AttributeTok{col=}\StringTok{"lightblue"}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\pandocbounded{\includegraphics[keepaspectratio]{chapter5_files/figure-pdf/unnamed-chunk-2-1.pdf}}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{summary}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{PV1MATH) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. 
  188.4   391.3   457.9   461.9   528.9   819.5 
\end{verbatim}

As a recap, the first step in exploring a continuous variable, such as
math achievement scores (PV1MATH), is to visualize its distribution and
review key summary statistics. As covered in Chapters 2 and 3, these
basic descriptive measures and visualizations help us quickly understand
the main features of the data before proceeding to more advanced
analyses.

\section*{2 Checking for Normality}\label{checking-for-normality}
\addcontentsline{toc}{section}{2 Checking for Normality}

\markright{2 Checking for Normality}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{library}\NormalTok{(psych) }
\FunctionTok{skew}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{PV1MATH, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.1730788
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{kurtosi}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{PV1MATH, }\AttributeTok{na.rm =} \ConstantTok{TRUE}\NormalTok{) }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] -0.3150574
\end{verbatim}

\textbf{Interpretation}: The skewness of PV1MATH is 0.17, indicating a
slight right (positive) skew, meaning there are a few more high values
than low values. The kurtosis is --0.32, suggesting the distribution is
slightly flatter (platykurtic) than a perfectly normal distribution.
Both values fall well within the range of --1 to 1, so the math scores
are approximately normal, with only minor deviations from perfect
symmetry and peakedness.

\section*{3 Standardizing Scores: Calculating
Z-scores}\label{standardizing-scores-calculating-z-scores}
\addcontentsline{toc}{section}{3 Standardizing Scores: Calculating
Z-scores}

\markright{3 Standardizing Scores: Calculating Z-scores}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Standardize PV1MATH to obtain z{-}scores }
\NormalTok{z\_scores }\OtherTok{\textless{}{-}} \FunctionTok{scale}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{PV1MATH) }
\CommentTok{\# View summary statistics for the standardized scores }
\FunctionTok{mean}\NormalTok{(z\_scores, }\AttributeTok{na.rm=}\ConstantTok{TRUE}\NormalTok{)      }\CommentTok{\# Should be 0 }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] -2.350414e-16
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\FunctionTok{sd}\NormalTok{(z\_scores, }\AttributeTok{na.rm=}\ConstantTok{TRUE}\NormalTok{)        }\CommentTok{\# Should be 1 }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 1
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{data}\SpecialCharTok{$}\NormalTok{z\_PV1MATH }\OtherTok{\textless{}{-}}\NormalTok{ z\_scores }\CommentTok{\# Add z{-}scores to the dataset }
\FunctionTok{head}\NormalTok{(data[, }\FunctionTok{c}\NormalTok{(}\StringTok{"PV1MATH"}\NormalTok{, }\StringTok{"z\_PV1MATH"}\NormalTok{)]) }\CommentTok{\#show first 6 rows of PV1MATH and z\_PV1MATH }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
# A tibble: 6 x 2
  PV1MATH z_PV1MATH[,1]
    <dbl>         <dbl>
1    575.      1.19    
2    442.     -0.209   
3    680.      2.30    
4    462.      0.000270
5    501.      0.414   
6    322.     -1.47    
\end{verbatim}

\textbf{Interpretation:}

To standardize the math scores (PV1MATH), we calculated z-scores for
each student. The mean of the z-scores is effectively 0 and the standard
deviation is 1, as expected. This confirms that the z-scores represent
how many standard deviations each math score is from the mean.

Let's look at two example students from the first six rows:

\begin{itemize}
\item
  The first student has a raw math score of 573, which corresponds to a
  z-score of 1.17. This means their math score is 1.17 standard
  deviations above the mean (with the mean previously reported as about
  462).
\item
  The second student has a math score of 440, giving a z-score of
  --0.23. This means their score is slightly below the mean (about 0.23
  standard deviations lower).
\end{itemize}

By adding the z-scores to the dataset, it's easier to compare each
student's performance relative to the whole group, regardless of the
scale of the original scores.

\section{4 Example: Calculating Z-scores and Probabilities (Questions
1-3)
\{.unnumbered}\label{example-calculating-z-scores-and-probabilities-questions-1-3-.unnumbered}

Suppose the mean (𝜇) = 98.2 and standard deviation (𝜎) = 0.73, as in the
body temperature example.

\textbf{Q1. Probability that X ≥ 98.6}

Here we replicate the example from section Finding Areas Under the
Normal Curve, calculating the probability that a randomly selected
person has a body temperature of 98.6°F or higher. \}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{mu }\OtherTok{\textless{}{-}} \FloatTok{98.2} 
\NormalTok{sigma }\OtherTok{\textless{}{-}} \FloatTok{0.73} 
\NormalTok{x1 }\OtherTok{\textless{}{-}} \FloatTok{98.6} 
 
\CommentTok{\# Calculate z }
\NormalTok{z1 }\OtherTok{\textless{}{-}}\NormalTok{ (x1 }\SpecialCharTok{{-}}\NormalTok{ mu) }\SpecialCharTok{/}\NormalTok{ sigma }
\NormalTok{z1 }\CommentTok{\#\textasciitilde{}0.55 }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.5479452
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Probability that X \textgreater{}= 98.6 }
\NormalTok{prob1 }\OtherTok{\textless{}{-}} \FunctionTok{pnorm}\NormalTok{(z1, }\AttributeTok{lower.tail =} \ConstantTok{FALSE}\NormalTok{) }\CommentTok{\#lower.tail = FALSE gives the right tail probability }
\NormalTok{prob1   }\CommentTok{\# \textasciitilde{}0.2912 }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.2918648
\end{verbatim}

\textbf{Interpretation}: About 29.1\% of the population have a body
temperature of 98.6 or higher.

\textbf{Q2. Probability that X ≤ 97.5} Now we calculate the probability
that a randomly selected person has a body temperature of 97.5°F or
lower.

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x2 }\OtherTok{\textless{}{-}} \FloatTok{97.5} 
\NormalTok{z2 }\OtherTok{\textless{}{-}}\NormalTok{ (x2 }\SpecialCharTok{{-}}\NormalTok{ mu) }\SpecialCharTok{/}\NormalTok{ sigma }
\NormalTok{z2 }\CommentTok{\# \textasciitilde{}{-}0.96 }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] -0.9589041
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Probability that X \textless{}= 97.5 (left tail) }
\NormalTok{prob2 }\OtherTok{\textless{}{-}} \FunctionTok{pnorm}\NormalTok{(z2) }\CommentTok{\# lower.tail = TRUE is the default, so we can omit it to get the left tail probability }
\NormalTok{prob2   }\CommentTok{\# \textasciitilde{}0.1688 }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.1688035
\end{verbatim}

\textbf{Interpretation}: About 16.9\% of the population have a body
temperature of 97.5°F or lower.

\textbf{Q3. Probability between 97.5 and 98.6}

Finally, we calculate the probability that a randomly selected person
has a body temperature between 97.5°F and 98.6°F.

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Load required package }
\CommentTok{\# Probability that 97.5 \textless{}= X \textless{}= 98.6 }
\NormalTok{prob\_between }\OtherTok{\textless{}{-}} \DecValTok{1}\SpecialCharTok{{-}}\NormalTok{(prob1}\SpecialCharTok{+}\NormalTok{prob2) }\CommentTok{\#same as in previous section }
\CommentTok{\#prob\_between \textless{}{-} pnorm(z1) {-} pnorm(z2) \#alternative }
 
\NormalTok{prob\_between  }\CommentTok{\# \textasciitilde{}0.54 }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.5393317
\end{verbatim}

\textbf{Interpretation}: About 53.9\% (difference due to rounding) of
the population have a body temperature between 97.5°F and 98.6°F.

\bookmarksetup{startatroot}

\chapter{Chapter 6}\label{chapter-6}

\bookmarksetup{startatroot}

\chapter{Chapter 6}\label{chapter-6-1}

This is Chapter 6.

\bookmarksetup{startatroot}

\chapter{Chapter 7}\label{chapter-7}

\bookmarksetup{startatroot}

\chapter{Chapter 7}\label{chapter-7-1}

This is Chapter 7.

\bookmarksetup{startatroot}

\chapter{Chapter 8}\label{chapter-8}

\bookmarksetup{startatroot}

\chapter*{Evaluating Group Averages Against a Benchmark: One-Sample t
Test and Goodness-of-Fit
Analysis}\label{evaluating-group-averages-against-a-benchmark-one-sample-t-test-and-goodness-of-fit-analysis}
\addcontentsline{toc}{chapter}{Evaluating Group Averages Against a
Benchmark: One-Sample t Test and Goodness-of-Fit Analysis}

\markboth{Evaluating Group Averages Against a Benchmark: One-Sample t
Test and Goodness-of-Fit Analysis}{Evaluating Group Averages Against a
Benchmark: One-Sample t Test and Goodness-of-Fit Analysis}

\section*{Learning Objectives}\label{learning-objectives-4}
\addcontentsline{toc}{section}{Learning Objectives}

\markright{Learning Objectives}

By the end of this chapter, you will be able to:

\begin{itemize}
\item
  Describe the differences between z tests and t tests, including when
  to use each based on knowledge of population parameters and sample
  characteristics.
\item
  Calculate and interpret the test statistic, p value, and effect size
  for one-sample t tests and goodness-of-fit analyses.
\item
  Clearly explain the distinction between statistical significance and
  practical significance, and evaluate both to provide a comprehensive
  understanding of research findings.
\item
  Use confidence intervals to quantify uncertainty in sample mean
  estimates and evaluate the plausibility of a null hypothesis.
\end{itemize}

We just learned about z tests in the previous chapter, which you can
conduct if you want to compare a sample mean to a population with a
known mean and standard deviation. Often, however, we do not know the
population standard deviation (σ), and we must estimate it with the
sample statistic, in this case sample standard deviation (s). When this
happens, we can no longer rely on the standard normal distribution, also
called the z distribution, as it assumes precise knowledge of σ.
Instead, we use a t distribution, which accounts for the added
uncertainty of estimating σ. In these cases, we use a t distribution,
which closely resembles its z-distribution counterpart but with two key
differences.

First, the tail ends of the t distribution take longer to approach the
X-axis compared to the z distribution. This is because we are estimating
the population standard deviation, and that means we have more
uncertainty. If you look at figure 8.1, imagine drawing a vertical line
out towards one of the ends. Can you see how there would be more area
under the curve for the t distribution with just 5 degrees of freedom?
Shaded areas under the curve represent 5\% of the total area. This means
the sample mean must be farther from the population mean before we can
confidently conclude that the observed difference reflects something
other than random chance.

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{chapter8_files/figure-pdf/unnamed-chunk-1-1.pdf}}

}

\caption{Fig 8.1: Comparison of Standard Normal and t-Distributions}

\end{figure}%

Second, there is more than one t distribution, and the shape of the
distribution depends on your sample size. Recall that a larger sample
size leads to a smaller standard error, which in turn increases your
statistical power to detect a significant effect. You can see how
different t distributions look in figure 8.1. The larger the sample
size, the closer the t distribution becomes to a z distribution. Once n
\textgreater{} 120, the two distributions are almost identical. Having
such a large sample makes up for the fact that we are estimating the
population standard deviation.

Another way to look at this is that the t distribution is related to the
degrees of freedom. You must know the degrees of freedom to utilize a t
table. Since sample standard deviation is used to compute the estimate
of standard error, the degrees of freedom for a t test are calculated by
using n - 1.

\section{Example: Conducting one sample
t-test}\label{example-conducting-one-sample-t-test}

Now, let's walk through an example of conducting a one-sample t test.
Let's think back to the example from the last chapter, where we compared
a sample mean to a population mean with the z test. We have a similar,
but slightly different, scenario this time. We have a sample of just 35
test scores from a single third grade class. The teacher is concerned
that their students are underperforming in English Language Arts (ELA)
and hypothesize their class will score below the state average of 440.
When the results were returned to the school, the class's average score
was 425.26. Is this average significantly lower than the expected value
of 440? Let's conduct a hypothesis test to find out.

\subsubsection*{Step 1: Determine the null and alternative
hypotheses.}\label{step-1-determine-the-null-and-alternative-hypotheses.}
\addcontentsline{toc}{subsubsection}{Step 1: Determine the null and
alternative hypotheses.}

Just like last time, the null hypothesis is that there is no difference
between the teacher's class and the overall state population. So, the
null hypothesis is:

\[
H_0: \mu = 440
\]

This time, however, we are going to use a directional, or one-tailed
test, because the teacher is specifically concerned that the students
are scoring lower than the general population. So, the alternative
hypothesis is:

\[
H_1: \mu < 440
\]

There are 35 students in this class, which might be why they are not
doing so well, so n = 35. The sampling distribution in this case would
be composed of the means of all possible samples of size n = 35 taken
from the state population of third graders. It would be centered at μ =
440 (the population mean, which we know), but we must calculate the
standard error using the sample standard deviation. The sample standard
deviation (s) happens to be 44.994.

\[
\frac{s}{\sqrt{n}} = \frac{44.994}{\sqrt{35}} = 7.605
\]

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{chapter8_files/figure-pdf/unnamed-chunk-2-1.pdf}}

}

\caption{Fig 8.2: Comparison of Sampling Distribution (t, df=34) and
Normal Distribution}

\end{figure}%

\subsubsection*{Step 2: Set the criteria for a
decision.}\label{step-2-set-the-criteria-for-a-decision.}
\addcontentsline{toc}{subsubsection}{Step 2: Set the criteria for a
decision.}

We will set the alpha level (α) to 0.05. So, we will reject the null
hypothesis if the p value is less than 0.05. Since we are conducting a
one-sided directional test, the entire rejection region is in the left
tail of the distribution. To find the critical value we first need to
consider our degrees of freedom. For a t test, df = n -- 1, so we have
34 degrees of freedom. In the Critical Values of t distribution table in
the appendix, there is no line for df = 34, so round down to the nearest
degrees of freedom (30) as a conservative estimate. If you use an online
calculator, which allows you to enter precise degrees of freedom, you
will find that the critical value for 34 degrees of freedom at a .05
alpha level is -1.69. It is negative because we expect the sample mean
to be lower than the population mean.

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{chapter8_files/figure-pdf/unnamed-chunk-3-1.pdf}}

}

\caption{Sampling Distribution of the Sample Mean (n=35)}

\end{figure}%

\subsubsection*{Step 3: Check assumptions and compute the test
statistic.}\label{step-3-check-assumptions-and-compute-the-test-statistic.}
\addcontentsline{toc}{subsubsection}{Step 3: Check assumptions and
compute the test statistic.}

There are three assumptions underlying this test. They are:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Random sampling: The scores come from a random, representative sample.
\item
  Independence: The scores are independent of one another.
\item
  Normality: The population score distribution is approximately normal.
\end{enumerate}

It is okay that we didn't choose a random sample representing all third
graders in the state because the inferences we will draw are only about
this one class. We can't see the entire population distribution, but the
histogram for the scores looks almost perfectly normal.

(fig 4 here)

If the distribution was way off, we could still proceed but be very
cautious about our interpretation of the results. Since our sample size
is greater than 30, we would be okay even without such a normal
distribution due to the central limit theorem. So, we are all set to
calculate our test statistic.

For the one-sample t test, we use the formula:

\[
t = \frac{\bar{X} - \mu}{\frac{s}{\sqrt{n}}}
\]

Notice that the formula is very similar to what we used to calculate the
z statistic. The only difference is that we now estimate the standard
error using the sample standard deviation and sample size in the
denominator because we don't know the value of the population standard
deviation (𝜎).

The numerator represents the difference between the observed and test
value specified in the null hypothesis. The symbol 𝜇 represents the
population parameter specified in the hypothesis, often referred to as
the null value, as it reflects the assumed true population mean under
the null hypothesis. The other values for the test statistic come from
our sample data.

\[
t
= \frac{425.26 - 440}{\dfrac{44.994}{\sqrt{35}}}
= \frac{-14.74}{7.605}
= -1.938
\]

\subsection*{Step 4}\label{step-4}
\addcontentsline{toc}{subsection}{Step 4}

\subsubsection*{a: Find the p value.}\label{a-find-the-p-value.}
\addcontentsline{toc}{subsubsection}{a: Find the p value.}

We must now determine whether our t statistic is significant, which
would mean that it is very unlikely we obtained this value due to chance
alone. In step 2, we said that the critical value for this test was
-1.69. Picture a t distribution and draw a vertical line about 1.7
standard deviations below the mean. Anything to the left of that is in
the rejection region. Is our t statistic of -1.938 in the rejection
region? Yes -- it is farther away from the center than our critical
value. This means our p value is less than 0.05.

(fig 8.5 here)

\subsubsection*{b: Draw a conclusion and report your
conclusion.}\label{b-draw-a-conclusion-and-report-your-conclusion.}
\addcontentsline{toc}{subsubsection}{b: Draw a conclusion and report
your conclusion.}

Given that our p value is less than 0.05, the appropriate decision is to
reject the null hypothesis. In other words, we have found evidence in
favor of the alternative hypothesis. Students in this class scored
significantly lower than the overall state population on the test. Let's
assume we use statistical software to land on a p value of 0.03 for this
analysis.

\section{Reporting Results in APA
Format}\label{reporting-results-in-apa-format}

The American Psychological Association, 7th edition (APA, 2019) has
decided that there is a correct way to report the results of a t test.
In this case, you would report the results of our test like this: Scores
for students in this third-grade class (M = 425.26) were significantly
lower than scores of third graders statewide, t(34) = -1.938, p = 0.03.

The most important part of the reporting is the section that starts with
the observed t statistic. You put the degrees of freedom in parentheses
immediately following the t; all statistics (e.g., M, t, p) are in
italics, and you report the p value after the exact t statistic.
Remember, that even if the statistical software says p = 0.000, you
should never report that. Instead, you should report p \textless{}
0.001. You can also just report that p \textless{} 0.05 (your alpha
level) if you do not have access to statistical software.

\section{Cautions with One-Sided
Tests}\label{cautions-with-one-sided-tests}

One caution with directional, one-tailed tests is that you need to
decide on one-tailed versus two-tailed tests up front. It is really not
okay to decide after you see the results that you are going to conduct a
one-tailed test. You should only be conducting a one-tailed test if you
have good reason to expect the sample mean to be lower or higher than,
rather than just different from, the population mean. And while a
one-tailed test has greater power to detect a significant result, there
is risk involved. Imagine that we expected the sample mean to be
significantly lower, but it turned out to be significantly higher than
the population mean. If we had been conducting a two-tailed test, that
would be a significant difference from the population mean. But since
our alternative hypothesis was that μ \textless{} 440, we would fail to
reject the null (non-significant result). Most of the time you will be
using a two-tailed test for the possibility of a difference in either
direction. Only use a one-tailed test if you have a very good reason and
you decide ahead of time to do so.

Another caution is that type III error is introduced when using
directional tests. Type III error occurs when a correct conclusion is
drawn for the wrong reason or when the wrong question is asked, and a
statistically valid result is obtained for that incorrect question. For
example, a school district wants to determine whether lessons learned
from a new professional development program will reduce the time
teachers spend on grading. After collecting data and running a
statistical test, the null hypothesis (that the program has no effect on
grading time) is rejected, and a significant difference was found.
District leadership interpreted this result as evidence that the program
was reducing grading time, so they decided to expand the program.
However, a few weeks later, teachers were demanding that the district
take a closer look at the data because they claimed to be spending more
time grading. Sure enough, district leadership found they made a
mistake: while the statistical test correctly identified a significant
difference in grading time (rejecting the null hypothesis), they
misinterpreted the direction of the effect. The program increased
grading time instead of reducing it. This misinterpretation of the
direction of the significant effect is a Type III error: correctly
rejecting the null hypothesis but misunderstanding what the result
actually implies.

\section{Understanding Effect Size}\label{understanding-effect-size}

Hypothesis testing is important for knowing if there is a significant
difference between a sample and the population, but it will not tell you
how big the difference is. A hypothesis test will almost always return a
significant result when you have a large sample size because a test
statistic is a function of standard error, which decreases as sample
size increases.

Given the mean difference and standard deviation remain the same, as
sample size increases, the test statistic always increases. For example,
if you wanted to conduct a one-sample t test with a sample mean of 22
and a SD of 2 compared to a population mean of 23.5, you are sure to get
a statistically significant result when n = 100,000 even if the p value
is \textgreater{} 0.05 when n = 10. It is somewhat paradoxical that with
sufficiently large sample size, the statistical power becomes strong
enough to flag even small differences as significant, even if those
differences are practically trivial or meaningless. Therefore, it is
important to have another way to represent the meaningfulness, or
practical significance, of a statistically significant result. That is
why we calculate and report effect size.

Of course, one way of understanding the size of the difference is to
simply compare the sample mean and population mean on the original
measurement scale. Someone who understands a particular testing
instrument could make their own judgment as to whether a difference of
about 15 points was meaningful, or cause for concern. But it helps to
have a common language for reporting effect size, and the way we do that
is to standardize the effect.

Effect size is a numeric measure that indicates the strength or
meaningfulness of the relationship or difference between variables in a
study. Unlike p values, which only tell you whether an effect exists,
effect size tells you how large or meaningful that effect is. It
provides a standardized way to understand the practical significance of
research findings, which is particularly useful when comparing results
across studies.

Estimated Cohen's d is a common measure of effect size for t tests. We
hope you remember back in chapter 5 when we learned about z scores,
which are also known as standardized scores because they are measured in
standard deviation units. When we use estimated Cohen's d as a measure
of effect size, we are doing the same thing. Cohen's d quantifies the
magnitude of the difference between two means relative to the
variability in the data and provides a standardized measure of practical
significance, helping to interpret the importance of the result by
allowing us to express the effect size in terms of standard deviation
units. This approach provides a common scale, making it easier to
compare results across different tests or measures, rather than relying
on raw score differences.

\section{Calculating Estimated Cohen's
d}\label{calculating-estimated-cohens-d}

One quick note to get us started -- we are being careful to refer to the
effect size we use for t tests as estimated Cohen's d to distinguish it
from Cohen's d used in z tests. This distinction arises because effect
sizes for t tests are calculated using sample data, whereas z tests rely
on known population parameters. But out in the world of educational
research, you will normally just hear it called Cohen's d regardless of
the type of analysis used.

Here is the formula for estimated Cohen's d for a one-sample t test:

\[
d = \frac{\bar{X} - \mu}{SD}
\]

In our example above, the sample mean (X̅) = 425.26 and SD = 44.994. The
population mean (μ) = 440. Using the formula,

\[
d
= \frac{425.26 - 440}{44.994}
= -0.33
\]

That is, our effect size is -0.33, which according to the following
table from Privitera (2018) is a medium effect size. In our sample
research question, this means the students in the third-grade class had
scores that were about one-third of a standard deviation below the state
average. A positive effect size indicates the sample mean was larger
than the population mean, and a negative effect size indicates it was
smaller. Since effect size can be negative, you need to consider the
absolute value when making a judgement about the strength of the effect.

\textbf{Cohen's Effect Size Conventions}

\begin{longtable}[]{@{}ll@{}}
\toprule\noalign{}
Description of Effect & Effect Size (d) \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Small & ( d \textless{} 0.2 ) \\
Medium & ( 0.2 \textless{} d \textless{} 0.8 ) \\
Large & ( d \textgreater{} 0.8 ) \\
\end{longtable}

\emph{Source:} Adapted from Privitera, G. J. (2018). \emph{Statistics
for the Behavioral Sciences} (3rd ed.). SAGE Publications.

Before we move on, we want to remind you of a few other similar formulas
because we had a tendency to get them confused in the past. First, here
is a reminder of the formula for calculating z scores:

\[
z = \frac{X - \mu}{\sigma}
\]

Do you see the differences between that and the formula used to
calculate Cohen's d? First, with a z score, we only have one X (i.e., a
particular value), rather than a sample mean, or X̅. But even more
importantly, we use the population standard deviation (σ) rather than
the sample SD to calculate a z score. But overall, the concept is
exactly the same: both z scores and Cohen's d involve converting a raw
difference from the mean into standard deviation units. The key
difference lies in the specific inputs used for calculations.

Do you see the differences between that and the formula used to
calculate Cohen's d? First, with a z score, we only have one X (i.e., a
particular value), rather than a sample mean, or X̅. But even more
importantly, we use the population standard deviation (σ) rather than
the sample SD to calculate a z score. But overall, the concept is
exactly the same: both z scores and Cohen's d involve converting a raw
difference from the mean into standard deviation units. The key
difference lies in the specific inputs used for calculations.

Another place we have seen learners, including ourselves, get tripped up
is confusing the formula for Cohen's d with the formula for the t
statistic itself. Here they are, side by side:

\[
\begin{aligned}
d = \frac{\bar{X}-\mu}{SD}, \quad
t = \frac{\bar{X}-\mu}{\dfrac{s}{\sqrt{n}}}
\end{aligned}
\]

Even though the first formula, Cohen's d, uses SD to represent the
sample standard deviation, and the second, t statistic, uses s, they
represent the same value. So, the only difference is that d has nothing
to do with sample size, but sample size (n) is hugely important in
calculating standard error, which is the denominator for the t
statistic. For Cohen's d, the mean difference is divided by the standard
deviation, which represents the variability in the actual scores within
the sample. For the t statistic, the mean difference is divided by the
standard error, which represents the standard deviation of the
theoretical sampling distribution. By definition, it is going to be much
smaller than the original sample standard deviation.

In this example, the same mean difference (-14.743) served as the
numerator for both d and t. When we divided it by standard error for the
t statistic, we got -1.938. This told us how far our sample mean was
from the center of the sampling distribution. When we divided it by the
sample standard deviation for Cohen's d, we got -0.33, which told us how
far our sample mean was from the population mean, the center of the
population distribution.

If nothing else, we hope this helps you see why we need both effect size
and hypothesis tests. Effect size tells us how big a difference is while
the hypothesis test tells us whether we can be confident the difference
is a result of something other than chance. Effect size does not depend
on sample size, but you are more likely to get a significant result from
your hypothesis test as you increase the sample size.

It is common to report effect size after the results of your hypothesis
test. For example, scores for students in this third-grade class (M =
425.26) were significantly lower than scores of third graders statewide,
t(34) = -1.938, p = 0.03, d = -0.33. You might also choose to report the
significant result followed by a statement that Cohen's d (-0.33)
indicated a medium effect size. Keep in mind that if you have a small
sample size, you might be able to calculate a large effect size even
without a significant result. This can be interesting to learn and might
suggest you should try to conduct your analysis again with a larger
sample size. But we don't report effect size after a non-significant
result since it does not make sense to describe the size of an effect
you just concluded does not exist.

\section{Confidence Intervals}\label{confidence-intervals}

How do confidence intervals fit in? Confidence intervals provide an
additional layer of transparency by offering a range of values likely to
contain the true population parameter.

The way we think about it, the reason we use confidence intervals is to
be up front about our level of uncertainty. We take a sample and report
the mean, but how confident are we that the mean of our sample is equal
to the true population mean? Or we have two samples and we find the
difference between the two means (stay tuned for the next chapter), but
how confident are we that we know the true difference? If we report a
point estimate (e.g., a mean) along with an interval estimate, we are
being completely transparent about how good our estimate is likely to
be.

Another high-level concept to be aware of before we dive into the
details is the balance between precision and accuracy. Say we want to
estimate the average height of all people in the entire world who are 18
and older. We could be highly confident in our accuracy if we estimated
that the mean height of all adults was somewhere between 2 feet
(60.96cm) and 8 feet (243.84cm) . Don't you feel pretty comfortable
accepting that the true population mean is somewhere within that
interval? And yet, do you feel like you have learned any useful
information? Probably not, because our estimate has almost zero
precision. We could go in another direction and estimate that the
average height was exactly 65.8 inches (167.13cm). That estimate is much
more precise, but who knows how accurate it is?

What we often do is report both point and interval estimates. We
commonly see this with survey results. We start with a sample mean and
then use the standard error to calculate a confidence interval. The more
confident we are (higher level of accuracy), the wider the interval
(less precision). We can decide on any confidence level we want and
calculate the exact interval, but it is most common to use 90, 95, or
99\%.

\subsection{Interpreting Confidence
Intervals}\label{interpreting-confidence-intervals}

Now that you have the general idea about the purpose of confidence
intervals, we are going to get a little pedantic and refer you to the
words of
\href{https://digitalcommons.unl.edu/cgi/viewcontent.cgi?referer=&httpsredir=1&article=1009&context=imseteach}{Dr.~Paul
Savory (2008)}. Say you want to know the mean of some variable in the
population. You take a sample, find the mean, and then create a 95\%
confidence interval around it. It is incorrect to say, ``There is a 95\%
probability that μ falls within this interval.'' This is incorrect
because it implies that μ is random and could take on different values.
In fact, μ is a constant -- we just don't know what it is -- and it
either falls within our interval or it doesn't. The correct
interpretation is, ``We are confident that if μ were known, this
interval would contain it.'' This may seem like a trivial difference,
but it makes an important point that the probability or confidence level
we report is referring to the interval, not the population parameter
itself.

We highly recommend visiting the book's website to test out the
\href{no\%20link\%20right\%20now\%20:/}{confidence intervals applet} for
a helpful visualization of the concept. The way the applet works is that
you select the confidence level and sample size, and then it will
randomly pull one or 25 samples from the population distribution. If you
request 25 samples, you will see them all lined up. Each sample will
have a mean and corresponding confidence interval around it.

We used a 90\% confidence interval with a sample size of 10 in the
following example and asked the applet to take 25 samples, see figure
8.6. Almost all resulting samples had a confidence interval that
contained the true population mean, but two did not. Note that 23/25 is
92\% and not 90\%, but when we try again, sometimes we get 24/25 or
22/25. Even our 90\% confidence interval is just an estimate. The applet
provides a great visualization of the difference between the original
population distribution and the sampling distribution. You recommend
playing around in the applet to see how increasing the confidence level
and/or the sample size will typically result in a greater percentage of
possible sample intervals containing the true population mean.

(fig 8.6 here)

\subsection{Calculating Confidence
Intervals}\label{calculating-confidence-intervals}

We are going to be calculating confidence intervals around a sample
mean, but keep in mind that you can estimate a confidence interval
around other statistics as well, such as a mean difference or an odds
ratio. Here is how we estimate an unknown population mean using a
confidence interval (CI) when population standard deviation is unknown:

\[
\text{CI for Population Mean}
=
\text{Sample Mean}
\;\pm\;
(\text{Level of Confidence}) \times (\text{Standard Error})
\]

In mathematical terms:

\[
CI(\mu) = \bar{X} \pm t_{\text{critical}} \times \frac{s}{\sqrt{n}}
\]

So, there are three steps to estimate a confidence interval for a
population mean.

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Compute the sample mean and standard error.
\item
  Choose the level of confidence and find the critical values of the t
  distribution for that level.
\item
  Compute the estimation formula to find the upper and lower confidence
  interval limits.
\end{enumerate}

The width of our interval depends on several factors: how confident we
want to be, the size of our sample, and the variability (or standard
deviation) of scores within our sample.

Let's work through an example. Suppose you selected a sample of 100
participants and computed sample statistics. You found a sample mean of
4 and SD of 5. Now, you want to estimate a confidence interval around
that mean.

\subsubsection*{Step 1: Compute the sample mean and standard
error}\label{step-1-compute-the-sample-mean-and-standard-error}
\addcontentsline{toc}{subsubsection}{Step 1: Compute the sample mean and
standard error}

From the information above, we already know that X̅ = 4, SD = 5, and n =
100, so we can calculate the standard error as follows:

\[
\frac{s}{\sqrt{n}} = \frac{5}{\sqrt{100}} = 0.5
\]

\subsubsection*{Step 2: Choose the level of confidence and find critical
values}\label{step-2-choose-the-level-of-confidence-and-find-critical-values}
\addcontentsline{toc}{subsubsection}{Step 2: Choose the level of
confidence and find critical values}

How do we know what level of confidence to choose? This is where you
must balance precision and accuracy. In social science research we
typically use a 95\% level of confidence, which as you saw is consistent
with our typical practice of choosing an alpha level of 0.05 for
hypothesis testing.

We look to the Critical Values of t Distribution Table in the appendix
to find the critical value. Find the t value for a 95\% level of
significance for a two-tailed test with df = 99. There is no row for 99
degrees of freedom in the table, so we will use df = 100 for an
approximation, and we get the t value of 1.98.

\subsubsection*{Step 3: Compute the estimation formula to find upper and
lower confidence interval
limits.}\label{step-3-compute-the-estimation-formula-to-find-upper-and-lower-confidence-interval-limits.}
\addcontentsline{toc}{subsubsection}{Step 3: Compute the estimation
formula to find upper and lower confidence interval limits.}

Using these numbers in the formula from earlier in the chapter, we get:

\[
4 \pm 1.98 \times 0.5 = (3.01,\; 4.99)
\]

We can conclude that we are 95\% confident the interval of 3.01 to 4.99
contains the true population mean.

\subsection{Using a Confidence Interval to Evaluate a Null
Hypothesis}\label{using-a-confidence-interval-to-evaluate-a-null-hypothesis}

We are going to walk through one more scenario using our original
one-sample t test example. This example will demonstrate how estimation
through confidence intervals is connected to hypothesis testing, showing
that both methods can provide related insights into evaluating a null
hypothesis.

\subsubsection*{Step 1: Compute the sample mean and standard
error}\label{step-1-compute-the-sample-mean-and-standard-error-1}
\addcontentsline{toc}{subsubsection}{Step 1: Compute the sample mean and
standard error}

We already calculated the relevant values, but to get everything back in
one place, our sample mean was X̅ = 425.26, our standard error was 7.605,
and we wanted to know if our sample mean could be considered equal to
the population mean of 440.

\subsubsection*{Step 2: Choose the level of confidence and find critical
values}\label{step-2-choose-the-level-of-confidence-and-find-critical-values-1}
\addcontentsline{toc}{subsubsection}{Step 2: Choose the level of
confidence and find critical values}

How do we know what level of confidence to choose? This is where you
must balance precision and accuracy. In social science research we
typically use a 95\% level of confidence, which as you saw is consistent
with our typical practice of choosing an alpha level of 0.05 for
hypothesis testing.

We look to the Critical Values of t Distribution Table in the appendix
to find the critical value. Find the t value for a 95\% level of
significance for a two-tailed test with df = 99. There is no row for 99
degrees of freedom in the table, so we will use df = 100 for an
approximation, and we get the t value of 1.98.

\subsubsection*{Step 3: Compute the estimation formula to find upper and
lower confidence interval
limits}\label{step-3-compute-the-estimation-formula-to-find-upper-and-lower-confidence-interval-limits}
\addcontentsline{toc}{subsubsection}{Step 3: Compute the estimation
formula to find upper and lower confidence interval limits}

Using these numbers in the formula from earlier in the chapter, we get:

\[
4 \pm 1.98 \times 0.5 = (3.01,\; 4.99)
\]

We can conclude that we are 95\% confident the interval of 3.01 to 4.99
contains the true population mean.

\subsection{Using a Confidence Interval to Evaluate a Null
Hypothesis}\label{using-a-confidence-interval-to-evaluate-a-null-hypothesis-1}

We are going to walk through one more scenario using our original
one-sample t test example. This example will demonstrate how estimation
through confidence intervals is connected to hypothesis testing, showing
that both methods can provide related insights into evaluating a null
hypothesis.

\subsubsection*{Step 1: Compute the sample mean and standard
error}\label{step-1-compute-the-sample-mean-and-standard-error-2}
\addcontentsline{toc}{subsubsection}{Step 1: Compute the sample mean and
standard error}

We already calculated the relevant values, but to get everything back in
one place, our sample mean was X̅ = 425.26, our standard error was 7.605,
and we wanted to know if our sample mean could be considered equal to
the population mean of 440.

\subsubsection*{Step 2: Choose the level of confidence and find critical
values}\label{step-2-choose-the-level-of-confidence-and-find-critical-values-2}
\addcontentsline{toc}{subsubsection}{Step 2: Choose the level of
confidence and find critical values}

When we conducted the t test, we decided to use a directional test. This
time for creating a confidence interval, we will use the critical value
associated with a two-tailed test. So, we need to find the t value for a
95\% level of significance for a two-tailed test with df = 34. Our table
doesn't have a row for df = 34, so we will use df = 30 for a
conservative approximation, and we get the t value of 2.042. Note that
the exact critical t value for df = 34 at a 95\% confidence level is t =
2.032.

\subsubsection*{Step 3: Compute the estimation formula to find upper and
lower confidence interval
limits.}\label{step-3-compute-the-estimation-formula-to-find-upper-and-lower-confidence-interval-limits.-1}
\addcontentsline{toc}{subsubsection}{Step 3: Compute the estimation
formula to find upper and lower confidence interval limits.}

Using the numbers from earlier in the chapter, we get: \[
425.26 \pm 2.042 \times 7.605 = (409.73,\; 440.79)
\]

If the value of the population parameter we started with (μ = 440) is
contained within our 95\% confidence interval, then we cannot reject the
null hypothesis. As you can see, 440 is just barely contained within our
95\% confidence interval. If we had used a two-tailed test instead of a
one-tailed test earlier, we would have failed to reject the null
hypothesis. This reinforces the importance of identifying one-tailed or
two-tailed tests before running your statistical analysis.

\section{Chi-Square Goodness-of-Fit
Test}\label{chi-square-goodness-of-fit-test}

Most of the methods we will learn throughout this book are parametric,
meaning that we are using a sample to test hypotheses about parameters
(e.g., means and variances) in a population. But not all research
questions involve population parameters, so parametric methods don't
always meet our needs. Most of the methods we learn also come with
certain assumptions, but real-world data won't always meet those
assumptions. Nonparametric statistical tests are methods used in
statistics that do not require the data to follow a specific
distribution, such as the normal distribution. These tests are
particularly useful when:

\begin{itemize}
\item
  The data violate assumptions required for parametric tests (e.g.,
  normality, homogeneity of variance).
\item
  The data are ordinal or ranked instead of being measured on an
  interval or ratio scale.
\item
  The data contain outliers or are severely skewed.
\end{itemize}

The first nonparametric technique we will learn is the
\textbf{chi-square goodness-of-fit test}. If you have data on a single
variable, and you want to compare it to known or expected values, you
will use the chi-square goodness-of-fit test. This test will allow you
to evaluate how well your observed data align with these expected
values. While the expected values may sometimes represent an even
distribution across groups, it is not a requirement. The key is that you
already know the expected values.

Use this test when your goal is to determine whether a discrepancy
between a set of proportions (frequencies) that you are expecting to see
and a set that you have observed is not simply due to a sampling error
or random chance. Let's look at the goodness-of-fit test more closely
with an example.

Suppose you want to investigate whether a state-level STEM enrichment
program provides equitable access based on gender. Across the state, 474
students were selected to participate in the program. You would expect
237 males and 237 females, assuming a 50/50 split if there were perfect
gender balance in the selection process. However, the actual frequencies
are displayed in the table below.

\textbf{Gender Distribution of Participants}

\begin{longtable}[]{@{}lll@{}}
\toprule\noalign{}
Gender & Frequency & Percent \\
\midrule\noalign{}
\endhead
\bottomrule\noalign{}
\endlastfoot
Female & 216 & 45.6 \\
Male & 258 & 54.4 \\
\textbf{Total} & \textbf{474} & \textbf{100} \\
\end{longtable}

The distribution is clearly not 50/50, but is it different enough that
the outcome is likely not a result of random chance? To answer this
question, we apply a chi-square goodness-of-fit test. This is another
way to test a hypothesis, so we follow the same steps we have already
learned.

\subsubsection*{Step 1: State the research question and determine the
null and alternative
hypotheses.}\label{step-1-state-the-research-question-and-determine-the-null-and-alternative-hypotheses.}
\addcontentsline{toc}{subsubsection}{Step 1: State the research question
and determine the null and alternative hypotheses.}

Typically, we can make two types of predictions for goodness-of-fit
tests. We often expect all categories to be equal. We would expect a
50/50 gender split if participants are limited to only selecting between
female and male. Similarly, we would expect 25\% of students to choose
each menu item if limited to four quality lunch options. But other
times, we have reason to expect distributions to not be evenly split
across options. For example, imagine we wanted to investigate the
proportion of students identified as gifted by race/ethnicity. If we had
participants from five different groups, we would not necessarily expect
20\% of the gifted students to come from each group. Instead, we would
expect that the distribution of gifted students match the percentage of
students from each group in the student body overall.

In this example, our null hypothesis is that there is no difference in
proportions by gender, or mathematically, the proportions of males and
females selected are equal to the expected proportions (i.e., 50\%
each). The alternative hypothesis is that there is a significant
difference by gender, or the proportions of males and females selected
are NOT equal to expected proportions.

\subsubsection*{Step 2: Set the criteria for a
decision.}\label{step-2-set-the-criteria-for-a-decision.-1}
\addcontentsline{toc}{subsubsection}{Step 2: Set the criteria for a
decision.}

We will once again use our typical alpha level of 0.05. If the p value
is smaller than 0.05, we will reject the null hypothesis. The next part
of this step is finding the critical value, and that brings us to a new
distribution: the chi-square distribution. Like the t distribution, the
chi-square distribution is a whole family of distributions that varies
depending on the degrees of freedom. The chi-square distribution is a
probability distribution that arises when a sum of the squares of ￼
independent standard normal random variables is calculated. If
(\textbf{\emph{Equation broken here in original doc need fixing}})
follows a chi-square distribution with ￼ degrees of freedom (df).

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{chapter8_files/figure-pdf/unnamed-chunk-4-1.pdf}}

}

\caption{Fig 8.7: Chi-Square Distributions with Different Degrees of
Freedom}

\end{figure}%

For the chi-square goodness-of-fit test, the degrees of freedom are
equal to k -1, where k = the number of categories or levels within your
variable. Our variable is gender, and participants were confined to just
two categories (k = 2), so we have just 1 degree of freedom. Since we
are using α = 0.05, we can flip to the Critical Values of Chi-Square
(\(x^2\)) in the appendix and find that the critical value is 3.84.

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{chapter8_files/figure-pdf/unnamed-chunk-5-1.pdf}}

}

\caption{Figure 8.8: Chi-square distribution with df = 1}

\end{figure}%

One thing you might notice about this table of critical values is that
unlike most of the tables we have used, the critical value increases as
degrees of freedom increase. The reason for this will make sense when
you see the formula for the test statistic. Another important point is
that the chi-square test is always one-tailed, even though our
alternative hypothesis does not imply any specific direction for the
difference. Instead, we test for the total deviation from the expected
values. We reject the null hypothesis if our observed chi-square
statistic exceeds the critical value.

\subsubsection*{Step 3: Verify that data meet necessary assumptions for
the statistical method, and calculate the test
statistic.}\label{step-3-verify-that-data-meet-necessary-assumptions-for-the-statistical-method-and-calculate-the-test-statistic.}
\addcontentsline{toc}{subsubsection}{Step 3: Verify that data meet
necessary assumptions for the statistical method, and calculate the test
statistic.}

There are two data conditions that are required for the use of a
chi-square test:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Observations are independent, meaning the same individual or case
  cannot be categorized into more than one group.
\item
  The expected frequency count (n) for each cell or category should be 5
  or more. If your expected values are too small, that will cause
  problems with the calculation of the test statistic. The most common
  solutions to this problem are increasing your sample size to ensure
  sufficient observations in each cell or merge categories to achieve
  larger expected frequencies. You might also choose to conduct a
  Fisher's exact test, which we will not detail in this book.
\end{enumerate}

When we talk about chi-square tests, we use the word ``cell.'' Think of
this as if you are creating a table with rows and columns. A cell is
just one square in the table. It belongs to one row and one column. In
this example, our table is one column (count of students) by two rows
(female/male). So, we have two cells altogether.

The formula for the goodness-of-fit test is:

\[
\chi^2 = \sum \frac{(O_i - E_i)^2}{E_i}
\]

In this formula, Oi = the observed count of ith cell (i.e., how many
individuals we observed actually fall within the cell) and Ei = the
expected count of ith cell (the number of individuals that we think
should be in the cell). For each cell, we would find a difference
between observed and expected values, square the difference (remember,
if we are adding up differences and want the answer to be something
other than zero, we must square the differences first), and divide it by
the expected value. We would then take these squared values for every
comparison between the expected and observed pair and add them together.

Can you see why it is problematic if expected counts are very small? If
you look at the formula for the chi-square test statistic, the expected
value goes in the denominator. If we have a very low expected value, we
can end up with an artificially inflated test statistic, which increases
the risk of Type I error.

In our example, we already know the expected frequencies (a 50/50 split
of 474 employees would be 237 in each group). We also have the observed
frequencies from the table earlier in the chapter, so we add those
values to the equation:

\[
\chi^2 = \frac{(216 - 237)^2}{237} + \frac{(258 - 237)^2}{237} = 3.72
\]

\subsubsection*{Step 4: Draw a conclusion for your research
question.}\label{step-4-draw-a-conclusion-for-your-research-question.}
\addcontentsline{toc}{subsubsection}{Step 4: Draw a conclusion for your
research question.}

We compare our observed chi-square test statistic to the critical value
we determined earlier. Remember, if our observed value is greater than
the critical value, we reject the null. In this case, 3.72 \textless{}
3.84, so we fail to reject the null. While there is not an exact 50/50
split in employees along gender lines, the difference is not large
enough to be considered significant. The exact p value associated with
(\textbf{\emph{Equation broken again pls fix}})

\begin{figure}[H]

{\centering \pandocbounded{\includegraphics[keepaspectratio]{chapter8_files/figure-pdf/unnamed-chunk-6-1.pdf}}

}

\caption{Figure 8.9: Chi-square distribution (df = 1) with critical
value and observed test statistic}

\end{figure}%

Here, we present an example of a chi-square goodness-of-fit result
written in APA style.

A chi-square goodness-of-fit test was conducted to determine whether the
state-level STEM enrichment program provides equitable access based on
gender, with expected proportions for male and female participants. All
assumptions for the analysis were met. The results indicated no
statistically significant difference in the proportions of males and
females selected for the program (\textbf{\emph{Equation broken here}}).
These findings suggest that the enrichment program provides equitable
access by gender.

\section{Conclusion}\label{conclusion-4}

Hypothesis testing bridges theoretical research questions and practical
data analysis, offering a structured framework for making evidence-based
decisions. By understanding statistical significance, effect sizes, and
confidence intervals, researchers can draw meaningful conclusions while
accounting for uncertainty and variability. For educational researchers,
it is particularly important to assess both statistical and practical
significance when interpreting empirical results.

While p values indicate whether a result is statistically significant,
they do not reveal the magnitude or practical relevance of the effect.
Measures of effect size, such as Cohen's d, provide valuable context by
quantifying the size of an effect, helping researchers determine its
importance in real-world settings. For example, a small p value paired
with a negligible effect size may lack practical significance, limiting
its applicability. Therefore, when statistically significant findings
are observed, it is essential to consider their implications within the
research context, ensuring that the results are both statistically sound
and practically meaningful. This dual evaluation enhances the relevance
and impact of research findings in education and beyond.

\subsection{Key Takeaways for Educational Researchers from Chapter
8}\label{key-takeaways-for-educational-researchers-from-chapter-8}

\begin{itemize}
\item
  Two statistical methods, one-sample t tests and Chi-square
  goodness-of-fit tests, are for comparing the observed statistic to a
  benchmark. Use a one-sample t test when comparing a sample mean to a
  known population mean with unknown standard deviation. Use a
  chi-square goodness-of-fit test when comparing observed proportions or
  frequencies in a single categorical variable to known or expected
  distributions.
\item
  Statistical significance (e.g., p values) indicates whether an
  observed effect is unlikely due to chance, while effect sizes (e.g.,
  Cohen's d) quantify the magnitude and practical relevance of the
  effect. By reporting both, education researchers can ensure their
  findings have actionable value, informing policy, curriculum design,
  or classroom interventions.
\item
  The correct interpretation of a 95\% confidence interval (CI) for a
  population mean is as follows: ``We are 95\% confident that the
  interval contains the true population mean.'' If we were to take many
  random samples of fixed sample size (e.g., n = 50) from the population
  and compute a confidence interval for each sample, approximately 95\%
  of those intervals would contain the true population mean. Confidence
  intervals help researchers communicate the degree of certainty in
  their findings, ensuring that conclusions account for variability and
  are not overly dependent on a single estimate. They are particularly
  beneficial when researchers want to evaluate the reliability of a
  point estimate or compare results across studies.
\item
  Larger sample sizes increase statistical power but may detect trivial
  differences as significant. Carefully designing studies to balance
  power with meaningfulness enables education researchers to use
  resources effectively and avoid misinterpreting results as practically
  relevant when they are not.
\end{itemize}

\section{Key Definitions for Chapter
8}\label{key-definitions-for-chapter-8}

The \textbf{chi-square distribution} is a probability distribution that
arises when a sum of the squares of k independent standard normal random
variables is calculated. If (\textbf{\emph{Equation broken here}})

follows a chi-square distribution with k degrees of freedom (df).

The \textbf{chi-square goodness-of-fit test} is a nonparametric method
used to determine whether observed data frequencies match expected
frequencies for a single categorical variable. This test is useful when
comparing observed proportions to known or hypothesized distributions,
such as evaluating gender equality in a workforce. By calculating the
chi-square statistic and comparing it to a critical value, researchers
can assess whether deviations from expected frequencies are due to
random chance or are statistically significant.

\textbf{Cohen's} \textbf{\emph{d}} quantifies the magnitude of the
difference between two means relative to the variability in the data and
provides a standardized measure of practical significance, helping to
interpret the importance of the result by allowing us to express the
effect size in terms of standard deviation units.

\textbf{Confidence intervals (CIs)} provide an additional layer of
transparency by offering a range of values likely to contain the true
population parameter. The chapter highlighted the interplay between CIs
and hypothesis testing, explaining that if a null value (e.g., a
population mean) falls within the CI, the null hypothesis cannot be
rejected.

\textbf{Effect size} is a numeric measure that indicates the strength or
meaningfulness of the relationship or difference between variables in a
study.

\textbf{Nonparametric statistical tests} are methods used in statistics
that do not require the data to follow a specific distribution, such as
the normal distribution.

\textbf{Type III error} occurs when researchers correctly calculate
statistical results but misinterpret or apply them to the wrong question
or hypothesis.

\section{Check Your Understanding}\label{check-your-understanding-4}

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  \textbf{When should a one-sample \emph{t} test be used instead of a
  \emph{z} test?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    When the population mean is known\\
  \item
    When the population standard deviation is unknown\\
  \item
    When the sample size is less than 30\\
  \item
    When comparing two sample means
  \end{enumerate}
\item
  \textbf{What is the formula for the degrees of freedom (\emph{df} ) in
  a one-sample \emph{t} test?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    \emph{df} = \emph{n} + 1\\
  \item
    \emph{df} = \emph{n} × 2\\
  \item
    \emph{df} = \emph{n} − 1\\
  \item
    \emph{df} = \emph{n} / 2
  \end{enumerate}
\item
  \textbf{Which of the following is \emph{not} an assumption of the
  one-sample \emph{t} test?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Random sampling\\
  \item
    Independence of observations\\
  \item
    Population variance of 0\\
  \item
    Dataset is approximately normally distributed
  \end{enumerate}
\item
  \textbf{What is the purpose of calculating effect size, such as
  Cohen's \emph{d}?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    To determine statistical significance\\
  \item
    To quantify the magnitude of a difference\\
  \item
    To check if assumptions are met\\
  \item
    To calculate degrees of freedom
  \end{enumerate}
\item
  \textbf{What is the risk of using a one-tailed test when the actual
  effect is in the opposite direction?}

  \begin{enumerate}
  \def\labelenumii{\alph{enumii}.}
  \tightlist
  \item
    Type I error\\
  \item
    Type II error\\
  \item
    Type III error\\
  \item
    Increased power
  \end{enumerate}
\end{enumerate}

\bookmarksetup{startatroot}

\chapter*{Chapter 8: One-Sample t Test and Goodness-of-Fit Analysis in
R}\label{chapter-8-one-sample-t-test-and-goodness-of-fit-analysis-in-r}
\addcontentsline{toc}{chapter}{Chapter 8: One-Sample t Test and
Goodness-of-Fit Analysis in R}

\markboth{Chapter 8: One-Sample t Test and Goodness-of-Fit Analysis in
R}{Chapter 8: One-Sample t Test and Goodness-of-Fit Analysis in R}

This section demonstrates how to conduct a One-Sample t Test and a
Chi-Square Goodness-of-Fit Test using R. These tests are useful for
evaluating whether a sample mean significantly differs from a known
benchmark and for comparing observed frequencies to an expected
distribution. We will use the PISA 2022 U.S. dataset to illustrate these
methods.

\section*{1 One-Sample t Test}\label{one-sample-t-test}
\addcontentsline{toc}{section}{1 One-Sample t Test}

\markright{1 One-Sample t Test}

Example: Comparing U.S. Math Scores to an International Benchmark

We will compare PV1MATH (math plausible value 1) for U.S. students to a
benchmark value of 441, which represents the approximate mean of
participants from all participating countries.

\subsection*{1.1 Define Hypothesis}\label{define-hypothesis}
\addcontentsline{toc}{subsection}{1.1 Define Hypothesis}

\[
\begin{aligned}
H_0 &: \text{The mean PV1MATH score is equal to } 441 \; (\mu = 441). \\
H_a &: \text{The mean PV1MATH score is not equal to } 441 \; (\mu \ne 441).
\end{aligned}
\]

\subsection*{1.2 Conduct the Test}\label{conduct-the-test}
\addcontentsline{toc}{subsection}{1.2 Conduct the Test}

\textbf{(Reading data is hidden right now, but maybe should include
anyways?)}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Extract math scores }
\NormalTok{math\_scores }\OtherTok{\textless{}{-}}\NormalTok{ data}\SpecialCharTok{$}\NormalTok{PV1MATH }
 
\CommentTok{\# Conduct the one{-}sample t test }
\NormalTok{benchmark }\OtherTok{\textless{}{-}} \DecValTok{441} 
\NormalTok{t\_test\_result }\OtherTok{\textless{}{-}} \FunctionTok{t.test}\NormalTok{(math\_scores, }\AttributeTok{mu =}\NormalTok{ benchmark, }\AttributeTok{alternative =} \StringTok{"two.sided"}\NormalTok{) }

\CommentTok{\# Display results }
\NormalTok{t\_test\_result }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    One Sample t-test

data:  math_scores
t = 14.804, df = 4551, p-value < 2.2e-16
alternative hypothesis: true mean is not equal to 441
95 percent confidence interval:
 459.1090 464.6375
sample estimates:
mean of x 
 461.8733 
\end{verbatim}

\textbf{Interpretation}: The results show that U.S. students scored
significantly higher than the international benchmark of 441. With an
average score of 461.87, the difference is large enough that it is very
unlikely to have occurred by random chance
(\((p < 2.2 \times 10^{-16})\)). The confidence interval {[}459.11,
464.64{]} tells us that, with 95\% certainty, the true average score for
U.S. students falls within this range. Since the entire confidence
interval is above 441, we can confidently say that U.S. students
outperform the international average in mathematics.

\subsection*{1.3 Calculate Effect Size (Cohen's
d)}\label{calculate-effect-size-cohens-d}
\addcontentsline{toc}{subsection}{1.3 Calculate Effect Size (Cohen's d)}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Extract math scores }
\CommentTok{\# Calculate Cohen\textquotesingle{}s d }
\NormalTok{effect\_size }\OtherTok{\textless{}{-}}\NormalTok{ (}\FunctionTok{mean}\NormalTok{(math\_scores) }\SpecialCharTok{{-}}\NormalTok{ benchmark) }\SpecialCharTok{/} \FunctionTok{sd}\NormalTok{(math\_scores) }
\NormalTok{effect\_size }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 0.2194198
\end{verbatim}

Interpretation: With an effect size of 0.22, the difference between the
U.S. mean and the international benchmark is statistically significant
but relatively small. This means that while U.S. students score higher
on average, the difference is not large enough to suggest a dramatic
practical advantage.

\section*{2 Chi-Square Goodness-of-Fit
Test}\label{chi-square-goodness-of-fit-test-1}
\addcontentsline{toc}{section}{2 Chi-Square Goodness-of-Fit Test}

\markright{2 Chi-Square Goodness-of-Fit Test}

\textbf{Example: Evaluating Perception of Mathematics as Easier Than
Other Subjects }

We will analyze MATHEASE, which is a binary variable (0 = No, 1 = Yes)
measuring students' perceptions of whether mathematics is easier than
other subjects. The goal is to test whether responses are equally
distributed (50/50).

\subsection*{2.1 Define Hypothesis}\label{define-hypothesis-1}
\addcontentsline{toc}{subsection}{2.1 Define Hypothesis}

\[
\begin{aligned}
H_0 &: \text{The responses are equally distributed (50\% ``Yes'', 50\% ``No'').} \\
H_a &: \text{The responses are not equally distributed (deviation from 50/50 split).}
\end{aligned}
\]

\subsection*{2.2 Conduct the Test}\label{conduct-the-test-1}
\addcontentsline{toc}{subsection}{2.2 Conduct the Test}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Remove missing values from MATHEASE }
\NormalTok{mathease\_data }\OtherTok{\textless{}{-}} \FunctionTok{na.omit}\NormalTok{(data}\SpecialCharTok{$}\NormalTok{MATHEASE) }
\FunctionTok{length}\NormalTok{(mathease\_data) }\CommentTok{\#used sample size }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 3998
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Count occurrences of Yes (1) and No (0) }
\NormalTok{observed\_counts }\OtherTok{\textless{}{-}} \FunctionTok{table}\NormalTok{(mathease\_data) }
\NormalTok{observed\_counts }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
mathease_data
   0    1 
3497  501 
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Expected counts under the null hypothesis (50/50 distribution) }
\NormalTok{expected\_counts }\OtherTok{\textless{}{-}} \FunctionTok{rep}\NormalTok{(}\FunctionTok{sum}\NormalTok{(observed\_counts) }\SpecialCharTok{/} \DecValTok{2}\NormalTok{, }\DecValTok{2}\NormalTok{) }
\NormalTok{expected\_counts }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] 1999 1999
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# 2. Perform chi{-}square test }
\NormalTok{chi\_sq\_result }\OtherTok{\textless{}{-}} \FunctionTok{chisq.test}\NormalTok{(}\AttributeTok{x =}\NormalTok{ observed\_counts, }\AttributeTok{p =} \FunctionTok{c}\NormalTok{(}\FloatTok{0.5}\NormalTok{, }\FloatTok{0.5}\NormalTok{)) }
\end{Highlighting}
\end{Shaded}

\begin{Shaded}
\begin{Highlighting}[]
\CommentTok{\# Display results }
\NormalTok{chi\_sq\_result }
\end{Highlighting}
\end{Shaded}

\begin{verbatim}

    Chi-squared test for given probabilities

data:  observed_counts
X-squared = 2245.1, df = 1, p-value < 2.2e-16
\end{verbatim}

\textbf{Interpretation}: The results show a highly significant
difference between observed and expected responses ( X2=2245.1 ,
p\textless2.2e−16 ). This means that students' perceptions of whether
math is easier than other subjects are far from evenly split. In
reality, only 501 students (12.5\%) felt math was easier, while 3497
students (87.5\%) believed otherwise. The large chi-square statistic
suggests this imbalance is not due to random chance but reflects a real
tendency among students to find math more difficult compared to other
subjects.

\bookmarksetup{startatroot}

\chapter{Chapter 9}\label{chapter-9}

\bookmarksetup{startatroot}

\chapter{Chapter 9}\label{chapter-9-1}

This is Chapter 9.

\bookmarksetup{startatroot}

\chapter{Chapter 10}\label{chapter-10}

\bookmarksetup{startatroot}

\chapter{Chapter 10}\label{chapter-10-1}

This is Chapter 10.

\bookmarksetup{startatroot}

\chapter{Chapter 11}\label{chapter-11}

\bookmarksetup{startatroot}

\chapter{Chapter 11}\label{chapter-11-1}

This is Chapter 11.

\bookmarksetup{startatroot}

\chapter{Chapter 12}\label{chapter-12}

\bookmarksetup{startatroot}

\chapter{Chapter 12}\label{chapter-12-1}

This is Chapter 12.

\bookmarksetup{startatroot}

\chapter{Chapter 13}\label{chapter-13}

\bookmarksetup{startatroot}

\chapter{Chapter 13}\label{chapter-13-1}

This is Chapter 13.

\bookmarksetup{startatroot}

\chapter{Chapter 14}\label{chapter-14}

\bookmarksetup{startatroot}

\chapter{Chapter 14}\label{chapter-14-1}

This is Chapter 14.

\bookmarksetup{startatroot}

\chapter*{Summary}\label{summary}
\addcontentsline{toc}{chapter}{Summary}

\markboth{Summary}{Summary}

In summary, this book has no content whatsoever.

\bookmarksetup{startatroot}

\chapter*{References}\label{references}
\addcontentsline{toc}{chapter}{References}

\markboth{References}{References}

\phantomsection\label{refs}
\begin{CSLReferences}{1}{0}
\bibitem[\citeproctext]{ref-knuth84}
Knuth, Donald E. 1984. {``Literate Programming.''} \emph{Comput. J.} 27
(2): 97--111. \url{https://doi.org/10.1093/comjnl/27.2.97}.

\end{CSLReferences}




\end{document}
