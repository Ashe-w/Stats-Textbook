---
title: "Chapter 3"
---

# Exploring Data Spread: Measures of Variability {.unnumbered}

## Learning Objectives {.unnumbered}

By the end of this chapter, you will be able to:   

- Define and explain the concept of variability and its importance in statistical analysis, distinguishing between measures such as range, variance, and standard deviation. 

- Calculate measures of variability (range, interquartile range, variance, and standard deviation) for a given dataset and interpret what these values indicate about the spread of the data. 

In this chapter, we are continuing the discussion about summarizing data. In the last chapter, we discussed how we can use one statistic to represent the central tendency of a dataset. Most people are likely already familiar with the concept of using an average, or the mean, to summarize data. In this chapter, we will explore how we can use other statistics to represent the spread, or variability, in a dataset. Variability refers to the extent to which data points in a dataset differ from each other and/or from the central value (e.g., the mean). It is a measure of how spread out or dispersed the data are. High variability indicates that the data points are more spread out, while low variability means they are closer together. A measure of variability should be reported along with any measure of central tendency because they provide different, but complementary information.  

 

Variability is often a little tricky to understand, so let’s look at two simple datasets to help with our learning.  

- **Set A**: 47, 48, 49, 50, 51, 52, 53 

- **Set B**: 20, 30, 40, 50, 60, 70, 80 

The mean for each dataset is 50, but these sets of numbers are quite different! The numbers in Set B are more spread out from the center, which is another way of saying they have greater variability. If we only report the mean for each dataset, then we have really not summarized the data well.  

 

Before we talk about the different measures of variability, the table below includes some examples of variables with high, low, and no variability.  

::: {.variability-table}
::: {.vcell}
<div class="vtitle">High variability</div>


- Semester tuition rates across all public and private colleges and universities in the United States.
- Salaries of college graduates entering the workforce.
:::

::: {.vcell}
<div class="vtitle">Low variability</div>

- Starting salaries of state employees in similar positions.
- High school GPAs of students who are accepted to elite private colleges.
- Standardized math scores for 8th grade students in a gifted education program.
:::

::: {.vcell}
<div class="vtitle">No variability</div>

- Grade level (for a program that only includes 5th graders).
- Fixed-rate mortgage payments over the course of a loan.
- Enrollment fee for a program in dollars.
:::
:::

## Measures of Variability 

***note: Perhpas splitting into sections would be nice here?***

Just like with central tendency, there are several different ways to measure and report the variability of a dataset. We introduce four different measures of variability: range, interquartile range, standard deviation and variance.  

 

A relatively crude or simple indicator of variability is the **range**.  The range is a measure of variability that represents the difference between the largest and smallest values in a dataset. It provides a simple indication of how spread out the data are. For example, the range in Set A above is 6 (the range = the largest value – the smallest value, i.e., $53 - 47 = 6$), and the range in Set B is $60$ ($80 - 20$). You can see that even if we only knew the range, we would get a sense that these two datasets were different.  

 

However, a major limitation of range is that it is affected by outliers. Let's insert a third dataset that looks like this: $0, 56, 57, 58, 59, 60, 60$. We still have a mean of $50$ and a range of $60$, but the range seems misleading. That $0$ in our third dataset is a complete outlier because it is far below the rest of the values. It could be an odd case, or even a typo, but it changes the range of our third dataset from $4$ to $60$, completely changing our understanding of what the data look like. 

 

**Interquartile range**, or **IQR**, is the difference between the first and third quartiles, or the 25th and 75th percentiles. Much like how the median was better than the mean when the data included major outliers, the IQR is better than the range in the same circumstances, because it is less likely to be affected by extreme values.  

 

To find the IQR, start by organizing your dataset in ascending order. Next, divide the data into two halves to calculate the first quartile ($Q_1$) and third quartile ($Q_3$). The first quartile ($Q_1$) is the median of the lower half of the data, representing the 25th percentile, while the third quartile ($Q_3$) is the median of the upper half of the data, representing the 75th percentile. The IQR is then calculated as $Q_3$−$Q_1$, which measures the range of the middle 50% of the data, providing a robust indicator of variability that is less sensitive to outliers. Let’s compute the IQR with a sample set of data.  

Suppose you have the dataset: $3, 7, 8, 12, 13, 15, 16, 20, 21$.   

- We first find the median.  

    - Since we have 9 values, the median is the fifth value: $(9+1)/2 = 5$. The fifth value is $13$. The median of this dataset is $13$.  

- We next find the median of the lower half of the dataset. Since we have four values, we will need to add the two middle values ($7 + 8$) and then divide by $2$ to get $7.5$. The first quartile is equal to $7.5$.  

- We then find the median of the upper half of the dataset. Since we again have four values, we will need to add the two middle values ($16 + 20$) and divide by $2$ to get $18$. The third quartile is equal to $18$.  

- Last, we find the difference between the third quartile and first quartile ($18 – 7.5$), which is equal to $10.5$. Thus, the IQR for this example is $10.5$.   

**Standard deviation (SD)** tells us how far away, on average, each individual data point is from the mean. In other words, is this a dataset where most of the individual points are clustered close together, so each point is very close to the mean (small standard deviation)? Or are the individual points spread much farther apart, so any given value could be quite different from the mean (large standard deviation)?  

 

Think about how you might figure out the standard deviation if you had to invent statistics yourself. It was stated earlier that it is how far away each data point is, on average. We know that we find an average by adding up a bunch of values and dividing by the number of values, right? So, in theory, we could find the mean, then take every single individual data point, and figure out how far away they are from the mean. Add them all up, divide by the number of data points, and then we have the average distance away, right? 

 

Not quite, due to one problem – negative numbers. By definition, some data points are going to be above the mean and some will be below. Say you have a mean of 50, and one data point is 45 while another is 55. If we take the differences and add them up, we get -5 + 5 = 0. This is not helpful at all. So, what do we do? Think back to the last chapter when we were learning about the mean. The mathematical way to solve this problem is to square the differences, which results in only positive numbers.   

 

This brings us to **variance**, which is a measure of variability that indicates how far the data points in a dataset are spread out from the mean. It is calculated as the average of the squared differences between each data point and the mean, and the equation is listed in the next table. By squaring the differences, variance gives greater weight to larger deviations, making it sensitive to extreme values in the dataset. While variance effectively quantifies dispersion, it is expressed in squared units of the original data, which can make it less intuitive for interpretation compared to the standard deviation. Variance is a fundamental concept in statistics, as it forms the basis for other measures of variability, such as the standard deviation, and is crucial in fields like probability, regression, and hypothesis testing. 

Let’s calculate the variance for a very simple dataset containing just four numbers: $5, 7, 9, 11$. 

- The mean for our dataset is $8$: 

    - $(5 + 7 + 9 + 11)/4 = 8$

- Subtract the mean from each of our values and square the result: 

    - $5 – 8 = -3, -3^2 = 9$

    - $7 – 8 = -1, -1^2 = 1$

    - $9 – 8 = 1, 1^2 = 1$

    - $11 – 8 = 3, 3^2 = 9$

- Sum the squared differences and divide by the number of data points: 

    - $9 + 1 + 1 + 9 = 20, 20/4 = 5$

 

- The variance for our dataset is $5$.  


This concept of the “sum of squares” is going to become very important when we learn topics like Analysis of Variance (ANOVA) in later chapters. If you find yourself confused later about what “sum of squares” means, just remember that it has to do with variance.  

 

Back to the current topic, we now have a value of 5, the variance, that tells us the average squared distance each point is from the mean. However, it is hard to interpret squared values, so what is our last step? We take the square root of the variance, and we call that the standard deviation, which is on the same scale as the original variable. Essentially, we square the differences to deal with negative numbers, and then we un-square the result so the statistic is easier to work with for interpretation. This means the standard deviation for our example is approximately 2.24 (i.e., 
5–√
5
 
). Any time you see variance and standard deviation reported, the second value will be the square root of the first.  

 

A couple more notes before we get to the formulas. First, we can find the variance and standard deviation for a population or a sample, and the notation is different for each (see the following table). Most of the time we’re working with samples, but it’s good to know both. Second, if we only have a sample, then we have to divide by (n – 1) rather than N, where N represents the population size and n represents sample size, respectively. The quantity of (n – 1) for the sample variance and standard deviation estimators is known as the “degrees of freedom” (or df). We use the df to obtain an unbiased, or more accurate, estimate of the population parameter when we only have a sample to analyze.  

 

The table below shows how to calculate the variance and standard deviation by hand, with the correct notations for populations and samples: $σ_x^2$ and $σ_x$ represent population variance and standard deviation of variable $x$, respectively. 
$s_x^2$ and $s_x$ represent sample variance and standard deviation of variable $x$, respectively. 
$\mu$ is a population mean and $\bar{x}$ is a sample mean of variable $x$. 

::: {.formula-table}

|  | Population ($\sigma^2,\,\sigma$) | Sample ($s^2,\,s$) |
|---|---|---|
| **Variance** | $\sigma_x^2=\dfrac{\sum (x_i-\mu)^2}{N}$ | $s_x^2=\dfrac{\sum (x_i-\bar{x})^2}{n-1}$ |
| **Standard<br>Deviation** | $\sigma_x=\sqrt{\dfrac{\sum (x_i-\mu)^2}{N}}$ | $s_x=\sqrt{\dfrac{\sum (x_i-\bar{x})^2}{n-1}}$ |

:::

## Characteristics of Variability 


Here is a summary of the characteristics of data variability measures. 

- Higher values indicate a larger amount of variability than lower numbers.  

- When you have no variability, the numbers are constant (e.g., for the dataset 3, 3, 3, 3, 3, 3, the variance and standard deviation equal 0). 

- The variance tells you the average distance from the mean, in squared units.  

- The standard deviation is just the square root of the variance (i.e., it brings the squared units back to regular units).  

- The standard deviation tells you approximately how far the numbers tend to vary from the mean. Making a judgment about how large or small the standard deviation is depends entirely on the unit of measurement for your variable.  

    - If the entire range of scores is 1 to 100, then you can say a dataset with a standard deviation of 20 has more variability than a dataset with a standard deviation of 10.  

    - However, a standard deviation of 10 on a dataset that ranges from 1 to 100 could indicate much greater variability than a standard deviation of 20 on a dataset that ranges from 1 to 1,000.  

## Standard Deviation and the Normal Curve 

We are going to learn a lot more about the normal curve, or bell curve, later but many current and future educators already have at least some knowledge of the concept. Thus, let’s focus on how the normal curve relates to standard deviation.  

 

If data are normally distributed, then an easy rule to apply to the data is the **empirical rule**, or *68/95/99.7 percent rule*. That is: 

- Approximately 68%, or just over two-thirds, of all individual cases will fall within one standard deviation of the mean, either above or below.  

- Approximately 95% of cases will fall within two standard deviations of the mean.  

- Approximately 99.7% of cases will fall within three standard deviations of the mean.  

```{r}
#| label: fig-empirical-rule
#| fig-cap: "Figure 3.1 Normal Distribution Showing the Empirical Rule"
#| fig-width: 7
#| fig-height: 4
#| warning: false
#| message: false
#| echo: false

library(ggplot2)

# Parameters (edit if you want a different mean/SD)
mu <- 0
sigma <- 1

# Curve data
x <- seq(mu - 4*sigma, mu + 4*sigma, length.out = 2000)
df <- data.frame(
  x = x,
  y = dnorm(x, mean = mu, sd = sigma)
)

ggplot(df, aes(x, y)) +
  # Shade ±3 SD (lightest) first
  geom_area(
    data = subset(df, x >= mu - 3*sigma & x <= mu + 3*sigma),
    aes(fill = "99.7% area (±3 SD)"),
    alpha = 0.25
  ) +
  # Shade ±2 SD (medium)
  geom_area(
    data = subset(df, x >= mu - 2*sigma & x <= mu + 2*sigma),
    aes(fill = "95% area (±2 SD)"),
    alpha = 0.35
  ) +
  # Shade ±1 SD (darkest)
  geom_area(
    data = subset(df, x >= mu - 1*sigma & x <= mu + 1*sigma),
    aes(fill = "68% area (±1 SD)"),
    alpha = 0.55
  ) +
  # Normal curve line
  geom_line(aes(color = "Normal Distribution"), linewidth = 1) +
  # Mean (dashed)
  geom_vline(aes(linetype = "Mean"), xintercept = mu, color = "black", linewidth = 0.8) +
  coord_cartesian(xlim = c(mu - 4*sigma, mu + 4*sigma), expand = FALSE) +
  labs(x = NULL, y = NULL) +
  scale_fill_manual(
    values = c(
      "68% area (±1 SD)"  = "#3B4BFF",
      "95% area (±2 SD)"  = "#6C78FF",
      "99.7% area (±3 SD)"= "#9AA2FF"
    )
  ) +
  scale_color_manual(values = c("Normal Distribution" = "#1F2BFF")) +
  scale_linetype_manual(values = c("Mean" = "dashed")) +
  guides(
    fill = guide_legend(order = 2),
    color = guide_legend(order = 1),
    linetype = guide_legend(order = 3)
  ) +
  theme_minimal(base_size = 11) +
  theme(
    panel.grid.minor = element_blank(),
    legend.position = "left",
    legend.title = element_blank()
  )
```

When you hear the term standardized tests, what that usually means is that the test has been standardized to fit the normal distribution. If you hear that a national standardized test has a mean of 500 and a standard deviation of 100, then you can apply the empirical rule and know that 68% of test takers score between 400 and 600 points.  

```{r}
#| label: fig-standardized-test-scores
#| fig-cap: "Figure 3.2 Normal Distribution of Standardized Test Scores"
#| fig-width: 7
#| fig-height: 4
#| warning: false
#| message: false
#| echo: false


library(ggplot2)

# Parameters for standardized test scores
mu <- 500
sigma <- 100

# Generate curve data
x <- seq(mu - 4*sigma, mu + 4*sigma, length.out = 2000)
df <- data.frame(
  x = x,
  y = dnorm(x, mean = mu, sd = sigma)
)

ggplot(df, aes(x, y)) +
  
  # ±3 SD (lightest)
  geom_area(
    data = subset(df, x >= mu - 3*sigma & x <= mu + 3*sigma),
    aes(fill = "99.7% area (±3 SD)"),
    alpha = 0.25
  ) +
  
  # ±2 SD
  geom_area(
    data = subset(df, x >= mu - 2*sigma & x <= mu + 2*sigma),
    aes(fill = "95% area (±2 SD)"),
    alpha = 0.35
  ) +
  
  # ±1 SD (darkest)
  geom_area(
    data = subset(df, x >= mu - 1*sigma & x <= mu + 1*sigma),
    aes(fill = "68% area (±1 SD)"),
    alpha = 0.55
  ) +
  
  # Normal curve
  geom_line(aes(color = "Normal Distribution"), linewidth = 1.2) +
  
  # Mean line
  geom_vline(aes(linetype = "Mean"), 
             xintercept = mu, 
             linewidth = 0.9, 
             color = "black") +
  
  coord_cartesian(
    xlim = c(mu - 4*sigma, mu + 4*sigma),
    expand = FALSE
  ) +
  
  labs(x = NULL, y = NULL) +
  
  scale_fill_manual(
    values = c(
      "68% area (±1 SD)"  = "#3B4BFF",
      "95% area (±2 SD)"  = "#6C78FF",
      "99.7% area (±3 SD)"= "#9AA2FF"
    )
  ) +
  
  scale_color_manual(values = c("Normal Distribution" = "#1F2BFF")) +
  scale_linetype_manual(values = c("Mean" = "dashed")) +
  
  guides(
    fill = guide_legend(order = 2),
    color = guide_legend(order = 1),
    linetype = guide_legend(order = 3)
  ) +
  
  theme_minimal(base_size = 11) +
  theme(
    panel.grid.minor = element_blank(),
    legend.position = "left",
    legend.title = element_blank()
  )
```
### Common Misconceptions About Variability 
Here are some common past misconceptions that students have had about variability, with brief explanations of the correct understanding for each concept.  

 

1.  **Misconception**: Variability is undesirable and should be minimized or eliminated. 
 
    **Reality**: Variability is an inherent and necessary component of any dataset. In many cases, it offers crucial insights and is essential for understanding differences within a dataset and patterns. 

 

2.  **Misconception**: Variability in data occurs due to measurement errors. 
 
    **Reality**: Although variability may include measurement error, it also reflects genuine differences between observations, which are essential for understanding patterns and drawing meaningful conclusions.  

 

3.  **Misconception**: Low variability is often interpreted as a sign of grader accuracy and consistency in the data and considered a desired characteristic of data. 
 
    **Reality**: While low variability may indicate precision, it does not necessarily ensure accuracy or representativeness. For example, a thermometer that consistently provides incorrect readings has low variability but isn't accurate. 

 

4.  **Misconception**: Increasing the sample size (e.g., from 10 cases to 100 cases) eliminates variability in the data. 
 
    **Reality**: Larger sample sizes reduce sampling variability (the differences between sample statistics and the population parameters, which is sampling error), but the natural variability in the population remains unchanged. This means that, in theory, the sample size doesn’t affect the inherent variability in data. 

 

5.  **Misconception**: It is commonly believed that all measures of variability, such as range, variance, standard deviation, or interquartile range, tell the same story. 
 
    **Reality**: Various measures of variability emphasize different characteristics of a dataset. For example, the range focuses solely on the extremes, whereas the standard deviation accounts for the distances of all data points from the mean. 

 

6.  **Misconception**: Variability measures, like standard deviation, are unrelated to measures of central tendency, like the mean. 
 
    **Reality**: Variability is closely linked to the central value used in its calculation. For instance, variance is calculated relative to the mean, highlighting the importance of the chosen measures of central tendency in interpreting variability. 

 

7.  **Misconception**: Mean and standard deviation values alone give enough insights into a dataset. 
 
    **Reality**: Mean and standard deviation offer valuable statistical summaries, but they may miss crucial patterns like extreme values (outliers), asymmetric distributions (skewed distributions), or multiple peaks in the data. Additional analysis beyond these basic measures is often needed for a complete understanding. 
 

8.  **Misconception**: High variability indicates that the data may be unreliable. 
 
    **Reality**: Actually, high variability in data doesn't necessarily mean the data are untrustworthy. High variability simply means there's a wide spread or dispersion in the values. The data could be perfectly accurate and reliable while still showing large variations. 
 

9.  **Misconception**: Knowing standard deviation of a dataset is sufficient to understand how data are spread. 
 
    **Reality**: While standard deviation measures typical distance from the mean, it fails to capture the full picture of how data are distributed, including the shape, symmetry, and concentration of values. 

## Conclusion 

In conclusion, this chapter has provided a thorough exploration of variability, a crucial aspect of statistical analysis that complements measures of central tendency. By understanding range, interquartile range, variance, and standard deviation, readers can better assess how data are spread and how individual data points deviate from the mean. This chapter emphasized the importance of selecting the appropriate measure of variability based on data characteristics, ensuring a more accurate interpretation of statistical findings. Additionally, it highlighted common misconceptions about variability, reinforcing the idea that variability is not an error but an essential component of data analysis. As we move forward, the next chapter will introduce visual representations of data, providing further insights into how data distributions can be effectively communicated and understood. 

 

### Key Takeaways for Educational Researchers from Chapter 3  

    - Variability highlights the extent of differences of each data point within a dataset beyond the central value. Two datasets may have the same mean values but differ drastically in their spread, impacting the interpretation of results.  

    - High variability in datasets (e.g., 8th graders’ standardized reading test scores across states) may suggest diverse population characteristics. On the other hand, low variability (e.g., standardized test scores within a specialized or sub-group) indicates uniformity, useful for specific interventions.   

    - Range can be largely impacted by outliers, while IQR or standard deviation will be less impacted by outliers. 

    - Central tendency, such as mean, median and mode, and variability are complementary metrics for understanding how data look. Reporting both metrics (e.g., mean and standard deviation) ensures a more complete summary.  

    - Educational researchers should choose measures of central tendency and variability that align with their data characteristics and research questions.

## Key Definitions from Chapter 3 

The **empirical rule**, also known as the 68-95-99.7 rule, is a guideline stating that 68% of the data falls within 1 standard deviation of the mean, 95% of the data falls within 2 standard deviations of the mean, and 99.7% of the data falls within 3 standard deviations of the mean. 

 
**First quartile** is another name for the median of the lower half of the values in a dataset (25th percentile). 

 

**Interquartile range (IQR)** is a measure of variability that indicates the spread of the middle 50% of a dataset. It is the difference between the third quartile and the first quartile. 

 

**Range** is a measure of variability that represents the difference between the largest and smallest values in a dataset. It provides a simple indication of how spread out the data is. 

 

**Standard deviation (SD)** is the square root of the variance and is expressed in the same units as the data.  It quantifies how much the individual data points deviate, on average, from the mean of the dataset. 

 

**Third quartile** is another name for the median of the upper half of the values in a dataset (75th percentile). 

 

**Variability** refers to the extent to which data points in a dataset differ from each other and from the central value (e.g., the mean or median). It is a measure of how spread out or dispersed the data is. High variability indicates that the data points are more spread out, while low variability means they are closer together. 

 

**Variance** is a measure of variability that indicates how far the data points in a dataset are spread out from the mean. 

 

 

 

 

## Check Your Understanding  

1.  **What is the range of the following dataset? (1, 2, 3, 4, 5, 6, 7, 8, 9, 10)**

    a. 10 

    b. 9 

    c. 1 

    d. 0 

 

2.  **The ______ is the square root of the variance?**

    a. Interquartile Range  

    b. Range 

    c. Standard Deviation 

    d. Mean 

 

3.  **What is the population variance of the following dataset? (1, 2, 3, 4, 5)**

    a. 1 

    b. 2 

    c. 2.5 

    d. 3 

 

4.  **What is the population standard deviation of the following dataset? (1, 2, 3, 4, 5)**

    a. 1.41 

    b. 1.73 

    c. 4.00 

    d. 9.00 

 

5.  **What percentage of values fall within 1 SD of the mean if data are normally distributed?**

    a. 50% 

    b. 68% 

    c. 95% 

    d. 99.7% 