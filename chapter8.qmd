---
title: "Chapter 8"
---

# Evaluating Group Averages Against a Benchmark: One-Sample t Test and Goodness-of-Fit Analysis {.unnumbered}

## Learning Objectives {.unnumbered}

By the end of this chapter, you will be able to: 

- Describe the differences between z tests and t tests, including when to use each based on knowledge of population parameters and sample characteristics. 

- Calculate and interpret the test statistic, p value, and effect size for one-sample t tests and goodness-of-fit analyses. 

- Clearly explain the distinction between statistical significance and practical significance, and evaluate both to provide a comprehensive understanding of research findings. 

- Use confidence intervals to quantify uncertainty in sample mean estimates and evaluate the plausibility of a null hypothesis. 


<hr>


We just learned about z tests in the previous chapter, which you can conduct if you want to compare a sample mean to a population with a known mean and standard deviation. Often, however, we do not know the population standard deviation (œÉ), and we must estimate it with the sample statistic, in this case sample standard deviation (s). When this happens, we can no longer rely on the standard normal distribution, also called the z distribution, as it assumes precise knowledge of œÉ. Instead, we use a t distribution, which accounts for the added uncertainty of estimating œÉ. In these cases, we use a t distribution, which closely resembles its z-distribution counterpart but with two key differences.  

 

First, the tail ends of the t distribution take longer to approach the X-axis compared to the z distribution. This is because we are estimating the population standard deviation, and that means we have more uncertainty. If you look at figure 8.1, imagine drawing a vertical line out towards one of the ends. Can you see how there would be more area under the curve for the t distribution with just 5 degrees of freedom? Shaded areas under the curve represent 5% of the total area. This means the sample mean must be farther from the population mean before we can confidently conclude that the observed difference reflects something other than random chance. 


```{r}
#| echo: false
#| message: false
#| warning: false
#| fig-cap: "Fig 8.1: Comparison of Standard Normal and t-Distributions"
#| fig-align: center
#| fig-width: 7
#| fig-height: 4.5

library(ggplot2)

alpha <- 0.05
df1 <- 5
df2 <- 120

# labels (so we can reuse them consistently)
lab_z  <- "Standard Normal Distribution (z)"
lab_t1 <- paste0("t-Distribution (df=", df1, ")")
lab_t2 <- paste0("t-Distribution (df=", df2, ")")

lab_cz  <- "Critical z-value"
lab_ct1 <- paste0("Critical t-value (df=", df1, ")")
lab_ct2 <- paste0("Critical t-value (df=", df2, ")")

x <- seq(-4.2, 4.2, by = 0.001)

# critical values (right tail)
z_crit  <- qnorm(1 - alpha)
t1_crit <- qt(1 - alpha, df = df1)
t2_crit <- qt(1 - alpha, df = df2)

# densities
dens <- rbind(
  data.frame(x = x, y = dnorm(x),            series = lab_z),
  data.frame(x = x, y = dt(x, df = df1),     series = lab_t1),
  data.frame(x = x, y = dt(x, df = df2),     series = lab_t2)
)

# shade right tails (like your picture: shade z and df=5)
shade <- rbind(
  data.frame(x = x[x >= z_crit],  y = dnorm(x[x >= z_crit]),        fill_series = lab_cz),
  data.frame(x = x[x >= t1_crit], y = dt(x[x >= t1_crit], df = df1), fill_series = lab_ct1)
)

# vertical lines for critical values
vlines <- data.frame(
  x = c(z_crit, t1_crit, t2_crit),
  line_series = factor(c(lab_cz, lab_ct1, lab_ct2),
                       levels = c(lab_cz, lab_ct1, lab_ct2))
)

# colors (close to the reference)
dist_cols  <- setNames(c("#1f77b4", "#d62728", "#2ca02c"), c(lab_z, lab_t1, lab_t2))
vline_cols <- setNames(c("#1f77b4", "#d62728", "#2ca02c"), c(lab_cz, lab_ct1, lab_ct2))
fill_cols  <- setNames(c("#1f77b4", "#d62728"), c(lab_cz, lab_ct1))

# One combined color scale so curves + vlines share the same legend "Color"
all_color_values <- c(dist_cols, vline_cols)

ggplot() +
  # curves
  geom_line(
    data = dens,
    aes(x = x, y = y, color = series, linetype = series),
    linewidth = 1
  ) +
  # shaded tails
  geom_area(
    data = shade,
    aes(x = x, y = y, fill = fill_series),
    alpha = 0.25
  ) +
  # critical value lines
  geom_vline(
    data = vlines,
    aes(xintercept = x, color = line_series, linetype = line_series),
    linewidth = 0.9
  ) +
  # axes/labels
  labs(
    title = "Comparison of Standard Normal and t-Distributions",
    x = "x",
    y = NULL,
    color = NULL,
    linetype = NULL,
    fill = NULL
  ) +
  coord_cartesian(xlim = c(-4, 4)) +
  scale_x_continuous(breaks = seq(-4, 4, by = 1)) +
  theme_minimal(base_size = 12) +
  theme(
    legend.position = "top",
    plot.title = element_text(hjust = 0.5)
  ) +
  # manual scales
  scale_color_manual(values = all_color_values) +
  scale_fill_manual(values = fill_cols) +
  scale_linetype_manual(values = c(
    # curves
    setNames(c("solid", "dashed", "dashed"), c(lab_z, lab_t1, lab_t2)),
    # vlines
    setNames(c("dashed", "dashed", "dashed"), c(lab_cz, lab_ct1, lab_ct2))
  ))
```

Second, there is more than one t distribution, and the shape of the distribution depends on your sample size. Recall that a larger sample size leads to a smaller standard error, which in turn increases your statistical power to detect a significant effect. You can see how different t distributions look in figure 8.1. The larger the sample size, the closer the t distribution becomes to a z distribution. Once n > 120, the two distributions are almost identical. Having such a large sample makes up for the fact that we are estimating the population standard deviation.  

 

Another way to look at this is that the t distribution is related to the degrees of freedom. You must know the degrees of freedom to utilize a t table. Since sample standard deviation is used to compute the estimate of standard error, the degrees of freedom for a t test are calculated by using n - 1.   

## Example: Conducting one sample t-test

Now, let's walk through an example of conducting a one-sample t test. Let‚Äôs think back to the example from the last chapter, where we compared a sample mean to a population mean with the z test. We have a similar, but slightly different, scenario this time. We have a sample of just 35 test scores from a single third grade class. The teacher is concerned that their students are underperforming in English Language Arts (ELA) and hypothesize their class will score below the state average of 440. When the results were returned to the school, the class's average score was 425.26. Is this average significantly lower than the expected value of 440? Let's conduct a hypothesis test to find out. 

### Step 1: Determine the null and alternative hypotheses. 

Just like last time, the null hypothesis is that there is no difference between the teacher‚Äôs class and the overall state population. So, the null hypothesis is: 

$$
H_0: \mu = 440
$$

This time, however, we are going to use a directional, or one-tailed test, because the teacher is specifically concerned that the students are scoring lower than the general population. So, the alternative hypothesis is: 

$$
H_1: \mu < 440
$$

There are 35 students in this class, which might be why they are not doing so well, so n = 35. The sampling distribution in this case would be composed of the means of all possible samples of size n = 35 taken from the state population of third graders. It would be centered at Œº = 440 (the population mean, which we know), but we must calculate the standard error using the sample standard deviation. The sample standard deviation (s) happens to be 44.994.  

$$
\frac{s}{\sqrt{n}} = \frac{44.994}{\sqrt{35}} = 7.605
$$
```{r}
#| echo: false
#| message: false
#| warning: false
#| fig-align: center
#| fig-width: 7
#| fig-height: 4.5
#| fig-cap: "Fig 8.2: Comparison of Sampling Distribution (t, df=34) and Normal Distribution"

library(ggplot2)

mu <- 440
df <- 34
se <- 7.6

# label strings (so we can reuse them safely in scales)
lab_t <- paste0("Sampling Distribution (t, df=", df, ")")
lab_n <- "Normal Distribution"
lab_mean <- paste0("Mean (\u03BC = ", mu, ")")

x <- seq(410, 470, by = 0.05)

y_t <- dt((x - mu)/se, df = df) / se
y_n <- dnorm(x, mean = mu, sd = se)

df_plot <- rbind(
  data.frame(x = x, y = y_t, curve = lab_t),
  data.frame(x = x, y = y_n, curve = lab_n)
)

ggplot(df_plot, aes(x = x, y = y, color = curve, linetype = curve)) +
  geom_line(linewidth = 1) +

  # mean line
  geom_vline(aes(xintercept = mu, color = lab_mean, linetype = lab_mean),
             linewidth = 0.9) +

  labs(
    title = paste0("Comparison of Sampling Distribution (t, df=", df, ") and Normal Distribution"),
    x = "Sample Mean",
    y = NULL,
    color = NULL,
    linetype = NULL
  ) +

  # manual styling (NO paste0() on the left side; use setNames)
  scale_color_manual(values = setNames(
    c("#1f77b4", "#ff7f0e", "red"),
    c(lab_t, lab_n, lab_mean)
  )) +
  scale_linetype_manual(values = setNames(
    c("solid", "dashed", "dashed"),
    c(lab_t, lab_n, lab_mean)
  )) +

  theme_minimal(base_size = 12) +
  theme(
    legend.position = c(0.78, 0.88),
    legend.background = element_rect(fill = "white", color = "grey80"),
    plot.title = element_text(hjust = 0.5)
  ) +
  coord_cartesian(xlim = c(410, 470)) +
  scale_x_continuous(breaks = seq(410, 470, by = 10))
```

### Step 2: Set the criteria for a decision. 

We will set the alpha level (Œ±) to 0.05. So, we will reject the null hypothesis if the p value is less than 0.05. Since we are conducting a one-sided directional test, the entire rejection region is in the left tail of the distribution. To find the critical value we first need to consider our degrees of freedom. For a t test, df = n ‚Äì 1, so we have 34 degrees of freedom. In the Critical Values of t distribution table in the appendix, there is no line for df = 34, so round down to the nearest degrees of freedom (30) as a conservative estimate. If you use an online calculator, which allows you to enter precise degrees of freedom, you will find that the critical value for 34 degrees of freedom at a .05 alpha level is -1.69. It is negative because we expect the sample mean to be lower than the population mean.  
```{r}
#| echo: false
#| message: false
#| warning: false
#| fig-align: center
#| fig-width: 7
#| fig-height: 4.2
#| fig-cap: "Sampling Distribution of the Sample Mean (n=35)"

library(ggplot2)

# ---- parameters (edit if your problem uses different values) ----
mu <- 440
n  <- 35
alpha <- 0.05
df <- n - 1

# Use your standard error if you know it; this matches earlier numbers:
s <- 44.994
se <- s / sqrt(n)

# left-tailed critical value for t
tcrit <- qt(alpha, df = df)          # e.g., about -1.69
xcrit <- mu + tcrit * se             # critical x value on "Sample Mean" scale

# x grid (match image range)
x <- seq(410, 470, by = 0.05)

# sampling distribution of xbar using t with scaling:
y <- dt((x - mu)/se, df = df) / se

dens <- data.frame(x = x, y = y)

shade <- subset(dens, x <= xcrit)

# label shown in the figure (t critical)
label_txt <- sprintf("%.2f", tcrit)

ggplot(dens, aes(x = x, y = y)) +
  # main curve
  geom_line(aes(color = "Sampling Distribution (t)"), linewidth = 1) +

  # rejection region (left tail)
  geom_area(data = shade,
            aes(fill = "Rejection Region (\u03B1=0.05)"),
            alpha = 0.25) +

  # mean line
  geom_vline(aes(xintercept = mu, color = "Mean (\u03BC = 440)"),
             linetype = "dashed", linewidth = 0.9) +

  # critical value line
  geom_vline(aes(xintercept = xcrit, color = "Critical Value"),
             linetype = "dashed", linewidth = 0.9) +

  # little callout label near the shaded region
  annotate("label",
           x = xcrit - 3.5,
           y = max(y) * 0.25,
           label = label_txt,
           label.size = 0.3) +

  labs(
    title = "Sampling Distribution of the Sample Mean (n=35)",
    x = "Sample Mean",
    y = NULL,
    color = NULL,
    fill = NULL
  ) +
  coord_cartesian(xlim = c(410, 470)) +
  scale_x_continuous(breaks = seq(410, 470, by = 10)) +
  theme_minimal(base_size = 12) +
  theme(
    plot.title = element_text(hjust = 0.5),
    legend.position = c(0.78, 0.88),
    legend.background = element_rect(fill = "white", color = "grey80")
  ) +
  # manual styling to match your picture
  scale_color_manual(values = c(
    "Sampling Distribution (t)" = "blue",
    "Mean (\u03BC = 440)" = "red",
    "Critical Value" = "black"
  )) +
  scale_fill_manual(values = c(
    "Rejection Region (\u03B1=0.05)" = "purple"
  ))
```


### Step 3: Check assumptions and compute the test statistic. 
There are three assumptions underlying this test. They are: 

1. Random sampling: The scores come from a random, representative sample. 

2. Independence: The scores are independent of one another. 

3. Normality: The population score distribution is approximately normal. 

It is okay that we didn‚Äôt choose a random sample representing all third graders in the state because the inferences we will draw are only about this one class. We can‚Äôt see the entire population distribution, but the histogram for the scores looks almost perfectly normal.  

(fig 4 here)

If the distribution was way off, we could still proceed but be very cautious about our interpretation of the results. Since our sample size is greater than 30, we would be okay even without such a normal distribution due to the central limit theorem. So, we are all set to calculate our test statistic.  

 

For the one-sample t test, we use the formula: 

$$
t = \frac{\bar{X} - \mu}{\frac{s}{\sqrt{n}}}
$$

Notice that the formula is very similar to what we used to calculate the z statistic. The only difference is that we now estimate the standard error using the sample standard deviation and sample size in the denominator because we don‚Äôt know the value of the population standard deviation (ùúé).  


The numerator represents the difference between the observed and test value specified in the null hypothesis. The symbol ùúá represents the population parameter specified in the hypothesis, often referred to as the null value, as it reflects the assumed true population mean under the null hypothesis. The other values for the test statistic come from our sample data.  

$$
t
= \frac{425.26 - 440}{\dfrac{44.994}{\sqrt{35}}}
= \frac{-14.74}{7.605}
= -1.938
$$

### Step 4

#### a: Find the p value. 

We must now determine whether our t statistic is significant, which would mean that it is very unlikely we obtained this value due to chance alone. In step 2, we said that the critical value for this test was -1.69. Picture a t distribution and draw a vertical line about 1.7 standard deviations below the mean. Anything to the left of that is in the rejection region. Is our t statistic of -1.938 in the rejection region? Yes ‚Äì it is farther away from the center than our critical value. This means our p value is less than 0.05.  

(fig 8.5 here)

#### b: Draw a conclusion and report your conclusion.  

Given that our p value is less than 0.05, the appropriate decision is to reject the null hypothesis. In other words, we have found evidence in favor of the alternative hypothesis. Students in this class scored significantly lower than the overall state population on the test. Let‚Äôs assume we use statistical software to land on a p value of 0.03 for this analysis.  

 ## Reporting Results in APA Format 

 The American Psychological Association, 7th edition (APA, 2019) has decided that there is a correct way to report the results of a t test. In this case, you would report the results of our test like this: Scores for students in this third-grade class (M = 425.26) were significantly lower than scores of third graders statewide, t(34) = -1.938, p = 0.03.  

 

The most important part of the reporting is the section that starts with the observed t statistic. You put the degrees of freedom in parentheses immediately following the t; all statistics (e.g., M, t, p) are in italics, and you report the p value after the exact t statistic. Remember, that even if the statistical software says p = 0.000, you should never report that. Instead, you should report p < 0.001. You can also just report that p < 0.05 (your alpha level) if you do not have access to statistical software.  

## Cautions with One-Sided Tests  

 One caution with directional, one-tailed tests is that you need to decide on one-tailed versus two-tailed tests up front. It is really not okay to decide after you see the results that you are going to conduct a one-tailed test. You should only be conducting a one-tailed test if you have good reason to expect the sample mean to be lower or higher than, rather than just different from, the population mean. And while a one-tailed test has greater power to detect a significant result, there is risk involved. Imagine that we expected the sample mean to be significantly lower, but it turned out to be significantly higher than the population mean. If we had been conducting a two-tailed test, that would be a significant difference from the population mean. But since our alternative hypothesis was that Œº < 440, we would fail to reject the null (non-significant result). Most of the time you will be using a two-tailed test for the possibility of a difference in either direction. Only use a one-tailed test if you have a very good reason and you decide ahead of time to do so.  

 

Another caution is that type III error is introduced when using directional tests. Type III error occurs when a correct conclusion is drawn for the wrong reason or when the wrong question is asked, and a statistically valid result is obtained for that incorrect question. For example, a school district wants to determine whether lessons learned from a new professional development program will reduce the time teachers spend on grading. After collecting data and running a statistical test, the null hypothesis (that the program has no effect on grading time) is rejected, and a significant difference was found. District leadership interpreted this result as evidence that the program was reducing grading time, so they decided to expand the program. However, a few weeks later, teachers were demanding that the district take a closer look at the data because they claimed to be spending more time grading. Sure enough, district leadership found they made a mistake: while the statistical test correctly identified a significant difference in grading time (rejecting the null hypothesis), they misinterpreted the direction of the effect. The program increased grading time instead of reducing it. This misinterpretation of the direction of the significant effect is a Type III error: correctly rejecting the null hypothesis but misunderstanding what the result actually implies. 

## Understanding Effect Size 

Hypothesis testing is important for knowing if there is a significant difference between a sample and the population, but it will not tell you how big the difference is. A hypothesis test will almost always return a significant result when you have a large sample size because a test statistic is a function of standard error, which decreases as sample size increases.  

 

Given the mean difference and standard deviation remain the same, as sample size increases, the test statistic always increases. For example, if you wanted to conduct a one-sample t test with a sample mean of 22 and a SD of 2 compared to a population mean of 23.5, you are sure to get a statistically significant result when n = 100,000 even if the p value is > 0.05 when n = 10. It is somewhat paradoxical that with sufficiently large sample size, the statistical power becomes strong enough to flag even small differences as significant, even if those differences are practically trivial or meaningless. Therefore, it is important to have another way to represent the meaningfulness, or practical significance, of a statistically significant result. That is why we calculate and report effect size. 

 

Of course, one way of understanding the size of the difference is to simply compare the sample mean and population mean on the original measurement scale. Someone who understands a particular testing instrument could make their own judgment as to whether a difference of about 15 points was meaningful, or cause for concern. But it helps to have a common language for reporting effect size, and the way we do that is to standardize the effect.  

 

Effect size is a numeric measure that indicates the strength or meaningfulness of the relationship or difference between variables in a study. Unlike p values, which only tell you whether an effect exists, effect size tells you how large or meaningful that effect is. It provides a standardized way to understand the practical significance of research findings, which is particularly useful when comparing results across studies. 

 

Estimated Cohen‚Äôs d is a common measure of effect size for t tests. We hope you remember back in chapter 5 when we learned about z scores, which are also known as standardized scores because they are measured in standard deviation units. When we use estimated Cohen‚Äôs d as a measure of effect size, we are doing the same thing. Cohen's d quantifies the magnitude of the difference between two means relative to the variability in the data and provides a standardized measure of practical significance, helping to interpret the importance of the result by allowing us to express the effect size in terms of standard deviation units. This approach provides a common scale, making it easier to compare results across different tests or measures, rather than relying on raw score differences.  


## Calculating Estimated Cohen‚Äôs d 

One quick note to get us started ‚Äì we are being careful to refer to the effect size we use for t tests as estimated Cohen‚Äôs d to distinguish it from Cohen‚Äôs d used in z tests. This distinction arises because effect sizes for t tests are calculated using sample data, whereas z tests rely on known population parameters. But out in the world of educational research, you will normally just hear it called Cohen‚Äôs d regardless of the type of analysis used.  

 

Here is the formula for estimated Cohen‚Äôs d for a one-sample t test: 

$$
d = \frac{\bar{X} - \mu}{SD}
$$

In our example above, the sample mean (XÃÖ) = 425.26 and SD = 44.994. The population mean (Œº) = 440. Using the formula,  

$$
d
= \frac{425.26 - 440}{44.994}
= -0.33
$$


That is, our effect size is -0.33, which according to the following table from Privitera (2018) is a medium effect size. In our sample research question, this means the students in the third-grade class had scores that were about one-third of a standard deviation below the state average. A positive effect size indicates the sample mean was larger than the population mean, and a negative effect size indicates it was smaller. Since effect size can be negative, you need to consider the absolute value when making a judgement about the strength of the effect. 

 **Cohen‚Äôs Effect Size Conventions**

| Description of Effect | Effect Size (d) |
|----------------------|-----------------|
| Small                | \( d < 0.2 \)   |
| Medium               | \( 0.2 < d < 0.8 \) |
| Large                | \( d > 0.8 \)   |

*Source:* Adapted from Privitera, G. J. (2018). *Statistics for the Behavioral Sciences* (3rd ed.). SAGE Publications.



Before we move on, we want to remind you of a few other similar formulas because we had a tendency to get them confused in the past. First, here is a reminder of the formula for calculating z scores: 

$$
z = \frac{X - \mu}{\sigma}
$$


Do you see the differences between that and the formula used to calculate Cohen‚Äôs d? First, with a z score, we only have one X (i.e., a particular value), rather than a sample mean, or XÃÖ. But even more importantly, we use the population standard deviation (œÉ) rather than the sample SD to calculate a z score. But overall, the concept is exactly the same: both z scores and Cohen‚Äôs d involve converting a raw difference from the mean into standard deviation units. The key difference lies in the specific inputs used for calculations.  

Do you see the differences between that and the formula used to calculate Cohen‚Äôs d? First, with a z score, we only have one X (i.e., a particular value), rather than a sample mean, or XÃÖ. But even more importantly, we use the population standard deviation (œÉ) rather than the sample SD to calculate a z score. But overall, the concept is exactly the same: both z scores and Cohen‚Äôs d involve converting a raw difference from the mean into standard deviation units. The key difference lies in the specific inputs used for calculations.  

 

Another place we have seen learners, including ourselves, get tripped up is confusing the formula for Cohen‚Äôs d with the formula for the t statistic itself. Here they are, side by side: 

$$
\begin{aligned}
d &= \frac{\bar{X} - \mu}{SD}
\qquad
t &= \frac{\bar{X} - \mu}{\dfrac{s}{\sqrt{n}}}
\end{aligned}
$$


Even though the first formula, Cohen‚Äôs d, uses SD to represent the sample standard deviation, and the second, t statistic, uses s, they represent the same value. So, the only difference is that d has nothing to do with sample size, but sample size (n) is hugely important in calculating standard error, which is the denominator for the t statistic. For Cohen‚Äôs d, the mean difference is  divided by the standard deviation, which represents the variability in the actual scores within the sample. For the t statistic, the mean difference is divided by the standard error, which represents the standard deviation of the theoretical sampling distribution. By definition, it is going to be much smaller than the original sample standard deviation.  

 

In this example, the same mean difference (-14.743) served as the numerator for both d and t. When we divided it by standard error for the t statistic, we got -1.938. This told us how far our sample mean was from the center of the sampling distribution. When we divided it by the sample standard deviation for Cohen‚Äôs d, we got -0.33, which told us how far our sample mean was from the population mean, the center of the population distribution.  

 

If nothing else, we hope this helps you see why we need both effect size and hypothesis tests. Effect size tells us how big a difference is while the hypothesis test tells us whether we can be confident the difference is a result of something other than chance. Effect size does not depend on sample size, but you are more likely to get a significant result from your hypothesis test as you increase the sample size.  

 

It is common to report effect size after the results of your hypothesis test. For example, scores for students in this third-grade class (M = 425.26) were significantly lower than scores of third graders statewide, t(34) = -1.938, p = 0.03, d = -0.33. You might also choose to report the significant result followed by a statement that Cohen‚Äôs d (-0.33) indicated a medium effect size. Keep in mind that if you have a small sample size, you might be able to calculate a large effect size even without a significant result. This can be interesting to learn and might suggest you should try to conduct your analysis again with a larger sample size. But we don‚Äôt report effect size after a non-significant result since it does not make sense to describe the size of an effect you just concluded does not exist.  

## Confidence Intervals 

How do confidence intervals fit in? Confidence intervals provide an additional layer of transparency by offering a range of values likely to contain the true population parameter.  

The way we think about it, the reason we use confidence intervals is to be up front about our level of uncertainty. We take a sample and report the mean, but how confident are we that the mean of our sample is equal to the true population mean? Or we have two samples and we find the difference between the two means (stay tuned for the next chapter), but how confident are we that we know the true difference? If we report a point estimate (e.g., a mean) along with an interval estimate, we are being completely transparent about how good our estimate is likely to be.  

 

Another high-level concept to be aware of before we dive into the details is the balance between precision and accuracy. Say we want to estimate the average height of all people in the entire world who are 18 and older. We could be highly confident in our accuracy if we estimated that the mean height of all adults was somewhere between 2 feet (60.96cm) and 8 feet (243.84cm) . Don‚Äôt you feel pretty comfortable accepting that the true population mean is somewhere within that interval? And yet, do you feel like you have learned any useful information? Probably not, because our estimate has almost zero precision. We could go in another direction and estimate that the average height was exactly 65.8 inches (167.13cm). That estimate is much more precise, but who knows how accurate it is?  

 

What we often do is report both point and interval estimates. We commonly see this with survey results. We start with a sample mean and then use the standard error to calculate a confidence interval. The more confident we are (higher level of accuracy), the wider the interval (less precision). We can decide on any confidence level we want and calculate the exact interval, but it is most common to use 90, 95, or 99%.  

### Interpreting Confidence Intervals 

Now that you have the general idea about the purpose of confidence intervals, we are going to get a little pedantic and refer you to the words of [Dr. Paul Savory (2008)](https://digitalcommons.unl.edu/cgi/viewcontent.cgi?referer=&httpsredir=1&article=1009&context=imseteach). Say you want to know the mean of some variable in the population. You take a sample, find the mean, and then create a 95% confidence interval around it. It is incorrect to say, ‚ÄúThere is a 95% probability that Œº falls within this interval.‚Äù This is incorrect because it implies that Œº is random and could take on different values. In fact, Œº is a constant ‚Äì we just don‚Äôt know what it is ‚Äì and it either falls within our interval or it doesn‚Äôt. The correct interpretation is, ‚ÄúWe are confident that if Œº were known, this interval would contain it.‚Äù This may seem like a trivial difference, but it makes an important point that the probability or confidence level we report is referring to the interval, not the population parameter itself. 


We highly recommend visiting the book‚Äôs website to test out the [confidence intervals applet](no link right now :/) for a helpful visualization of the concept. The way the applet works is that you select the confidence level and sample size, and then it will randomly pull one or 25 samples from the population distribution. If you request 25 samples, you will see them all lined up. Each sample will have a mean and corresponding confidence interval around it.  

 

We used a 90% confidence interval with a sample size of 10 in the following example and asked the applet to take 25 samples, see figure 8.6. Almost all resulting samples had a confidence interval that contained the true population mean, but two did not. Note that 23/25 is 92% and not 90%, but when we try again, sometimes we get 24/25 or 22/25. Even our 90% confidence interval is just an estimate. The applet provides a great visualization of the difference between the original population distribution and the sampling distribution. You recommend playing around in the applet to see how increasing the confidence level and/or the sample size will typically result in a greater percentage of possible sample intervals containing the true population mean.

(fig 8.6 here)
 

### Calculating Confidence Intervals 

We are going to be calculating confidence intervals around a sample mean, but keep in mind that you can estimate a confidence interval around other statistics as well, such as a mean difference or an odds ratio. Here is how we estimate an unknown population mean using a confidence interval (CI) when population standard deviation is unknown: 

$$
\text{CI for Population Mean}
=
\text{Sample Mean}
\;\pm\;
(\text{Level of Confidence}) \times (\text{Standard Error})
$$


In mathematical terms: 

$$
CI(\mu) = \bar{X} \pm t_{\text{critical}} \times \frac{s}{\sqrt{n}}
$$

 
So, there are three steps to estimate a confidence interval for a population mean.  

1. Compute the sample mean and standard error.  

2. Choose the level of confidence and find the critical values of the t distribution for that level. 

3. Compute the estimation formula to find the upper and lower confidence interval limits.  

The width of our interval depends on several factors: how confident we want to be, the size of our sample, and the variability (or standard deviation) of scores within our sample.  

 

Let‚Äôs work through an example. Suppose you selected a sample of 100 participants and computed sample statistics. You found a sample mean of 4 and SD of 5. Now, you want to estimate a confidence interval around that mean.  

#### Step 1: Compute the sample mean and standard error 

From the information above, we already know that XÃÖ = 4, SD = 5, and n = 100, so we can calculate the standard error as follows: 

$$
\frac{s}{\sqrt{n}} = \frac{5}{\sqrt{100}} = 0.5
$$

 
#### Step 2: Choose the level of confidence and find critical values 

How do we know what level of confidence to choose? This is where you must balance precision and accuracy. In social science research we typically use a 95% level of confidence, which as you saw is consistent with our typical practice of choosing an alpha level of 0.05 for hypothesis testing.  

 

We look to the Critical Values of t Distribution Table in the appendix to find the critical value. Find the t value for a 95% level of significance for a two-tailed test with df = 99. There is no row for 99 degrees of freedom in the table, so we will use df = 100 for an approximation, and we get the t value of 1.98.  

#### Step 3: Compute the estimation formula to find upper and lower confidence interval limits.  

Using these numbers in the formula from earlier in the chapter, we get: 

$$
4 \pm 1.98 \times 0.5 = (3.01,\; 4.99)
$$


We can conclude that we are 95% confident the interval of 3.01 to 4.99 contains the true population mean.  

### Using a Confidence Interval to Evaluate a Null Hypothesis 

We are going to walk through one more scenario using our original one-sample t test example.  This example will demonstrate how estimation through confidence intervals is connected to hypothesis testing, showing that both methods can provide related insights into evaluating a null hypothesis.  



#### Step 1: Compute the sample mean and standard error 
We already calculated the relevant values, but to get everything back in one place, our sample mean was XÃÖ = 425.26, our standard error was 7.605, and we wanted to know if our sample mean could be considered equal to the population mean of 440. 


#### Step 2: Choose the level of confidence and find critical values 